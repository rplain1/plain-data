{"title":"TidyPyMC","markdown":{"yaml":{"title":"TidyPyMC","author":"Ryan Plain","date":"2025-05-14","categories":["Bayesian","PyMC","Plotnine"],"engine":"jupyter"},"headingText":"Tidybayes in Python would be cool","containsRefs":false,"markdown":"\n\n\nA few weeks ago, Benjamin Vincent posted [this blog post](https://drbenvincent.github.io/posts/mcmc_grammar_of_graphics.html) on using Bayesian models in Python _and_ leveraging the grammar of graphics for plotting. Please take the time to read that post as this is derived and inteded to augment the ideas shared there.\n\nAt the end of the post, Vincent asked \"It would be interesting to see if this approach is appealing to people.\" My answer to that is... YES‼️\n\nI like `PyMC` and `ArviZ` a lot, but it was a huge blow coming from `R` and libraries like `tidybayes`, `bayesplots`, and others that helped wrangle and visualize the posterior.\n\nI fully agree with the approach of `ArviZ` to work with high-dimensional data, but comming from a stats background it is more intuitive to work with things as dataframes rather than objects whenever it makes sense. This is especially true with visualizations and the grammar of graphics.\n\nI'm not sure how or what the best way to contribute to this, and it was mentioned on bluesky that [GoG-like interface is being developped for ArviZ 1.0](https://bsky.app/profile/sethaxen.com/post/3loaw2tpucs2v). The best thing I can do is create a post for me.\n\n\n## TidyPyMC\n\nThis is definitely subjective, but I think the missing commponent right now is a consistent way to turn the `arviz.InferenceData` object into a dataframe. Both this and the code Vincent shared is highly custom to the model, and from my experience that is typically a common design choice between R and Python libraries. There is tradeoffs to both paradigms.\n\nThere are a couple of plots in `tidybayes` [add_epred_draws()](https://mjskay.github.io/tidybayes/reference/add_predicted_draws.html) and [add_predicted_draws()](https://mjskay.github.io/tidybayes/reference/add_predicted_draws.html) that show some of its capabilities. The goal of this is to replicate them.\n\nTo accomplish this, we will bring the observed data, linear predictions, and posterior predictions in the same dataframe.\n\n### Libraries and data\n\nWe'll use the `mtcars` dataset to replicate some of the `tidybayes` examples.\n\n```{python}\nimport pandas as pd\nimport pymc as pm\nimport arviz as az\nimport numpy as np\n\nfrom plotnine.data import mtcars\nfrom plotnine import * # elmo_fire.gif namespace, but they mostly start with geom_*\nmtcars.head()\n```\n\n## Model\n\nThis is an attempt at replicating the model [here](https://mjskay.github.io/tidybayes/reference/add_predicted_draws.html).\n\nThe formula is following `mpg ~ hp * cyl` fit with `brms`.\n\n::: {.callout-note}\nThe scope of this wasn't necessarily to walk through creating a pymc model or walk through the workflow of prior predictive checks, diagnostics, etc. I took the priors from `brms::stancode(brms::brm(mpg ~ hp * cyl, data = mtcars))` in R. I also used `PyMC` over `Bambi`, but both libraries work off the `arviz.InferenceData` object.\n:::\n\n```{python}\n\n# build model and sample posterior\nwith pm.Model(\n    coords={\"obs\": mtcars.index, \"predictors\": ['hp', 'cyl']}\n) as mod:\n\n    X = pm.Data(\"X\", mtcars[[\"hp\", \"cyl\"]], dims=(\"obs\", \"predictors\"))\n\n    alpha = pm.StudentT(\"alpha\", nu=3, mu=19.2, sigma=5.4)\n    sigma = pm.HalfStudentT(\"sigma\", nu=3, sigma=5.54)\n    beta = pm.Normal(\"b\", mu=0, sigma=1, dims='predictors')\n\n    mu = pm.Deterministic(\"mu\", alpha + pm.math.dot(X, beta), dims='obs')\n\n    y = pm.Normal(\n        \"y\",\n        mu=mu,\n        sigma=sigma,\n        shape=X.shape[0],\n        observed=mtcars[\"mpg\"],\n        dims=\"obs\",\n    )\n\n    idata = pm.sample(random_seed=527) # got an anniversary comming up\n\n# sample posterior predictive\nwith mod as model:\n    pp = pm.sample_posterior_predictive(idata, predictions=True)\n```\n\n### Tidy up\n\nThe biggest takeaway I had from Vincent's post was it wsa possible to get tidy data out of the `arviz.InferenceData` object, and this was by far the most difficult part to get my head around.\n\nFrom `idata.posterior`, we'll take three things:\n\n- global parameters: `sigma`, `alpha`\n- parameters `beta` (2)\n- linear predictions `mu`\n\n\n\nThe key is to understand the dimensions of which attribute you want ot get and which ones are the same.\n\n```{python}\nprint(f'alpha: {idata.posterior[\"alpha\"].shape}')\nprint(f'sigma: {idata.posterior[\"sigma\"].shape}')\nprint(f'beta: {idata.posterior[\"b\"].shape}')\nprint(f'mu: {idata.posterior[\"mu\"].shape}')\n\n```\n\n- Both `alpha` and `sigma` are the same shape becuause they are global parameters.\n- `beta` has the same number of draws, each is represented as a row that will pivot\n- `mu` has the same number of draws but for each observation\n\n```{python}\nparams = idata.posterior[[\"sigma\", \"alpha\"]].to_dataframe().reset_index()\nbetas = (\n    idata.posterior[\"b\"]\n    .to_dataframe()\n    .reset_index()\n    .pivot(index=[\"chain\", \"draw\"], columns=\"predictors\", values=\"b\")\n    .reset_index()\n)\n\ndf_posterior = params.merge(betas, on=[\"chain\", \"draw\"])\n\ndf_posterior = (\n    idata.posterior[\"mu\"]\n    .to_dataframe()\n    .reset_index()\n    .merge(mtcars[[\"cyl\", \"mpg\", \"hp\"]], left_on=\"obs\", right_on=mtcars.index)\n    .merge(params, on=[\"chain\", \"draw\"])\n    .merge(betas, on=[\"chain\", \"draw\"], suffixes=[\"\", \"_b\"])\n    .assign( # for plotting later\n        group=lambda x: x.cyl.astype(str)\n        + \"_\"\n        + x.draw.astype(str)\n        + \"_\"\n        + x.chain.astype(str)\n    )\n)\n\ndf_posterior\n```\n\nThe critical takeaway I had from implementing this was learn to leverage `coords` and `dims` in the model container. This makes it easier to work with the data later, especially as dimensions increase (i.e. groups in a multilevel model).\n\nThis sets the posterior to be represented in a tidy dataframe, exactly how Vincent did it. I'm really interested what ways we can cofigure the atributes with `PyMC` to make this generalize across different models and data.\n\n### Posterior predictive\n\nOne way would be to do this would be to use `arviz.summary()` on the sampled posterior predictions. This is a common workflow I would do with `brms` and `tidybayes` of parsing parameter outputs name to match the group, or join an id with the original dataset.\n\n```{python}\ndf_predictions = az.summary(pp)\n```\n\nNext steps to join it with the observed data.\n\n```{python}\ndf_predictions.index = df_predictions.index.str.extract(r\"y\\[(.*?)\\]\")[0]\n\ndf_predictions = df_predictions.merge(mtcars[[\"hp\", \"cyl\", \"mpg\"]], on=df_predictions.index)\ndf_predictions.head()\n```\n\nThis worked well with the named index on `mtcars`. I'm not a fan of `pandas`, and I've long forgotten a lot of tips and tricks to work with the nuances of `pandas` after a couple of years of using `polars`. For future me, I'm going to include a standarad approach of working with the posterior.\n\n```{python}\n\ndf_posterior_predictive = (\n    pp.predictions[\"y\"]\n    .to_dataframe()\n    .reset_index()\n    .merge(mtcars[[\"cyl\", \"hp\"]], left_on=\"obs\", right_on=mtcars.index)\n)\n\ndf_posterior_predictive\n\n```\n\nThe data is aggregated to match the `az.summary()` output since this particular `geom_ribbon()` visualization will only need the HDI values of the posterior predictive distribution.\n\n```{python}\ndf_predictions = (\n    df_posterior_predictive.groupby([\"obs\", \"cyl\", \"hp\"])\n    .agg(\n        pp_mean=(\"y\", \"mean\"),\n        pp_min=(\"y\", lambda x: x.quantile(0.03)),\n        pp_max=(\"y\", lambda x: x.quantile(0.97)),\n    )\n    .reset_index()\n)\ndf_predictions.head()\n```\n\n### The plot is coming together\n\nPlotnine! With the grammar of graphics, we're able to:\n\n- use different datasets\n- layer aesthetics together\n- think about plots with data\n\n\n```{python}\n\n# sample draws for plotting purposes\nsamples = np.random.choice(\n    [x for x in range(999)], size=int(5), replace=False\n)\n\n(\n    ggplot(mtcars, aes(\"hp\", \"mpg\", color=\"factor(cyl)\", fill=\"factor(cyl)\"))\n    + geom_ribbon(\n        aes(y=\"pp_mean\", ymin=\"pp_min\", ymax=\"pp_max\"), data=df_predictions, alpha=0.2\n    )\n    + geom_line(\n        aes(y=\"mu\", group=\"group\"),\n        data=df_posterior[df_posterior.draw.isin(samples)],\n        alpha=0.6,\n    )\n    + geom_point()\n    + theme_minimal()\n    + labs(color='cyl', fill='cyl')\n)\n```\n\n\n\n## But Why?\n\nI'm in the middle of a journey to learn Bayesian statsistics.\n\nI can't state enough what great work the community has done to make it open and accessible. I started out, like many others, with [Richard McElreath's Statistical Rethinking](https://xcelab.net/rm/), and then following along with Solomon Kurz's [Statistical rethinking with brms, ggplot, and the tidyverse](https://bookdown.org/content/4857/). Those are R environments but the PyMC community has gret material on getting started and using it for Bayesian Stats.\n\nThere is a steep learning curve on the theory, but also with the tooling. To do modern Bayesian modeling, you need to interact with a probablelistic programmming languages (PPL). The frameworks and libraries are dramatically different depending on the language or environment being used.\n\nIt's daunting! Knowing the theory doesn't necessarily make it easier to use the tool. Learning one PPL framework doesn't mean it will be trivial to move to another. As a beginner, [brms](https://paulbuerkner.com/brms/) and [Bambi](https://bambinos.github.io/bambi/) are excellent libraries that lower the barrier of entry and make the model building process easier. The downside is with so much abstracted away, it can make it difficult to really learn what is going on.\n\nStepping out from `brms` to [Stan](https://mc-stan.org/) was like falling off a cliff. I wanted something that worked better for me, and `PyMC` was a good blend of what I needed in my journey, and to my surprise have liked it a lot. I felt like I made a lot of progress solidifying concepts building models I had made in `brms` to `PyMC`, however I spent most of the time trying to learn how to use the `az.InferenceData` object.\n\nI see a lot of value getting a way to work with tidy datasets, both in learning and in application.\n","srcMarkdownNoYaml":"\n\n## Tidybayes in Python would be cool\n\nA few weeks ago, Benjamin Vincent posted [this blog post](https://drbenvincent.github.io/posts/mcmc_grammar_of_graphics.html) on using Bayesian models in Python _and_ leveraging the grammar of graphics for plotting. Please take the time to read that post as this is derived and inteded to augment the ideas shared there.\n\nAt the end of the post, Vincent asked \"It would be interesting to see if this approach is appealing to people.\" My answer to that is... YES‼️\n\nI like `PyMC` and `ArviZ` a lot, but it was a huge blow coming from `R` and libraries like `tidybayes`, `bayesplots`, and others that helped wrangle and visualize the posterior.\n\nI fully agree with the approach of `ArviZ` to work with high-dimensional data, but comming from a stats background it is more intuitive to work with things as dataframes rather than objects whenever it makes sense. This is especially true with visualizations and the grammar of graphics.\n\nI'm not sure how or what the best way to contribute to this, and it was mentioned on bluesky that [GoG-like interface is being developped for ArviZ 1.0](https://bsky.app/profile/sethaxen.com/post/3loaw2tpucs2v). The best thing I can do is create a post for me.\n\n\n## TidyPyMC\n\nThis is definitely subjective, but I think the missing commponent right now is a consistent way to turn the `arviz.InferenceData` object into a dataframe. Both this and the code Vincent shared is highly custom to the model, and from my experience that is typically a common design choice between R and Python libraries. There is tradeoffs to both paradigms.\n\nThere are a couple of plots in `tidybayes` [add_epred_draws()](https://mjskay.github.io/tidybayes/reference/add_predicted_draws.html) and [add_predicted_draws()](https://mjskay.github.io/tidybayes/reference/add_predicted_draws.html) that show some of its capabilities. The goal of this is to replicate them.\n\nTo accomplish this, we will bring the observed data, linear predictions, and posterior predictions in the same dataframe.\n\n### Libraries and data\n\nWe'll use the `mtcars` dataset to replicate some of the `tidybayes` examples.\n\n```{python}\nimport pandas as pd\nimport pymc as pm\nimport arviz as az\nimport numpy as np\n\nfrom plotnine.data import mtcars\nfrom plotnine import * # elmo_fire.gif namespace, but they mostly start with geom_*\nmtcars.head()\n```\n\n## Model\n\nThis is an attempt at replicating the model [here](https://mjskay.github.io/tidybayes/reference/add_predicted_draws.html).\n\nThe formula is following `mpg ~ hp * cyl` fit with `brms`.\n\n::: {.callout-note}\nThe scope of this wasn't necessarily to walk through creating a pymc model or walk through the workflow of prior predictive checks, diagnostics, etc. I took the priors from `brms::stancode(brms::brm(mpg ~ hp * cyl, data = mtcars))` in R. I also used `PyMC` over `Bambi`, but both libraries work off the `arviz.InferenceData` object.\n:::\n\n```{python}\n\n# build model and sample posterior\nwith pm.Model(\n    coords={\"obs\": mtcars.index, \"predictors\": ['hp', 'cyl']}\n) as mod:\n\n    X = pm.Data(\"X\", mtcars[[\"hp\", \"cyl\"]], dims=(\"obs\", \"predictors\"))\n\n    alpha = pm.StudentT(\"alpha\", nu=3, mu=19.2, sigma=5.4)\n    sigma = pm.HalfStudentT(\"sigma\", nu=3, sigma=5.54)\n    beta = pm.Normal(\"b\", mu=0, sigma=1, dims='predictors')\n\n    mu = pm.Deterministic(\"mu\", alpha + pm.math.dot(X, beta), dims='obs')\n\n    y = pm.Normal(\n        \"y\",\n        mu=mu,\n        sigma=sigma,\n        shape=X.shape[0],\n        observed=mtcars[\"mpg\"],\n        dims=\"obs\",\n    )\n\n    idata = pm.sample(random_seed=527) # got an anniversary comming up\n\n# sample posterior predictive\nwith mod as model:\n    pp = pm.sample_posterior_predictive(idata, predictions=True)\n```\n\n### Tidy up\n\nThe biggest takeaway I had from Vincent's post was it wsa possible to get tidy data out of the `arviz.InferenceData` object, and this was by far the most difficult part to get my head around.\n\nFrom `idata.posterior`, we'll take three things:\n\n- global parameters: `sigma`, `alpha`\n- parameters `beta` (2)\n- linear predictions `mu`\n\n\n\nThe key is to understand the dimensions of which attribute you want ot get and which ones are the same.\n\n```{python}\nprint(f'alpha: {idata.posterior[\"alpha\"].shape}')\nprint(f'sigma: {idata.posterior[\"sigma\"].shape}')\nprint(f'beta: {idata.posterior[\"b\"].shape}')\nprint(f'mu: {idata.posterior[\"mu\"].shape}')\n\n```\n\n- Both `alpha` and `sigma` are the same shape becuause they are global parameters.\n- `beta` has the same number of draws, each is represented as a row that will pivot\n- `mu` has the same number of draws but for each observation\n\n```{python}\nparams = idata.posterior[[\"sigma\", \"alpha\"]].to_dataframe().reset_index()\nbetas = (\n    idata.posterior[\"b\"]\n    .to_dataframe()\n    .reset_index()\n    .pivot(index=[\"chain\", \"draw\"], columns=\"predictors\", values=\"b\")\n    .reset_index()\n)\n\ndf_posterior = params.merge(betas, on=[\"chain\", \"draw\"])\n\ndf_posterior = (\n    idata.posterior[\"mu\"]\n    .to_dataframe()\n    .reset_index()\n    .merge(mtcars[[\"cyl\", \"mpg\", \"hp\"]], left_on=\"obs\", right_on=mtcars.index)\n    .merge(params, on=[\"chain\", \"draw\"])\n    .merge(betas, on=[\"chain\", \"draw\"], suffixes=[\"\", \"_b\"])\n    .assign( # for plotting later\n        group=lambda x: x.cyl.astype(str)\n        + \"_\"\n        + x.draw.astype(str)\n        + \"_\"\n        + x.chain.astype(str)\n    )\n)\n\ndf_posterior\n```\n\nThe critical takeaway I had from implementing this was learn to leverage `coords` and `dims` in the model container. This makes it easier to work with the data later, especially as dimensions increase (i.e. groups in a multilevel model).\n\nThis sets the posterior to be represented in a tidy dataframe, exactly how Vincent did it. I'm really interested what ways we can cofigure the atributes with `PyMC` to make this generalize across different models and data.\n\n### Posterior predictive\n\nOne way would be to do this would be to use `arviz.summary()` on the sampled posterior predictions. This is a common workflow I would do with `brms` and `tidybayes` of parsing parameter outputs name to match the group, or join an id with the original dataset.\n\n```{python}\ndf_predictions = az.summary(pp)\n```\n\nNext steps to join it with the observed data.\n\n```{python}\ndf_predictions.index = df_predictions.index.str.extract(r\"y\\[(.*?)\\]\")[0]\n\ndf_predictions = df_predictions.merge(mtcars[[\"hp\", \"cyl\", \"mpg\"]], on=df_predictions.index)\ndf_predictions.head()\n```\n\nThis worked well with the named index on `mtcars`. I'm not a fan of `pandas`, and I've long forgotten a lot of tips and tricks to work with the nuances of `pandas` after a couple of years of using `polars`. For future me, I'm going to include a standarad approach of working with the posterior.\n\n```{python}\n\ndf_posterior_predictive = (\n    pp.predictions[\"y\"]\n    .to_dataframe()\n    .reset_index()\n    .merge(mtcars[[\"cyl\", \"hp\"]], left_on=\"obs\", right_on=mtcars.index)\n)\n\ndf_posterior_predictive\n\n```\n\nThe data is aggregated to match the `az.summary()` output since this particular `geom_ribbon()` visualization will only need the HDI values of the posterior predictive distribution.\n\n```{python}\ndf_predictions = (\n    df_posterior_predictive.groupby([\"obs\", \"cyl\", \"hp\"])\n    .agg(\n        pp_mean=(\"y\", \"mean\"),\n        pp_min=(\"y\", lambda x: x.quantile(0.03)),\n        pp_max=(\"y\", lambda x: x.quantile(0.97)),\n    )\n    .reset_index()\n)\ndf_predictions.head()\n```\n\n### The plot is coming together\n\nPlotnine! With the grammar of graphics, we're able to:\n\n- use different datasets\n- layer aesthetics together\n- think about plots with data\n\n\n```{python}\n\n# sample draws for plotting purposes\nsamples = np.random.choice(\n    [x for x in range(999)], size=int(5), replace=False\n)\n\n(\n    ggplot(mtcars, aes(\"hp\", \"mpg\", color=\"factor(cyl)\", fill=\"factor(cyl)\"))\n    + geom_ribbon(\n        aes(y=\"pp_mean\", ymin=\"pp_min\", ymax=\"pp_max\"), data=df_predictions, alpha=0.2\n    )\n    + geom_line(\n        aes(y=\"mu\", group=\"group\"),\n        data=df_posterior[df_posterior.draw.isin(samples)],\n        alpha=0.6,\n    )\n    + geom_point()\n    + theme_minimal()\n    + labs(color='cyl', fill='cyl')\n)\n```\n\n\n\n## But Why?\n\nI'm in the middle of a journey to learn Bayesian statsistics.\n\nI can't state enough what great work the community has done to make it open and accessible. I started out, like many others, with [Richard McElreath's Statistical Rethinking](https://xcelab.net/rm/), and then following along with Solomon Kurz's [Statistical rethinking with brms, ggplot, and the tidyverse](https://bookdown.org/content/4857/). Those are R environments but the PyMC community has gret material on getting started and using it for Bayesian Stats.\n\nThere is a steep learning curve on the theory, but also with the tooling. To do modern Bayesian modeling, you need to interact with a probablelistic programmming languages (PPL). The frameworks and libraries are dramatically different depending on the language or environment being used.\n\nIt's daunting! Knowing the theory doesn't necessarily make it easier to use the tool. Learning one PPL framework doesn't mean it will be trivial to move to another. As a beginner, [brms](https://paulbuerkner.com/brms/) and [Bambi](https://bambinos.github.io/bambi/) are excellent libraries that lower the barrier of entry and make the model building process easier. The downside is with so much abstracted away, it can make it difficult to really learn what is going on.\n\nStepping out from `brms` to [Stan](https://mc-stan.org/) was like falling off a cliff. I wanted something that worked better for me, and `PyMC` was a good blend of what I needed in my journey, and to my surprise have liked it a lot. I felt like I made a lot of progress solidifying concepts building models I had made in `brms` to `PyMC`, however I spent most of the time trying to learn how to use the `az.InferenceData` object.\n\nI see a lot of value getting a way to work with tidy datasets, both in learning and in application.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"cosmo","title-block-banner":true,"title":"TidyPyMC","author":"Ryan Plain","date":"2025-05-14","categories":["Bayesian","PyMC","Plotnine"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}