---
title: "Rython"
author: "Ryan Plain"
date: "2025-05-31"
categories: [Bayesian, PyMC, Reticulate]
df-print: kable
description: Bayesian Modeling with PyMC in R with Reticulate.
image: tidybayes_plot.png
---

## Bayesian models (Python) + grammar of graphics (R) = ❤️

The heading is a nod to the title of [Benjamin T. Vincent's blog post](https://drbenvincent.github.io/posts/mcmc_grammar_of_graphics.html) about using the grammar-of-graphics with PyMC, and what inspired me to look into different ways of working with PyMC.

This is also somewhat of a continuation of my [previous post](../2025-05-14-tidy-pymc/), where I used the grammar-of-graphics to vizualize the posterior distributions from a PyMC model. Accomplishing this built on Vincent's work by extracting the data from the high dimensional `arviz.InferenceData` object, and organizing it into a tidy dataframe to work with.

### But why?

In the first post, I implemented the formula incorrectly for the interaction model: `mpg ~ hp * cyl`. The interaction term needed to be explictly added in to look like: `mpg ~ hp + cyl + hp:cyl`. To rectify that I've added the interaction variable to the data passed into the model, which functions like `lm()` or `brms:brm()` would implicitly handle. Additionally, I centered the variables to help prevent unnecessary divergece issues.

Striking the right balance of abstraction in Bayesian frameworks is delicate, and in my view, PyMC does a good job preserving the explicit structure of the data-generating process while abstracting away lower-level details like data types and iteration. Because of this, I find it a great tool for learning. With so much to learn in both the statistical theory and the frameworks of probablistic programming languages, anything to reduce the cognitive load is beneficial. Using the Tidyverse to express declaritve ways to work with data before and after the model fit can help.


::: {.callout-note collapse="true"}
An R user reading this is probably bewildered and still wondering *but why?*  R has access to Stan and several packages that interface with it, including the brilliant `brms` package. Most R users are Stan users.

I started out, like many others, with Statistical Rethinking by Richard McElreath[^sr]. As a longtime R user, the data wrangling was something I comfortable with. Trying to use base R and `rethinking` to work with the posterior ended up adding more friction while learning. I felt comfortable using `brms` and moved off of `rethininking`, but when it came to exploring my own real-world applications[^side-note] -- I felt stuck. There was too much abstraction in `brms` and I didn't know enough to jump into Stan directly. That is where I began to look at PyMC.

McElreath intentionally built `rethinking` to be explicit and the course is designed to hammer in all sorts of regression techniques with it. For new folks diving into the course, I think I would reccommend using `rethinking` with [`tidybayes.rethinking`](https://mjskay.github.io/tidybayes.rethinking/articles/tidy-rethinking.html). `rethinking` will force good fundementals building out the model, and you can still use `tidybayes` to work with the posterior in a tidy format.
:::

### How

This was all doable before with Reticulate, but I recently had a chance to use the [new version of Reticulate](https://posit.co/blog/reticulate-1-41/) which uses `uv` to manage the Python environment. I assumed you would need manage a seperate Python environment to some degree. However, from the post:

> with `py_require()`, Reticulate will automatically create and manage Python environments behind the scenes so you don’t have to.

I used it for a simple package integration, but came away so impressed with its performance -- all while not having to manage anything in Python. Reticulate handles the installation of `uv` if not already available, and will take care of everything else for you.

## Workflow

### Set up with R

Load up `reticulate` and the `tidyverse`.

```{r}
#| warning: false

Sys.setenv(RETICULATE_PYTHON = "managed")
library(reticulate)
library(tidyverse)
mtcars |>
  head()
```

::: {.callout-note collapse="true"}
I set the environment variable for `RETICULATE_PYTHON` to force Reticulate to use an ephemeral environment. I didn't have to do this in an interactive session, but this blog already had a uv proejct setup - and I didn't want it to be used. This could also be configured outside the script or workflow.
:::

### Reticulate and uv environment

The function `reticulate::py_require()` will specify which packages are needed for the project or workflow, and pass them to `uv` to resolve all the dependencies for the Python virtual environment. This is feasible to do ephemerally due to how performant `uv` is.

I've listend to enough talks from Charlie Marsh[^charlie] to know I can't do it justice to explain how `uv` works. An oversimplification of the process is that `uv` centralizes package downloads, and then resolves dependencies at the environment level so that you do not have to reinstall packages across environments.[^uv-overview]

To use it with Reticulate, simply run:

```{r}
py_require('pymc')
py_config() # only if you want to look at the config
```

Wow! You can see that we have an ephemeral Python environment created with `uv` to be used with `reticulate`. Everytime this document is rendered, `reticulate` and `uv` will cache a new virtual environment to use. PyMC and all of its dependencies only had to be downloaded once. Not shown here but via CLI is that this process only took `456ms` on this machine.

Specifying `pymc` with `py_require()` actually built a Python environment with a list of packages needed, all mapped and configured with `uv`. I've shown some of the most well-known dependencies included.

```{r}
py_list_packages() |> dim()
py_list_packages() |> dplyr::filter(package %in% c('pandas', 'scipy', 'matplotlib', 'arviz', 'pytensor'))
```

### Set up the data

As I mentioned before some basic transformations are needed, and it is great to be able to use the Tidyverse here. You can imagine a set of more complex joins, filters, and features created.

```{r}

mtcars_scaled <- mtcars %>%
  mutate(
    hp_c = scale(hp)[, 1], # scale() keeps attributes that need to be removed
    cyl_c = scale(cyl)[, 1],
    hp_cyl = hp_c * cyl_c
  )
```

### R to Python

PyMC doesn't work with R, and we will need objects and data types that it knows how to use. `reticulate::r_to_py()` will handle that.

```{r}
pandas_mtcars <- r_to_py(mtcars_scaled)
print(class(mtcars_scaled))
print(class(pandas_mtcars))
```

There are now two datasets:

- **`mtcars_scaled`** is an R `data.frame()` object
- **`pandas_mtcars`** is a Python `pandas.DataFrame()` object

We can now begin the Python portion of the workflow.

::: {.callout-warning}
Integrating Python and R has come a long way, and is incredibly accessible. There are some edge cases and things to be aware of when converting data and objects between the two. [This post by Karin Hrovatin](https://hrovatin.github.io/posts/r_python/) is one of the best consolidated sources of information to learn from.
:::


### PyMC Model

The Python sytax would look very similar to this, with one of the main changes being instead of using dot notation, methods and attributes are accessed with the `$` character. If you are purely a Python developer, this might look obscene. This post requires a specifc subset of being interested in using PyMC and R together and might only be relevant for a sample of n=1, but the 1 is me.

Python code from before.

```{python}
#| eval: false
#| code-fold: true
#|
with pm.Model(
    coords={"obs": mtcars.index, "predictors": ['hp', 'cyl']}
) as mod:

    X = pm.Data("X", mtcars[["hp", "cyl"]], dims=("obs", "predictors"))

    alpha = pm.StudentT("alpha", nu=3, mu=19.2, sigma=5.4)
    sigma = pm.HalfStudentT("sigma", nu=3, sigma=5.54)
    beta = pm.Normal("b", mu=0, sigma=1, dims='predictors')

    mu = pm.Deterministic("mu", alpha + pm.math.dot(X, beta), dims='obs')

    y = pm.Normal(
        "y",
        mu=mu,
        sigma=sigma,
        shape=X.shape[0],
        observed=mtcars["mpg"],
        dims="obs",
    )

    idata = pm.sample(random_seed=527)

# sample posterior predictive
with mod as model:
    pp = pm.sample_posterior_predictive(idata, predictions=True)
```


PyMC, but using Reticulate.

```{r}
# import pymc as pm
pm <- import('pymc', convert = FALSE)

mod <- pm$Model(
  coords = list(
    car = pandas_mtcars$index,
    predictors = c('hp', 'cyl', 'hp_cyl')
  )
)

# with pm.Model() as model:
# ...
with(mod, {
  X <- pm$Data('X', pandas_mtcars$loc[, c('hp', 'cyl', 'hp_cyl')])

  alpha <- pm$StudentT("alpha", nu = 3, mu = 19.2, sigma = 5.4)
  sigma <- pm$HalfStudentT("sigma", nu = 3, sigma = 5.54)
  beta <- pm$Normal("b", mu = 0, sigma = 1, dims = 'predictors')
  mu <- pm$Deterministic("mu", alpha + pm$math$dot(X, beta), dims = 'car')

  y <- pm$Normal(
    "y",
    mu = mu,
    sigma = sigma,
    shape = X$shape[0], # python index
    observed = pandas_mtcars$mpg,
    dims = "car",
  )
  # using a single core and chain because of Quarto page rendering,
  # normally this would be 4 chains
  idata = pm$sample(random_seed = 527L, cores = 1L, chains=4L)
})

with(mod, {
  pp = pm$sample_posterior_predictive(idata, predictions = TRUE)
})
```

### Gotchas

Some quirks to be aware of:

- **`pandas_mtcars$loc[, c('hp', 'cyl', 'hp_cyl')]`**
  Uses the `.loc` method from pandas, but with R-style dataframe indexing syntax.

- **`X$shape[0]`**
  Python is 0-based indexed while R is 1-based. Since `X` is a Python object, we use `0` for indexing.

- **`random_seed = 527L`**
  Integer literals in R require `L` to indicate an integer type, which Python expects here.


### Diagnostics

As you can see, any of the [ArviZ](https://python.arviz.org/en/stable/index.html) plots can be used.

My heart sank a bit when I finally thought about including this in, as I had already written most of the post and was adding some finishing touches. If diagnostic plots weren't available, this would all either be useless or require the very problem I sought out to destroy -- duplicating efforts across environments. I didn't become Anakin, an this is great to see!

```{r}
#| eval: false
az <- import('arviz', convert = FALSE)
plt <- import("matplotlib.pyplot", convert = FALSE)
az$plot_trace(idata, var_names = c('alpha', 'b', 'sigma'))
plt$show()

```

![](trace_plot.png)



### Posterior

Translating the PyMC model's `arviz.InferenceData` object and posterior predictions to dataframes is still the same, with the addition of `reticulate::py_to_r()` to convert a pandas dataframe to R.


```{r}
# posterior mu
# reset index to include chain, draw, and index
df_posterior <- idata["posterior"]['mu']$to_dataframe()$reset_index() |>
  py_to_r() |>
  as_tibble() |>
  left_join(rownames_to_column(mtcars, 'car')) |> # R mtcars has rownames for the car
  mutate(group = paste0(chain, draw, cyl)) # for a particular plot later

# posterior predictions of mpg
# can also access az.InferenceData objects with `$`
df_predictions <- pp$predictions$to_dataframe()$reset_index() |>
  py_to_r() |>
  as_tibble() |>
  left_join(rownames_to_column(mtcars, "car"))

```


We're completely back in R, doing joins with the Tidyverse!

### Plot

Now for my favorite part of this, plot with **ggplot2** and use [tidybayes](https://mjskay.github.io/tidybayes/reference/add_predicted_draws.html) directly!


```{r}

df_predictions |>
  ggplot(aes(hp, y, color = as.factor(cyl))) +
  tidybayes::stat_lineribbon(.width = c(.99, .95, .8, .5), alpha = 0.25) +
  geom_line(
    aes(y = mu, group = group),
    data = df_posterior |> filter(draw %in% round(seq(5, 900, length.out = 5))),
    alpha = 0.38
  ) +
  geom_point(aes(y = mpg), data = mtcars, shape = 21, size = 2, stroke = 1) +
  scale_fill_brewer(palette = "Greys") +
  theme_light(base_size = 12) +
  guides(fill = 'none') +
  labs(
    x = 'hp',
    y = 'mpg',
    color = 'cyl'
  )

```



## Rython

This is an opinionated way of using PyMC and the grammar-of-graphics together to say the least. I really do like PyMC, but I prefer to settle on the data and other parts of the model iteration process with R if possible. There is potential for `reticulate::py_run_string()` as well, if you wanted to be able to drop it directly back into a pure Python environment. Any AI model would also be able to easily reformat the R-PyMC model to Python, or at least get it most of the way there.

I'm genuinely impressed by how far integrating R and Python has come. When I started my career, you had to do a bunch of clunky I/O to get features of both languages. Now, in a single workflow, I can use a full-featured Python probabilistic programming library alongside R’s non-standard evaluation for data transformation and visualization.

A typo I had in drafting this at one point was *Rython*, and given my name... I quite like it.


[^charlie]: Charlie Marsh is the lead developer of Ruff and discussed `uv` in multiple talks.

[^uv-overview]: `uv` is a Rust-based Python package manager that installs dependencies in a global cache and reuses them in isolated environments, improving reproducibility and speed. For more, see the [uv docs](https://docs.astral.sh/uv/).

[^standard-scaler]: StandardScaler from sci-kit learn does this in the Python, so scaling isn't necessarily the point.

[^sr]: Stistical Rethinking https://xcelab.net/rm/
