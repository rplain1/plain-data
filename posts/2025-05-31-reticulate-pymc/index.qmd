---
title: "Rython"
author: "Ryan Plain"
date: "2025-05-31"
categories: [Bayesian, PyMC, Reticulate]
df-print: kable
description: Bayesian Modeling with PyMC and Reticulate in R.
---

## Bayesian models (Python) + grammar of graphics (R) = ❤️

This is a nod to the title of [Benjamin T. Vincent's blog post](https://drbenvincent.github.io/posts/mcmc_grammar_of_graphics.html), who inspired me to dive further into PyMC. The reason I started looking into using the grammar of graphics with PyMC, was to reduce a barrier and emulate **tidyverse** as much as possible. What if I instead just... use R and PyMC?

In my [previous post](../2025-05-14-tidy-pymc/), I used the Grammar of Graphics to vizualize the posterior distributions from a PyMC model. This required extracting the data from the `az.InferenceData` object, and organizing into a tidy dataframe to work with.

I recently got a chance to use the [new version of reticulate](https://posit.co/blog/reticulate-1-41/), which uses `uv` to manage the Python environment used in an R session, and fell in love. From the post:

> with `py_require()`, Reticulate will automatically create and manage Python environments behind the scenes so you don’t have to.

This workflow was available prior to the new version of Reticulate, but it is now incredibly simplified with `uv`.

## Workflow

### Set up the environment

Load up `reticulate` and the `tidyverse`.

::: {.callout-note collapse="true"}
I set the environment variable for `RETICULATE_PYTHON` to force Reticulate to use an ephemeral environment. I didn't have to do this in an interactive session, but this blog already had a uv proejct setup - and I didn't want it to be used. This could also be configured outside the script or workflow.
:::

```{r}
#| warning: false

Sys.setenv(RETICULATE_PYTHON = "managed")
library(reticulate)
library(tidyverse)
mtcars |>
  head()
```

The first difference in this workflow is that `mtcars` is a dataset available by default, and can be accessed directly.

### Reticulate and uv environment

I've listend to enough presentations and interviews from Charlie Marsh[^charlie] to know I can't fully explain what `uv` does. it centralizes downloads and resolves dependencies once, so you don't reinstall packages across environments.[^uv-overview]


Using `reticulate::py_require()` will specify which packages are needed and how to create a virtual environment.

```{r}
py_require('pymc')
py_config()
```

You can see that we have an ephemeral python environment created with `uv` to use with `reticulate`. This is really neat! Everytime I render this document, it will cache a new virtual environment for `reticulate` to use. Thanks to uv’s global package cache, I only had to download pymc once—even across multiple isolated environments.

```{r}
# > py_list_packages() |> dim()
# [1] 63  3

py_list_packages() |> dplyr::filter(package %in% c('pandas', 'scipy', 'matplotlib', 'arviz'))
```

Specifying the `pymc` dependency actually built a Python environment with 63 packages, all mapped and configured with `uv`. I've shown some of the most well-known dependencies of `pymc`.

### Set up the data

In my last post, I incorrectly implemented the formula for the interaction model `mpg ~ hp * cyl`. I needed to explictly add in a variable so that we have `mpg ~ hp + cyl + hp:cyl`. To correct that, I've added a variable that `lm()` or `brms` would typically do under the hood. Additionally added in centering the variables to help prevent divergece issues.

A use case for me to use R in the workflow is to be able to do the initial data wrangling with the `tidyverse`. In this example, I used `dplyr::mutate()` to scale the variables. You can imagine other types of transformations, filters, and joins that could be dropped in here.[^standard-scaler]

```{r}

mtcars_scaled <- mtcars %>%
  mutate(
    hp_c = scale(hp)[, 1], # scale() keeps attributes that need to be removed
    cyl_c = scale(cyl)[, 1],
    hp_cyl = hp_c * cyl_c
  )
```

### R to Python

```{r}
pandas_mtcars <- r_to_py(mtcars_scaled)
print(class(mtcars_scaled))
print(class(pandas_mtcars))
```

There are now two datasets:

- **`mtcars_scaled`** is an R `data.frame()` object
- **`pandas_mtcars`** is a Python `pandas.DataFrame()` object

The pandas dataframe can be passed into PyMC and begin the Python portion of the workflow.

::: {.callout-warning}
Integrating Python and R has come a long way, and is incredibly accessible. There are some edge cases and things to be aware of when converting data and objects between the two. [This post by Karin Hrovatin](https://hrovatin.github.io/posts/r_python/) is one of the best consolidated sources of information to learn from.
:::


### PyMC Model

The python sytax would look very similar to this, only now instead of using objects with dot notation, we access methods and attributes with the `$` character. If you are purely a python developer, this might look obscene. I choose to put up with this quirkyness because I find working with dataframes and plotting in R worth the trade off.

Python code from before.
```{python}
#| eval: false
#| code-fold: true
#|
with pm.Model(
    coords={"obs": mtcars.index, "predictors": ['hp', 'cyl']}
) as mod:

    X = pm.Data("X", mtcars[["hp", "cyl"]], dims=("obs", "predictors"))

    alpha = pm.StudentT("alpha", nu=3, mu=19.2, sigma=5.4)
    sigma = pm.HalfStudentT("sigma", nu=3, sigma=5.54)
    beta = pm.Normal("b", mu=0, sigma=1, dims='predictors')

    mu = pm.Deterministic("mu", alpha + pm.math.dot(X, beta), dims='obs')

    y = pm.Normal(
        "y",
        mu=mu,
        sigma=sigma,
        shape=X.shape[0],
        observed=mtcars["mpg"],
        dims="obs",
    )

    idata = pm.sample(random_seed=527)

# sample posterior predictive
with mod as model:
    pp = pm.sample_posterior_predictive(idata, predictions=True)
```


Now the same thing, but using `reticulate` to interface with `pymc`. I've also added in the new column `hp_cyl` for the interaction term.

```{r}
# import pymc as pm
pm <- import('pymc', convert = FALSE)

mod <- pm$Model(
  coords = list(
    car = pandas_mtcars$index,
    predictors = c('hp', 'cyl', 'hp_cyl')
  )
)

# with pm.Model() as model:
# ...
with(mod, {
  X <- pm$Data('X', pandas_mtcars$loc[, c('hp', 'cyl', 'hp_cyl')])

  alpha <- pm$StudentT("alpha", nu = 3, mu = 19.2, sigma = 5.4)
  sigma <- pm$HalfStudentT("sigma", nu = 3, sigma = 5.54)
  beta <- pm$Normal("b", mu = 0, sigma = 1, dims = 'predictors')
  mu <- pm$Deterministic("mu", alpha + pm$math$dot(X, beta), dims = 'car')

  y <- pm$Normal(
    "y",
    mu = mu,
    sigma = sigma,
    shape = X$shape[0], # python index
    observed = pandas_mtcars$mpg,
    dims = "car",
  )
  # using a single core and chain because of Quarto page rendering,
  # normally this would be 4 chains
  idata = pm$sample(random_seed = 527L, cores = 1L, chains=1L)
})

with(mod, {
  pp = pm$sample_posterior_predictive(idata, predictions = TRUE)
})
```

### Gotchas

Some quirks to be aware of:

- **`pandas_mtcars$loc[, c('hp', 'cyl', 'hp_cyl')]`**
  Uses the `.loc` method from pandas, but with R-style dataframe indexing syntax.

- **`X$shape[0]`**
  Python is 0-based indexed while R is 1-based. Since `X` is a Python object, we use `0` for indexing.

- **`random_seed = 527L`**
  Integer literals in R require `L` to indicate an integer type, which Python expects here.


### Posterior

Translating the PyMC model's `az.InferenceData` object and posterior predictions to dataframes is still the same, with the addition of `py_to_r()` to convert a pandas dataframe to R.


```{r}
# posterior mu
df_posterior <- idata$posterior$mu$to_dataframe()$reset_index() |>
  py_to_r() |>
  as_tibble() |>
  left_join(rownames_to_column(mtcars, 'car')) |> # R mtcars has rownames for the car
  mutate(group = paste0(chain, draw, cyl)) # for a particular plot later

# posterior predictions of mpg
df_predictions <- pp$predictions$to_dataframe()$reset_index() |>
  py_to_r() |>
  as_tibble() |>
  left_join(rownames_to_column(mtcars, "car"))

```

### Plot

Now for my favorite part of this, plot with **ggplot2** and use [tidybayes](https://mjskay.github.io/tidybayes/reference/add_predicted_draws.html) directly!


```{r}

df_predictions |>
  ggplot(aes(hp, y, color = as.factor(cyl))) +
  tidybayes::stat_lineribbon(.width = c(.99, .95, .8, .5), alpha = 0.25) +
  geom_line(
    aes(y = mu, group = group),
    data = df_posterior |> filter(draw %in% round(seq(5, 900, length.out = 5))),
    alpha = 0.38
  ) +
  geom_point(aes(y = mpg), data = mtcars, shape = 21, size = 2, stroke = 1) +
  scale_fill_brewer(palette = "Greys") +
  theme_light(base_size = 12) +
  guides(fill = 'none') +
  labs(
    x = 'hp',
    y = 'mpg',
    color = 'cyl'
  )

```


## Rython

This is an opinionated way of using PyMC and the grammar of graphics together to say the least. I really do like PyMC, but I prefer to settle on the data and other parts of the model iteration process with R if possible. There is potential for `reticulate::py_run_string()` as well, if you wanted to be able to drop it directly back into a pure Python environment. Access to an LLM would also be able to easily reformat the R-PyMC model to Python, or at least get it most of the way there.

I'm genuinely impressed by how far integrating R and Python has come. When I started my career, you had to do a bunch of clunky I/O to get features of both languages. Now, in a single workflow, I can use a full-featured Python probabilistic programming library alongside R’s non-standard evaluation for data transformation and visualization.

A typo I had in drafting this at one point was *Rython*, and given my name... and I quite like it.


[^charlie]: Charlie Marsh is the lead developer of Ruff and discussed `uv` in multiple talks.

[^uv-overview]: `uv` is a Rust-based Python package manager that installs dependencies in a global cache and reuses them in isolated environments, improving reproducibility and speed. For more, see the [uv docs](https://docs.astral.sh/uv/).

[^standard-scaler]: StandardScaler from sci-kit learn does this in the python, so scaling isn't necessarily the point.
