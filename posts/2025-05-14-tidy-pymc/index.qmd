---
title: "TidyPyMC"
author: "Ryan Plain"
date: "2025-05-14"
categories: [Bayesian, PyMC, Plotnine]
engine: jupyter
---

## Tidybayes in Python would be cool

A few weeks ago, Benjamin Vincent posted [this blog post](https://drbenvincent.github.io/posts/mcmc_grammar_of_graphics.html) on using Bayesian models in Python _and_ leveraging the grammar of graphics for plotting. Please take the time to read that post as this is derived and inteded to augment the ideas shared there.

At the end of the post, Vincent asked "It would be interesting to see if this approach is appealing to people." My answer to that is... YES‼️

I like `PyMC` and `ArviZ` a lot, but it was a huge blow coming from `R` and libraries like `tidybayes`, `bayesplots`, and others that helped wrangle and visualize the posterior.

I fully agree with the approach of `ArviZ` to work with high-dimensional data, but comming from a stats background it is more intuitive to work with things as dataframes rather than objects whenever it makes sense. This is especially true with visualizations and the grammar of graphics.

I'm not sure how or what the best way to contribute to this, and it was mentioned on bluesky that [GoG-like interface is being developped for ArviZ 1.0](https://bsky.app/profile/sethaxen.com/post/3loaw2tpucs2v). The best thing I can do is create a post for me.


## TidyPyMC

This is definitely subjective, but I think the missing commponent right now is a consistent way to turn the `arviz.InferenceData` object into a dataframe. Both this and the code Vincent shared is highly custom to the model, and from my experience that is typically a common design choice between R and Python libraries. There is tradeoffs to both paradigms.

There are a couple of plots in `tidybayes` [add_epred_draws()](https://mjskay.github.io/tidybayes/reference/add_predicted_draws.html) and [add_predicted_draws()](https://mjskay.github.io/tidybayes/reference/add_predicted_draws.html) that show some of its capabilities. The goal of this is to replicate them.

To accomplish this, we will bring the observed data, linear predictions, and posterior predictions in the same dataframe.

### Libraries and data

We'll use the `mtcars` dataset to replicate some of the `tidybayes` examples.

```{python}
import pandas as pd
import pymc as pm
import arviz as az
import numpy as np

from plotnine.data import mtcars
from plotnine import * # elmo_fire.gif namespace, but they mostly start with geom_*
mtcars.head()
```

## Model

This is an attempt at replicating the model [here](https://mjskay.github.io/tidybayes/reference/add_predicted_draws.html).

The formula is following `mpg ~ hp * cyl` fit with `brms`.

::: {.callout-note}
The scope of this wasn't necessarily to walk through creating a pymc model or walk through the workflow of prior predictive checks, diagnostics, etc. I took the priors from `brms::stancode(brms::brm(mpg ~ hp * cyl, data = mtcars))` in R. I also used `PyMC` over `Bambi`, but both libraries work off the `arviz.InferenceData` object.
:::

```{python}

# build model and sample posterior
with pm.Model(
    coords={"obs": mtcars.index, "predictors": ['hp', 'cyl']}
) as mod:

    X = pm.Data("X", mtcars[["hp", "cyl"]], dims=("obs", "predictors"))

    alpha = pm.StudentT("alpha", nu=3, mu=19.2, sigma=5.4)
    sigma = pm.HalfStudentT("sigma", nu=3, sigma=5.54)
    beta = pm.Normal("b", mu=0, sigma=1, dims='predictors')

    mu = pm.Deterministic("mu", alpha + pm.math.dot(X, beta), dims='obs')

    y = pm.Normal(
        "y",
        mu=mu,
        sigma=sigma,
        shape=X.shape[0],
        observed=mtcars["mpg"],
        dims="obs",
    )

    idata = pm.sample(random_seed=527)

# sample posterior predictive
with mod as model:
    pp = pm.sample_posterior_predictive(idata, predictions=True)
```

### Tidy up

The biggest takeaway I had from Vincent's post was it wsa possible to get tidy data out of the `arviz.InferenceData` object, and this was by far the most difficult part to get my head around.

From `idata.posterior`, we'll take three things:

- global parameters: `sigma`, `alpha`
- parameters `beta` (2)
- linear predictions `mu`



The key is to understand the dimensions of which attribute you want ot get and which ones are the same.

```{python}
print(f'alpha: {idata.posterior["alpha"].shape}')
print(f'sigma: {idata.posterior["sigma"].shape}')
print(f'beta: {idata.posterior["b"].shape}')
print(f'mu: {idata.posterior["mu"].shape}')

```

- Both `alpha` and `sigma` are the same shape becuause they are global parameters.
- `beta` has the same number of draws, each is represented as a row that will pivot
- `mu` has the same number of draws but for each observation

```{python}
params = idata.posterior[["sigma", "alpha"]].to_dataframe().reset_index()
betas = (
    idata.posterior["b"]
    .to_dataframe()
    .reset_index()
    .pivot(index=["chain", "draw"], columns="predictors", values="b")
    .reset_index()
)

df_posterior = params.merge(betas, on=["chain", "draw"])

df_posterior = (
    idata.posterior["mu"]
    .to_dataframe()
    .reset_index()
    .merge(mtcars[["cyl", "mpg", "hp"]], left_on="obs", right_on=mtcars.index)
    .merge(params, on=["chain", "draw"])
    .merge(betas, on=["chain", "draw"], suffixes=["", "_b"])
    .assign( # for plotting later
        group=lambda x: x.cyl.astype(str)
        + "_"
        + x.draw.astype(str)
        + "_"
        + x.chain.astype(str)
    )
)

df_posterior
```

The critical takeaway I had from implementing this was learn to leverage `coords` and `dims` in the model container. This makes it easier to work with the data later, especially as dimensions increase (i.e. groups in a multilevel model).

This sets the posterior to be represented in a tidy dataframe, exactly how Vincent did it. I'm really interested what ways we can cofigure the atributes with `PyMC` to make this generalize across different models and data.

::: {.callout-note}
I've come back to this and realized adding in the parameters doesn't make it "tidy", and also doesn't get used in the plots. It is still beneficial to include how to go about joining the parameters posterior draws to the data, and anyway... this post is for future me.
:::

### Posterior predictive

One way would be to do this would be to use `arviz.summary()` on the sampled posterior predictions. This is a common workflow I would do with `brms` and `tidybayes` of parsing parameter outputs name to match the group, or join an id with the original dataset.

```{python}
df_predictions = az.summary(pp)
```

Next steps to join it with the observed data.

```{python}
df_predictions.index = df_predictions.index.str.extract(r"y\[(.*?)\]")[0]

df_predictions = df_predictions.merge(mtcars[["hp", "cyl", "mpg"]], on=df_predictions.index)
df_predictions.head()
```

This worked well with the named index on `mtcars`. I'm not a fan of `pandas`, and I've long forgotten a lot of tips and tricks to work with the nuances of `pandas` after a couple of years of using `polars`. For future me, I'm going to include a standarad approach of working with the posterior.

```{python}

df_posterior_predictive = (
    pp.predictions["y"]
    .to_dataframe()
    .reset_index()
    .merge(mtcars[["cyl", "hp"]], left_on="obs", right_on=mtcars.index)
)

df_posterior_predictive

```

The data is aggregated to match the `az.summary()` output since this particular `geom_ribbon()` visualization will only need the HDI values of the posterior predictive distribution.

```{python}
df_predictions = (
    df_posterior_predictive.groupby(["obs", "cyl", "hp"])
    .agg(
        pp_mean=("y", "mean"),
        pp_min=("y", lambda x: x.quantile(0.03)),
        pp_max=("y", lambda x: x.quantile(0.97)),
    )
    .reset_index()
)
df_predictions.head()
```

### The plot is coming together

Plotnine! With the grammar of graphics, we're able to:

- use different datasets
- layer aesthetics together
- think about plots in terms of data


```{python}

# sample draws for plotting purposes
samples = np.random.choice(
    [x for x in range(999)], size=int(5), replace=False
)

(
    ggplot(mtcars, aes("hp", "mpg", color="factor(cyl)", fill="factor(cyl)"))
    + geom_ribbon(
        aes(y="pp_mean", ymin="pp_min", ymax="pp_max"), data=df_predictions, alpha=0.2
    )
    + geom_line(
        aes(y="mu", group="group"),
        data=df_posterior[df_posterior.draw.isin(samples)],
        alpha=0.6,
    )
    + geom_point()
    + theme_minimal()
    + labs(color='cyl', fill='cyl')
)
```
