[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Just Plain Data",
    "section": "",
    "text": "TidyPyMC\n\n\n\n\n\n\n\nBayesian\n\n\nPyMC\n\n\nPlotnine\n\n\n\n\n\n\n\n\n\n\n\nMay 14, 2025\n\n\nRyan Plain\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "TidyPyMC",
    "section": "",
    "text": "A few weeks ago, Benjamin Vincent posted this blog post on using Bayesian models in Python and leveraging the grammar of graphics for plotting. Please take the time to read that post as this is derived and inteded to augment the ideas shared there.\nAt the end of the post, Vincent asked “It would be interesting to see if this approach is appealing to people.” My answer to that is… YES‼️\nI like PyMC and ArviZ a lot, but it was a huge blow coming from R and libraries like tidybayes, bayesplots, and others that helped wrangle and visualize the posterior.\nI fully agree with the approach of ArviZ to work with high-dimensional data, but comming from a stats background it is more intuitive to work with things as dataframes rather than objects whenever it makes sense. This is especially true with visualizations and the grammar of graphics.\nI’m not sure how or what the best way to contribute to this, and it was mentioned on bluesky that GoG-like interface is being developped for ArviZ 1.0. The best thing I can do is create a post for me."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About that"
  },
  {
    "objectID": "posts/post-with-code/index.html#tidybayes-in-python-would-be-cool",
    "href": "posts/post-with-code/index.html#tidybayes-in-python-would-be-cool",
    "title": "TidyPyMC",
    "section": "",
    "text": "A few weeks ago, Benjamin Vincent posted this blog post on using Bayesian models in Python and leveraging the grammar of graphics for plotting. Please take the time to read that post as this is derived and inteded to augment the ideas shared there.\nAt the end of the post, Vincent asked “It would be interesting to see if this approach is appealing to people.” My answer to that is… YES‼️\nI like PyMC and ArviZ a lot, but it was a huge blow coming from R and libraries like tidybayes, bayesplots, and others that helped wrangle and visualize the posterior.\nI fully agree with the approach of ArviZ to work with high-dimensional data, but comming from a stats background it is more intuitive to work with things as dataframes rather than objects whenever it makes sense. This is especially true with visualizations and the grammar of graphics.\nI’m not sure how or what the best way to contribute to this, and it was mentioned on bluesky that GoG-like interface is being developped for ArviZ 1.0. The best thing I can do is create a post for me."
  },
  {
    "objectID": "posts/post-with-code/index.html#how-it-started",
    "href": "posts/post-with-code/index.html#how-it-started",
    "title": "TidyPyMC",
    "section": "How it started",
    "text": "How it started\nTL;DR here is the code\n\n\nCode\nimport polars as pl\nimport pymc as pm\nimport arviz as az\nimport numpy as np\n\nfrom plotnine import ggplot, aes, geom_point, geom_ribbon, theme_minimal, labs\n\nd = pl.read_csv(\n    \"https://raw.githubusercontent.com/dustinstansbury/statistical-rethinking-2023/refs/heads/main/data/Howell1.csv\",\n    separator=\";\",\n).filter(pl.col(\"age\") &gt;= 18)\n\nwith pm.Model() as model:\n    weight = pm.Data(\"weight\", d[\"weight\"], dims=[\"obs_id\"])\n\n    sigma = pm.Uniform(\"sigma\", lower=0, upper=50)\n    alpha = pm.Normal(\"alpha\", mu=178, sigma=20)\n\n    beta = pm.LogNormal(\"beta\", mu=0, sigma=1)\n\n    mu = alpha + beta * (weight - weight.mean())\n    y = pm.Normal(\n        \"y\",\n        mu=mu,\n        sigma=sigma,\n        observed=d[\"height\"],\n        shape=weight.shape,\n        dims=[\"obs_id\"],\n    )\n\n    idata = pm.sample()\n\n\nnew_weight = [46.95, 43.72, 64.78, 32.59, 54.63]\nwith model:\n    pm.set_data({\"weight\": new_weight})\n    pp = pm.sample_posterior_predictive(idata, predictions=True, var_names=[\"y\"])\n\nposterior = idata.posterior.to_dataframe().reset_index()\n\nsamples_beta = [\n    np.random.choice(posterior[\"beta\"], size=1000, replace=True).tolist()\n    for _ in new_weight\n]\nsamples_alpha = [\n    np.random.choice(posterior[\"alpha\"], size=1000, replace=True).tolist()\n    for _ in new_weight\n]\n\ndf = pl.DataFrame({\"weight\": new_weight, \"beta\": samples_beta, \"alpha\": samples_alpha})\n\n\ndf_post_epred = df.explode([\"beta\", \"alpha\"]).with_columns(\n    epred=pl.col(\"alpha\") + (pl.col(\"beta\") * (pl.col(\"weight\") - d[\"weight\"].mean()))\n)\ndf_base = df_post_epred.group_by(\"weight\").mean()\n\ndf_post_pred = pl.from_pandas(\n    pp.predictions.to_dataframe().reset_index()\n    # .assign(weight=lambda x: x.obs_id.map(dict(enumerate(new_weight))))\n).with_columns(\n    pl.col(\"obs_id\")\n    .cast(pl.Float64)\n    .replace(dict(enumerate(new_weight)))\n    .alias(\"weight\")\n)\n\n(\n    ggplot()\n    + geom_ribbon(\n        aes(x=\"weight\", ymin=\"y_min\", ymax=\"y_max\", fill=\"type\"),\n        data=df_post_pred.group_by(\"weight\")\n        .agg(y_min=pl.col(\"y\").min(), y_max=pl.col(\"y\").max())\n        .with_columns(type=pl.lit(\"prediction\")),\n        alpha=0.3,\n    )\n    + geom_ribbon(\n        aes(x=\"weight\", ymin=\"epred_min\", ymax=\"epred_max\", fill=\"type\"),\n        data=df_post_epred.group_by(\"weight\")\n        .agg(epred_min=pl.col(\"epred\").min(), epred_max=pl.col(\"epred\").max())\n        .with_columns(type=pl.lit(\"epred\")),\n        alpha=0.6,\n    )\n    + geom_point(aes(\"weight\", \"epred\"), data=df_base)\n    + theme_minimal()\n    + labs(title=\"expected predictions and posterior predictive distributions\")\n)"
  },
  {
    "objectID": "posts/post-with-code/index.html#why",
    "href": "posts/post-with-code/index.html#why",
    "title": "TidyPyMC",
    "section": "Why?",
    "text": "Why?\nI’m in the middle of a journey to learn Bayesian stats. Looking forward to getting to the top of the hill of overconfidence in my knowledge, but this has been hard and I am still at low competence low confidence.\nLearning Bayesian statistics has a steep learning curve on the theory and the tooling. Not to mention the tooling is dramatically different depending on the language or environment being used. When working with data, if are familiar with one language you can search for the most part how to use the API in another language. You learn the fundamentals of it, how data is structured, what works effectively, and it is largely transferrable. After that it is syntax and environment preference.\nNow the tools in probablelistic programmming languages (PPL)… it’s a lot. And it’s daunting!\nI can’t state enough what great work the community has done to make it open and accessible. I started out, like many others, with Richard McElreath’s Statistical Rethinking, and then following along with Solomon Kurz’s Statistical rethinking with brms, ggplot, and the tidyverse. Those are R environments but Allen Downey has great materieal and the work from PyMC community has a ton of efforts to lower the barrier to entry for new users.\nWorking with data in R allows access to some of the best tools to interactively work with data for wrangling and plotting.\nFor me, I love R - but wanted something different to learn in. brms abstracts so much of the model building away, it is fantastic to use but I found myself having trouble learning the\nTBC…"
  },
  {
    "objectID": "posts/post-with-code/index.html#tidypymc",
    "href": "posts/post-with-code/index.html#tidypymc",
    "title": "TidyPyMC",
    "section": "TidyPyMC",
    "text": "TidyPyMC\nThis is definitely subjective, but I think the missing commponent right now is a consistent way to turn the arviz.InferenceData object into a dataframe. Both this and the code Vincent shared is highly custom to the model, and from my experience that is typically a common design choice between R and Python libraries. There is tradeoffs to both paradigms.\nThere are a couple of plots in tidybayes add_epred_draws() and add_predicted_draws() that show some of its capabilities. The goal of this is to replicate them.\nTo accomplish this, we will bring the observed data, linear predictions, and posterior predictions in the same dataframe.\n\nLibraries and data\nWe’ll use the mtcars dataset to replicate some of the tidybayes examples.\n\nimport pandas as pd\nimport pymc as pm\nimport arviz as az\nimport numpy as np\n\nfrom plotnine.data import mtcars\nfrom plotnine import * # elmo_fire.gif namespace, but they mostly start with geom_*\nmtcars.head()\n\n\n\n\n\n\n\n\nname\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\nMazda RX4\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\nDatsun 710\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\nHornet 4 Drive\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n4\nHornet Sportabout\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2"
  },
  {
    "objectID": "posts/post-with-code/index.html#model",
    "href": "posts/post-with-code/index.html#model",
    "title": "TidyPyMC",
    "section": "Model",
    "text": "Model\nThis is an attempt at replicating the model here.\nThe formula is following mpg ~ hp * cyl fit with brms.\n\n\n\n\n\n\nNote\n\n\n\nThe scope of this wasn’t necessarily to walk through creating a pymc model or walk through the workflow of prior predictive checks, diagnostics, etc. I took the priors from brms::stancode(brms::brm(mpg ~ hp * cyl, data = mtcars)) in R. I also used PyMC over Bambi, but both libraries work off the arviz.InferenceData object.\n\n\n\n# build model and sample posterior\nwith pm.Model(\n    coords={\"obs\": mtcars.index, \"predictors\": ['hp', 'cyl']}\n) as mod:\n\n    X = pm.Data(\"X\", mtcars[[\"hp\", \"cyl\"]], dims=(\"obs\", \"predictors\"))\n\n    alpha = pm.StudentT(\"alpha\", nu=3, mu=19.2, sigma=5.4)\n    sigma = pm.HalfStudentT(\"sigma\", nu=3, sigma=5.54)\n    beta = pm.Normal(\"b\", mu=0, sigma=1, dims='predictors')\n\n    mu = pm.Deterministic(\"mu\", alpha + pm.math.dot(X, beta), dims='obs')\n\n    y = pm.Normal(\n        \"y\",\n        mu=mu,\n        sigma=sigma,\n        shape=X.shape[0],\n        observed=mtcars[\"mpg\"],\n        dims=\"obs\",\n    )\n\n    idata = pm.sample(random_seed=527)\n\n# sample posterior predictive\nwith mod as model:\n    pp = pm.sample_posterior_predictive(idata, predictions=True)\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [alpha, sigma, b]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 2 seconds.\nSampling: [y]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy up\nThe biggest takeaway I had from Vincent’s post was it wsa possible to get tidy data out of the arviz.InferenceData object, and this was by far the most difficult part to get my head around.\nFrom idata.posterior, we’ll take three things:\n\nglobal parameters: sigma, alpha\nparameters beta (2)\nlinear predictions mu\n\nThe key is to understand the dimensions of which attribute you want ot get and which ones are the same.\n\nprint(f'alpha: {idata.posterior[\"alpha\"].shape}')\nprint(f'sigma: {idata.posterior[\"sigma\"].shape}')\nprint(f'beta: {idata.posterior[\"b\"].shape}')\nprint(f'mu: {idata.posterior[\"mu\"].shape}')\n\nalpha: (4, 1000)\nsigma: (4, 1000)\nbeta: (4, 1000, 2)\nmu: (4, 1000, 32)\n\n\n\nBoth alpha and sigma are the same shape becuause they are global parameters.\nbeta has the same number of draws, each is represented as a row that will pivot\nmu has the same number of draws but for each observation\n\n\nparams = idata.posterior[[\"sigma\", \"alpha\"]].to_dataframe().reset_index()\nbetas = (\n    idata.posterior[\"b\"]\n    .to_dataframe()\n    .reset_index()\n    .pivot(index=[\"chain\", \"draw\"], columns=\"predictors\", values=\"b\")\n    .reset_index()\n)\n\ndf_posterior = params.merge(betas, on=[\"chain\", \"draw\"])\n\ndf_posterior = (\n    idata.posterior[\"mu\"]\n    .to_dataframe()\n    .reset_index()\n    .merge(mtcars[[\"cyl\", \"mpg\", \"hp\"]], left_on=\"obs\", right_on=mtcars.index)\n    .merge(params, on=[\"chain\", \"draw\"])\n    .merge(betas, on=[\"chain\", \"draw\"], suffixes=[\"\", \"_b\"])\n    .assign( # for plotting later\n        group=lambda x: x.cyl.astype(str)\n        + \"_\"\n        + x.draw.astype(str)\n        + \"_\"\n        + x.chain.astype(str)\n    )\n)\n\ndf_posterior\n\n\n\n\n\n\n\n\nchain\ndraw\nobs\nmu\ncyl\nmpg\nhp\nsigma\nalpha\ncyl_b\nhp_b\ngroup\n\n\n\n\n0\n0\n0\n0\n22.463651\n6\n21.0\n110\n4.074876\n33.091908\n-0.731355\n-0.056728\n6_0_0\n\n\n1\n0\n0\n1\n22.463651\n6\n21.0\n110\n4.074876\n33.091908\n-0.731355\n-0.056728\n6_0_0\n\n\n2\n0\n0\n2\n24.890745\n4\n22.8\n93\n4.074876\n33.091908\n-0.731355\n-0.056728\n4_0_0\n\n\n3\n0\n0\n3\n22.463651\n6\n21.4\n110\n4.074876\n33.091908\n-0.731355\n-0.056728\n6_0_0\n\n\n4\n0\n0\n4\n17.313594\n8\n18.7\n175\n4.074876\n33.091908\n-0.731355\n-0.056728\n8_0_0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n127995\n3\n999\n27\n24.983972\n4\n30.4\n113\n2.481254\n35.978550\n-2.033584\n-0.025312\n4_999_3\n\n\n127996\n3\n999\n28\n13.027543\n8\n15.8\n264\n2.481254\n35.978550\n-2.033584\n-0.025312\n8_999_3\n\n\n127997\n3\n999\n29\n19.347468\n6\n19.7\n175\n2.481254\n35.978550\n-2.033584\n-0.025312\n6_999_3\n\n\n127998\n3\n999\n30\n11.230399\n8\n15.0\n335\n2.481254\n35.978550\n-2.033584\n-0.025312\n8_999_3\n\n\n127999\n3\n999\n31\n25.085220\n4\n21.4\n109\n2.481254\n35.978550\n-2.033584\n-0.025312\n4_999_3\n\n\n\n\n128000 rows × 12 columns\n\n\n\nThe critical takeaway I had from implementing this was learn to leverage coords and dims in the model container. This makes it easier to work with the data later, especially as dimensions increase (i.e. groups in a multilevel model).\nThis sets the posterior to be represented in a tidy dataframe, exactly how Vincent did it. I’m really interested what ways we can cofigure the atributes with PyMC to make this generalize across different models and data.\n\n\n\n\n\n\nNote\n\n\n\nI’ve come back to this and realized adding in the parameters doesn’t make it “tidy”, and also doesn’t get used in the plots. It is still beneficial to include how to go about joining the parameters posterior draws to the data, and anyway… this post is for future me.\n\n\n\n\nPosterior predictive\nOne way would be to do this would be to use arviz.summary() on the sampled posterior predictions. This is a common workflow I would do with brms and tidybayes of parsing parameter outputs name to match the group, or join an id with the original dataset.\n\ndf_predictions = az.summary(pp)\n\n/Users/ryan/git-repos/plain-data/.venv/lib/python3.12/site-packages/arviz/stats/stats.py:1359: UserWarning: Selecting first found group: predictions\n\n\nNext steps to join it with the observed data.\n\ndf_predictions.index = df_predictions.index.str.extract(r\"y\\[(.*?)\\]\")[0]\n\ndf_predictions = df_predictions.merge(mtcars[[\"hp\", \"cyl\", \"mpg\"]], on=df_predictions.index)\ndf_predictions.head()\n\n\n\n\n\n\n\n\nkey_0\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\nhp\ncyl\nmpg\n\n\n\n\n0\n0\n21.561\n3.593\n14.962\n28.471\n0.059\n0.047\n3667.0\n3588.0\n1.0\n110\n6\n21.0\n\n\n1\n1\n21.557\n3.564\n15.082\n28.526\n0.055\n0.042\n4267.0\n3274.0\n1.0\n110\n6\n21.0\n\n\n2\n2\n25.175\n3.594\n18.473\n32.054\n0.062\n0.045\n3391.0\n3769.0\n1.0\n93\n4\n22.8\n\n\n3\n3\n21.533\n3.502\n15.182\n28.314\n0.057\n0.045\n3712.0\n3723.0\n1.0\n110\n6\n21.4\n\n\n4\n4\n16.380\n3.565\n9.941\n23.302\n0.059\n0.043\n3708.0\n4043.0\n1.0\n175\n8\n18.7\n\n\n\n\n\n\n\nThis worked well with the named index on mtcars. I’m not a fan of pandas, and I’ve long forgotten a lot of tips and tricks to work with the nuances of pandas after a couple of years of using polars. For future me, I’m going to include a standarad approach of working with the posterior.\n\ndf_posterior_predictive = (\n    pp.predictions[\"y\"]\n    .to_dataframe()\n    .reset_index()\n    .merge(mtcars[[\"cyl\", \"hp\"]], left_on=\"obs\", right_on=mtcars.index)\n)\n\ndf_posterior_predictive\n\n\n\n\n\n\n\n\nchain\ndraw\nobs\ny\ncyl\nhp\n\n\n\n\n0\n0\n0\n0\n33.841759\n6\n110\n\n\n1\n0\n0\n1\n15.208467\n6\n110\n\n\n2\n0\n0\n2\n26.193852\n4\n93\n\n\n3\n0\n0\n3\n20.961931\n6\n110\n\n\n4\n0\n0\n4\n19.215352\n8\n175\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n127995\n3\n999\n27\n23.539676\n4\n113\n\n\n127996\n3\n999\n28\n8.166691\n8\n264\n\n\n127997\n3\n999\n29\n17.398505\n6\n175\n\n\n127998\n3\n999\n30\n13.519691\n8\n335\n\n\n127999\n3\n999\n31\n28.362757\n4\n109\n\n\n\n\n128000 rows × 6 columns\n\n\n\nThe data is aggregated to match the az.summary() output since this particular geom_ribbon() visualization will only need the HDI values of the posterior predictive distribution.\n\ndf_predictions = (\n    df_posterior_predictive.groupby([\"obs\", \"cyl\", \"hp\"])\n    .agg(\n        pp_mean=(\"y\", \"mean\"),\n        pp_min=(\"y\", lambda x: x.quantile(0.03)),\n        pp_max=(\"y\", lambda x: x.quantile(0.97)),\n    )\n    .reset_index()\n)\ndf_predictions.head()\n\n\n\n\n\n\n\n\nobs\ncyl\nhp\npp_mean\npp_min\npp_max\n\n\n\n\n0\n0\n6\n110\n21.561444\n14.731715\n28.294984\n\n\n1\n1\n6\n110\n21.556898\n14.736975\n28.295248\n\n\n2\n2\n4\n93\n25.174758\n18.227956\n31.853902\n\n\n3\n3\n6\n110\n21.533499\n14.852328\n28.133632\n\n\n4\n4\n8\n175\n16.379716\n9.577502\n23.042973\n\n\n\n\n\n\n\n\n\nThe plot is coming together\nPlotnine! With the grammar of graphics, we’re able to:\n\nuse different datasets\nlayer aesthetics together\nthink about plots with data\n\n\n# sample draws for plotting purposes\nsamples = np.random.choice(\n    [x for x in range(999)], size=int(5), replace=False\n)\n\n(\n    ggplot(mtcars, aes(\"hp\", \"mpg\", color=\"factor(cyl)\", fill=\"factor(cyl)\"))\n    + geom_ribbon(\n        aes(y=\"pp_mean\", ymin=\"pp_min\", ymax=\"pp_max\"), data=df_predictions, alpha=0.2\n    )\n    + geom_line(\n        aes(y=\"mu\", group=\"group\"),\n        data=df_posterior[df_posterior.draw.isin(samples)],\n        alpha=0.6,\n    )\n    + geom_point()\n    + theme_minimal()\n    + labs(color='cyl', fill='cyl')\n)"
  },
  {
    "objectID": "posts/post-with-code/index.html#but-why",
    "href": "posts/post-with-code/index.html#but-why",
    "title": "TidyPyMC",
    "section": "But Why?",
    "text": "But Why?\nI’m in the middle of a journey to learn Bayesian statsistics.\nI can’t state enough what great work the community has done to make it open and accessible. I started out, like many others, with Richard McElreath’s Statistical Rethinking, and then following along with Solomon Kurz’s Statistical rethinking with brms, ggplot, and the tidyverse. Those are R environments but the PyMC community has gret material on getting started and using it for Bayesian Stats.\nThere is a steep learning curve on the theory, but also with the tooling. To do modern Bayesian modeling, you need to interact with a probablelistic programmming languages (PPL). The frameworks and libraries are dramatically different depending on the language or environment being used.\nIt’s daunting! Knowing the theory doesn’t necessarily make it easier to use the tool. Learning one PPL framework doesn’t mean it will be trivial to move to another. As a beginner, brms and Bambi are excellent libraries that lower the barrier of entry and make the model building process easier. The downside is with so much abstracted away, it can make it difficult to really learn what is going on.\nStepping out from brms to Stan was like falling off a cliff. I wanted something that worked better for me, and PyMC was a good blend of what I needed in my journey, and to my surprise have liked it a lot. I felt like I made a lot of progress solidifying concepts building models I had made in brms to PyMC, however I spent most of the time trying to learn how to use the az.InferenceData object.\nI see a lot of value getting a way to work with tidy datasets, both in learning and in application."
  }
]