[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Just Plain Data",
    "section": "",
    "text": "Rython\n\n\n\n\n\n\n\nBayesian\n\n\nPyMC\n\n\nReticulate\n\n\n\n\nUsing Reticulate and PyMC, wonderfully managed with uv\n\n\n\n\n\n\nMay 31, 2025\n\n\nRyan Plain\n\n\n\n\n\n\n  \n\n\n\n\nTidyPyMC\n\n\n\n\n\n\n\nBayesian\n\n\nPyMC\n\n\nPlotnine\n\n\n\n\nGrammar of Graphics with PyMC\n\n\n\n\n\n\nMay 14, 2025\n\n\nRyan Plain\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "TidyPyMC",
    "section": "",
    "text": "A few weeks ago, Benjamin Vincent posted this blog post on using Bayesian models in Python and leveraging the grammar of graphics for plotting. Please take the time to read that post as this is derived and inteded to augment the ideas shared there.\nAt the end of the post, Vincent asked “It would be interesting to see if this approach is appealing to people.” My answer to that is… YES‼️\nI like PyMC and ArviZ a lot, but it was a huge blow coming from R and libraries like tidybayes, bayesplots, and others that helped wrangle and visualize the posterior.\nI fully agree with the approach of ArviZ to work with high-dimensional data, but comming from a stats background it is more intuitive to work with things as dataframes rather than objects whenever it makes sense. This is especially true with visualizations and the grammar of graphics.\nI’m not sure how or what the best way to contribute to this, and it was mentioned on bluesky that GoG-like interface is being developped for ArviZ 1.0. The best thing I can do is create a post for me."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m an all things data professional living in Colorado Springs, currently working in healthcare supporting their provider network infrastructure. Previously I worked in tech doing product analytics and data science. Prior to that I worked in aviation simulating passenger traffic through airport facilties.\nI’m passionate about making things more efficient, and a proponent of working with what tool and environment works best for you. Designing designing solutions that are inclusive for everyone is something I enjoy.\nIn my free time I love exploring the outdoors, climbing, yoga, snowboarding, and spending time with my wife, dogs, and cat (if she lets me)."
  },
  {
    "objectID": "posts/post-with-code/index.html#tidybayes-in-python-would-be-cool",
    "href": "posts/post-with-code/index.html#tidybayes-in-python-would-be-cool",
    "title": "TidyPyMC",
    "section": "",
    "text": "A few weeks ago, Benjamin Vincent posted this blog post on using Bayesian models in Python and leveraging the grammar of graphics for plotting. Please take the time to read that post as this is derived and inteded to augment the ideas shared there.\nAt the end of the post, Vincent asked “It would be interesting to see if this approach is appealing to people.” My answer to that is… YES‼️\nI like PyMC and ArviZ a lot, but it was a huge blow coming from R and libraries like tidybayes, bayesplots, and others that helped wrangle and visualize the posterior.\nI fully agree with the approach of ArviZ to work with high-dimensional data, but comming from a stats background it is more intuitive to work with things as dataframes rather than objects whenever it makes sense. This is especially true with visualizations and the grammar of graphics.\nI’m not sure how or what the best way to contribute to this, and it was mentioned on bluesky that GoG-like interface is being developped for ArviZ 1.0. The best thing I can do is create a post for me."
  },
  {
    "objectID": "posts/post-with-code/index.html#how-it-started",
    "href": "posts/post-with-code/index.html#how-it-started",
    "title": "TidyPyMC",
    "section": "How it started",
    "text": "How it started\nTL;DR here is the code\n\n\nCode\nimport polars as pl\nimport pymc as pm\nimport arviz as az\nimport numpy as np\n\nfrom plotnine import ggplot, aes, geom_point, geom_ribbon, theme_minimal, labs\n\nd = pl.read_csv(\n    \"https://raw.githubusercontent.com/dustinstansbury/statistical-rethinking-2023/refs/heads/main/data/Howell1.csv\",\n    separator=\";\",\n).filter(pl.col(\"age\") &gt;= 18)\n\nwith pm.Model() as model:\n    weight = pm.Data(\"weight\", d[\"weight\"], dims=[\"obs_id\"])\n\n    sigma = pm.Uniform(\"sigma\", lower=0, upper=50)\n    alpha = pm.Normal(\"alpha\", mu=178, sigma=20)\n\n    beta = pm.LogNormal(\"beta\", mu=0, sigma=1)\n\n    mu = alpha + beta * (weight - weight.mean())\n    y = pm.Normal(\n        \"y\",\n        mu=mu,\n        sigma=sigma,\n        observed=d[\"height\"],\n        shape=weight.shape,\n        dims=[\"obs_id\"],\n    )\n\n    idata = pm.sample()\n\n\nnew_weight = [46.95, 43.72, 64.78, 32.59, 54.63]\nwith model:\n    pm.set_data({\"weight\": new_weight})\n    pp = pm.sample_posterior_predictive(idata, predictions=True, var_names=[\"y\"])\n\nposterior = idata.posterior.to_dataframe().reset_index()\n\nsamples_beta = [\n    np.random.choice(posterior[\"beta\"], size=1000, replace=True).tolist()\n    for _ in new_weight\n]\nsamples_alpha = [\n    np.random.choice(posterior[\"alpha\"], size=1000, replace=True).tolist()\n    for _ in new_weight\n]\n\ndf = pl.DataFrame({\"weight\": new_weight, \"beta\": samples_beta, \"alpha\": samples_alpha})\n\n\ndf_post_epred = df.explode([\"beta\", \"alpha\"]).with_columns(\n    epred=pl.col(\"alpha\") + (pl.col(\"beta\") * (pl.col(\"weight\") - d[\"weight\"].mean()))\n)\ndf_base = df_post_epred.group_by(\"weight\").mean()\n\ndf_post_pred = pl.from_pandas(\n    pp.predictions.to_dataframe().reset_index()\n    # .assign(weight=lambda x: x.obs_id.map(dict(enumerate(new_weight))))\n).with_columns(\n    pl.col(\"obs_id\")\n    .cast(pl.Float64)\n    .replace(dict(enumerate(new_weight)))\n    .alias(\"weight\")\n)\n\n(\n    ggplot()\n    + geom_ribbon(\n        aes(x=\"weight\", ymin=\"y_min\", ymax=\"y_max\", fill=\"type\"),\n        data=df_post_pred.group_by(\"weight\")\n        .agg(y_min=pl.col(\"y\").min(), y_max=pl.col(\"y\").max())\n        .with_columns(type=pl.lit(\"prediction\")),\n        alpha=0.3,\n    )\n    + geom_ribbon(\n        aes(x=\"weight\", ymin=\"epred_min\", ymax=\"epred_max\", fill=\"type\"),\n        data=df_post_epred.group_by(\"weight\")\n        .agg(epred_min=pl.col(\"epred\").min(), epred_max=pl.col(\"epred\").max())\n        .with_columns(type=pl.lit(\"epred\")),\n        alpha=0.6,\n    )\n    + geom_point(aes(\"weight\", \"epred\"), data=df_base)\n    + theme_minimal()\n    + labs(title=\"expected predictions and posterior predictive distributions\")\n)"
  },
  {
    "objectID": "posts/post-with-code/index.html#why",
    "href": "posts/post-with-code/index.html#why",
    "title": "TidyPyMC",
    "section": "Why?",
    "text": "Why?\nI’m in the middle of a journey to learn Bayesian stats. Looking forward to getting to the top of the hill of overconfidence in my knowledge, but this has been hard and I am still at low competence low confidence.\nLearning Bayesian statistics has a steep learning curve on the theory and the tooling. Not to mention the tooling is dramatically different depending on the language or environment being used. When working with data, if are familiar with one language you can search for the most part how to use the API in another language. You learn the fundamentals of it, how data is structured, what works effectively, and it is largely transferrable. After that it is syntax and environment preference.\nNow the tools in probablelistic programmming languages (PPL)… it’s a lot. And it’s daunting!\nI can’t state enough what great work the community has done to make it open and accessible. I started out, like many others, with Richard McElreath’s Statistical Rethinking, and then following along with Solomon Kurz’s Statistical rethinking with brms, ggplot, and the tidyverse. Those are R environments but Allen Downey has great materieal and the work from PyMC community has a ton of efforts to lower the barrier to entry for new users.\nWorking with data in R allows access to some of the best tools to interactively work with data for wrangling and plotting.\nFor me, I love R - but wanted something different to learn in. brms abstracts so much of the model building away, it is fantastic to use but I found myself having trouble learning the\nTBC…"
  },
  {
    "objectID": "posts/post-with-code/index.html#tidypymc",
    "href": "posts/post-with-code/index.html#tidypymc",
    "title": "TidyPyMC",
    "section": "TidyPyMC",
    "text": "TidyPyMC\nThis is definitely subjective, but I think the missing commponent right now is a consistent way to turn the arviz.InferenceData object into a dataframe. Both this and the code Vincent shared is highly custom to the model, and from my experience that is typically a common design choice between R and Python libraries. There is tradeoffs to both paradigms.\nThere are a couple of plots in tidybayes add_epred_draws() and add_predicted_draws() that show some of its capabilities. The goal of this is to replicate them.\nTo accomplish this, we will bring the observed data, linear predictions, and posterior predictions in the same dataframe.\n\nLibraries and data\nWe’ll use the mtcars dataset to replicate some of the tidybayes examples.\n\nimport pandas as pd\nimport pymc as pm\nimport arviz as az\nimport numpy as np\n\nfrom plotnine.data import mtcars\nfrom plotnine import * # elmo_fire.gif namespace, but they mostly start with geom_*\nmtcars.head()\n\n\n\n\n\n\n\n\nname\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\nMazda RX4\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\nDatsun 710\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\nHornet 4 Drive\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n4\nHornet Sportabout\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2"
  },
  {
    "objectID": "posts/post-with-code/index.html#model",
    "href": "posts/post-with-code/index.html#model",
    "title": "TidyPyMC",
    "section": "Model",
    "text": "Model\nThis is an attempt at replicating the model here.\nThe formula is following mpg ~ hp * cyl fit with brms.\n\n\n\n\n\n\nNote\n\n\n\nThe scope of this wasn’t necessarily to walk through creating a pymc model or walk through the workflow of prior predictive checks, diagnostics, etc. I took the priors from brms::stancode(brms::brm(mpg ~ hp * cyl, data = mtcars)) in R. I also used PyMC over Bambi, but both libraries work off the arviz.InferenceData object.\n\n\n\n# build model and sample posterior\nwith pm.Model(\n    coords={\"obs\": mtcars.index, \"predictors\": ['hp', 'cyl']}\n) as mod:\n\n    X = pm.Data(\"X\", mtcars[[\"hp\", \"cyl\"]], dims=(\"obs\", \"predictors\"))\n\n    alpha = pm.StudentT(\"alpha\", nu=3, mu=19.2, sigma=5.4)\n    sigma = pm.HalfStudentT(\"sigma\", nu=3, sigma=5.54)\n    beta = pm.Normal(\"b\", mu=0, sigma=1, dims='predictors')\n\n    mu = pm.Deterministic(\"mu\", alpha + pm.math.dot(X, beta), dims='obs')\n\n    y = pm.Normal(\n        \"y\",\n        mu=mu,\n        sigma=sigma,\n        shape=X.shape[0],\n        observed=mtcars[\"mpg\"],\n        dims=\"obs\",\n    )\n\n    idata = pm.sample(random_seed=527)\n\n# sample posterior predictive\nwith mod as model:\n    pp = pm.sample_posterior_predictive(idata, predictions=True)\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [alpha, sigma, b]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 2 seconds.\nSampling: [y]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy up\nThe biggest takeaway I had from Vincent’s post was it wsa possible to get tidy data out of the arviz.InferenceData object, and this was by far the most difficult part to get my head around.\nFrom idata.posterior, we’ll take three things:\n\nglobal parameters: sigma, alpha\nparameters beta (2)\nlinear predictions mu\n\nThe key is to understand the dimensions of which attribute you want ot get and which ones are the same.\n\nprint(f'alpha: {idata.posterior[\"alpha\"].shape}')\nprint(f'sigma: {idata.posterior[\"sigma\"].shape}')\nprint(f'beta: {idata.posterior[\"b\"].shape}')\nprint(f'mu: {idata.posterior[\"mu\"].shape}')\n\nalpha: (4, 1000)\nsigma: (4, 1000)\nbeta: (4, 1000, 2)\nmu: (4, 1000, 32)\n\n\n\nBoth alpha and sigma are the same shape becuause they are global parameters.\nbeta has the same number of draws, each is represented as a row that will pivot\nmu has the same number of draws but for each observation\n\n\nparams = idata.posterior[[\"sigma\", \"alpha\"]].to_dataframe().reset_index()\nbetas = (\n    idata.posterior[\"b\"]\n    .to_dataframe()\n    .reset_index()\n    .pivot(index=[\"chain\", \"draw\"], columns=\"predictors\", values=\"b\")\n    .reset_index()\n)\n\ndf_posterior = params.merge(betas, on=[\"chain\", \"draw\"])\n\ndf_posterior = (\n    idata.posterior[\"mu\"]\n    .to_dataframe()\n    .reset_index()\n    .merge(mtcars[[\"cyl\", \"mpg\", \"hp\"]], left_on=\"obs\", right_on=mtcars.index)\n    .merge(params, on=[\"chain\", \"draw\"])\n    .merge(betas, on=[\"chain\", \"draw\"], suffixes=[\"\", \"_b\"])\n    .assign( # for plotting later\n        group=lambda x: x.cyl.astype(str)\n        + \"_\"\n        + x.draw.astype(str)\n        + \"_\"\n        + x.chain.astype(str)\n    )\n)\n\ndf_posterior\n\n\n\n\n\n\n\n\nchain\ndraw\nobs\nmu\ncyl\nmpg\nhp\nsigma\nalpha\ncyl_b\nhp_b\ngroup\n\n\n\n\n0\n0\n0\n0\n22.463651\n6\n21.0\n110\n4.074876\n33.091908\n-0.731355\n-0.056728\n6_0_0\n\n\n1\n0\n0\n1\n22.463651\n6\n21.0\n110\n4.074876\n33.091908\n-0.731355\n-0.056728\n6_0_0\n\n\n2\n0\n0\n2\n24.890745\n4\n22.8\n93\n4.074876\n33.091908\n-0.731355\n-0.056728\n4_0_0\n\n\n3\n0\n0\n3\n22.463651\n6\n21.4\n110\n4.074876\n33.091908\n-0.731355\n-0.056728\n6_0_0\n\n\n4\n0\n0\n4\n17.313594\n8\n18.7\n175\n4.074876\n33.091908\n-0.731355\n-0.056728\n8_0_0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n127995\n3\n999\n27\n24.983972\n4\n30.4\n113\n2.481254\n35.978550\n-2.033584\n-0.025312\n4_999_3\n\n\n127996\n3\n999\n28\n13.027543\n8\n15.8\n264\n2.481254\n35.978550\n-2.033584\n-0.025312\n8_999_3\n\n\n127997\n3\n999\n29\n19.347468\n6\n19.7\n175\n2.481254\n35.978550\n-2.033584\n-0.025312\n6_999_3\n\n\n127998\n3\n999\n30\n11.230399\n8\n15.0\n335\n2.481254\n35.978550\n-2.033584\n-0.025312\n8_999_3\n\n\n127999\n3\n999\n31\n25.085220\n4\n21.4\n109\n2.481254\n35.978550\n-2.033584\n-0.025312\n4_999_3\n\n\n\n\n128000 rows × 12 columns\n\n\n\nThe critical takeaway I had from implementing this was learn to leverage coords and dims in the model container. This makes it easier to work with the data later, especially as dimensions increase (i.e. groups in a multilevel model).\nThis sets the posterior to be represented in a tidy dataframe, exactly how Vincent did it. I’m really interested what ways we can cofigure the atributes with PyMC to make this generalize across different models and data.\n\n\n\n\n\n\nNote\n\n\n\nI’ve come back to this and realized adding in the parameters doesn’t make it “tidy”, and also doesn’t get used in the plots. It is still beneficial to include how to go about joining the parameters posterior draws to the data, and anyway… this post is for future me.\n\n\n\n\nPosterior predictive\nOne way would be to do this would be to use arviz.summary() on the sampled posterior predictions. This is a common workflow I would do with brms and tidybayes of parsing parameter outputs name to match the group, or join an id with the original dataset.\n\ndf_predictions = az.summary(pp)\n\n/Users/ryan/git-repos/plain-data/.venv/lib/python3.12/site-packages/arviz/stats/stats.py:1359: UserWarning: Selecting first found group: predictions\n\n\nNext steps to join it with the observed data.\n\ndf_predictions.index = df_predictions.index.str.extract(r\"y\\[(.*?)\\]\")[0]\n\ndf_predictions = df_predictions.merge(mtcars[[\"hp\", \"cyl\", \"mpg\"]], on=df_predictions.index)\ndf_predictions.head()\n\n\n\n\n\n\n\n\nkey_0\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\nhp\ncyl\nmpg\n\n\n\n\n0\n0\n21.561\n3.593\n14.962\n28.471\n0.059\n0.047\n3667.0\n3588.0\n1.0\n110\n6\n21.0\n\n\n1\n1\n21.557\n3.564\n15.082\n28.526\n0.055\n0.042\n4267.0\n3274.0\n1.0\n110\n6\n21.0\n\n\n2\n2\n25.175\n3.594\n18.473\n32.054\n0.062\n0.045\n3391.0\n3769.0\n1.0\n93\n4\n22.8\n\n\n3\n3\n21.533\n3.502\n15.182\n28.314\n0.057\n0.045\n3712.0\n3723.0\n1.0\n110\n6\n21.4\n\n\n4\n4\n16.380\n3.565\n9.941\n23.302\n0.059\n0.043\n3708.0\n4043.0\n1.0\n175\n8\n18.7\n\n\n\n\n\n\n\nThis worked well with the named index on mtcars. I’m not a fan of pandas, and I’ve long forgotten a lot of tips and tricks to work with the nuances of pandas after a couple of years of using polars. For future me, I’m going to include a standarad approach of working with the posterior.\n\ndf_posterior_predictive = (\n    pp.predictions[\"y\"]\n    .to_dataframe()\n    .reset_index()\n    .merge(mtcars[[\"cyl\", \"hp\"]], left_on=\"obs\", right_on=mtcars.index)\n)\n\ndf_posterior_predictive\n\n\n\n\n\n\n\n\nchain\ndraw\nobs\ny\ncyl\nhp\n\n\n\n\n0\n0\n0\n0\n33.841759\n6\n110\n\n\n1\n0\n0\n1\n15.208467\n6\n110\n\n\n2\n0\n0\n2\n26.193852\n4\n93\n\n\n3\n0\n0\n3\n20.961931\n6\n110\n\n\n4\n0\n0\n4\n19.215352\n8\n175\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n127995\n3\n999\n27\n23.539676\n4\n113\n\n\n127996\n3\n999\n28\n8.166691\n8\n264\n\n\n127997\n3\n999\n29\n17.398505\n6\n175\n\n\n127998\n3\n999\n30\n13.519691\n8\n335\n\n\n127999\n3\n999\n31\n28.362757\n4\n109\n\n\n\n\n128000 rows × 6 columns\n\n\n\nThe data is aggregated to match the az.summary() output since this particular geom_ribbon() visualization will only need the HDI values of the posterior predictive distribution.\n\ndf_predictions = (\n    df_posterior_predictive.groupby([\"obs\", \"cyl\", \"hp\"])\n    .agg(\n        pp_mean=(\"y\", \"mean\"),\n        pp_min=(\"y\", lambda x: x.quantile(0.03)),\n        pp_max=(\"y\", lambda x: x.quantile(0.97)),\n    )\n    .reset_index()\n)\ndf_predictions.head()\n\n\n\n\n\n\n\n\nobs\ncyl\nhp\npp_mean\npp_min\npp_max\n\n\n\n\n0\n0\n6\n110\n21.561444\n14.731715\n28.294984\n\n\n1\n1\n6\n110\n21.556898\n14.736975\n28.295248\n\n\n2\n2\n4\n93\n25.174758\n18.227956\n31.853902\n\n\n3\n3\n6\n110\n21.533499\n14.852328\n28.133632\n\n\n4\n4\n8\n175\n16.379716\n9.577502\n23.042973\n\n\n\n\n\n\n\n\n\nThe plot is coming together\nPlotnine! With the grammar of graphics, we’re able to:\n\nuse different datasets\nlayer aesthetics together\nthink about plots with data\n\n\n# sample draws for plotting purposes\nsamples = np.random.choice(\n    [x for x in range(999)], size=int(5), replace=False\n)\n\n(\n    ggplot(mtcars, aes(\"hp\", \"mpg\", color=\"factor(cyl)\", fill=\"factor(cyl)\"))\n    + geom_ribbon(\n        aes(y=\"pp_mean\", ymin=\"pp_min\", ymax=\"pp_max\"), data=df_predictions, alpha=0.2\n    )\n    + geom_line(\n        aes(y=\"mu\", group=\"group\"),\n        data=df_posterior[df_posterior.draw.isin(samples)],\n        alpha=0.6,\n    )\n    + geom_point()\n    + theme_minimal()\n    + labs(color='cyl', fill='cyl')\n)"
  },
  {
    "objectID": "posts/post-with-code/index.html#but-why",
    "href": "posts/post-with-code/index.html#but-why",
    "title": "TidyPyMC",
    "section": "But Why?",
    "text": "But Why?\nI’m in the middle of a journey to learn Bayesian statsistics.\nI can’t state enough what great work the community has done to make it open and accessible. I started out, like many others, with Richard McElreath’s Statistical Rethinking, and then following along with Solomon Kurz’s Statistical rethinking with brms, ggplot, and the tidyverse. Those are R environments but the PyMC community has gret material on getting started and using it for Bayesian Stats.\nThere is a steep learning curve on the theory, but also with the tooling. To do modern Bayesian modeling, you need to interact with a probablelistic programmming languages (PPL). The frameworks and libraries are dramatically different depending on the language or environment being used.\nIt’s daunting! Knowing the theory doesn’t necessarily make it easier to use the tool. Learning one PPL framework doesn’t mean it will be trivial to move to another. As a beginner, brms and Bambi are excellent libraries that lower the barrier of entry and make the model building process easier. The downside is with so much abstracted away, it can make it difficult to really learn what is going on.\nStepping out from brms to Stan was like falling off a cliff. I wanted something that worked better for me, and PyMC was a good blend of what I needed in my journey, and to my surprise have liked it a lot. I felt like I made a lot of progress solidifying concepts building models I had made in brms to PyMC, however I spent most of the time trying to learn how to use the az.InferenceData object.\nI see a lot of value getting a way to work with tidy datasets, both in learning and in application."
  },
  {
    "objectID": "posts/20250531-reticulate-pymc/index.html",
    "href": "posts/20250531-reticulate-pymc/index.html",
    "title": "Reticulate and PyMC",
    "section": "",
    "text": "In my previous post, I used the grammar of graphics to work with the posterior distributions on a PyMC model. Recently, I used the new version of reticulate that uses uv to manage the Python environment. From the post:\n|&gt; with py_require(), Reticulate will automatically create and manage Python environments behind the scenes so you don’t have to.\nI’m a huge fan of uv, and how the developers of Reticulate integrated this really enhances what you can do bringing Python and R together."
  },
  {
    "objectID": "posts/20250531-reticulate-pymc/index.html#bayesian-models-python-grammar-of-graphics-r",
    "href": "posts/20250531-reticulate-pymc/index.html#bayesian-models-python-grammar-of-graphics-r",
    "title": "Reticulate and PyMC",
    "section": "Bayesian models (Python) + grammar of graphics (R) = ❤️",
    "text": "Bayesian models (Python) + grammar of graphics (R) = ❤️\nThis is a nod to the title of Benjamin T. Vincent’s blog post, who inspired me to dive further into PyMC.\nThe reason I started looking into using the grammar of graphics with PyMC was to emulate an R enviornment and tidyverse as much as possible. What if instead we just use the R environement."
  },
  {
    "objectID": "posts/20250531-reticulate-pymc/index.html#again-but-why",
    "href": "posts/20250531-reticulate-pymc/index.html#again-but-why",
    "title": "Reticulate and PyMC",
    "section": "Again… but why?",
    "text": "Again… but why?"
  },
  {
    "objectID": "posts/20250531-reticulate-pymc/index.html#workflow",
    "href": "posts/20250531-reticulate-pymc/index.html#workflow",
    "title": "Reticulate and PyMC",
    "section": "Workflow",
    "text": "Workflow\n\nSet up the environment\nLoad up reticulate and the tidyverse. The first difference in this workflow is that mtcars is a dataset loaded into R by default.\n\nSys.setenv(RETICULATE_PYTHON = \"managed\")\n\n\nlibrary(reticulate)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nmtcars |&gt;\n    head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\nValiant\n18.1\n6\n225\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\n\n\n\n\nI’ve listend to enough presentations and interviews from Charlie Marsh to know I can’t do it justice to explain how uv works. An oversimplification of uv is that it centralizes package downloads, and then distributes them when needed for new isolated enviornments after resolving the dependencies. The main difference is that there isn’t reinstallation needed across environements.\nI’ve used PyMC with other uv environements, so running this was pretty straightforward.\n\npy_require('pymc', python_version=\"3.11\")\npy_config()\n\npython:         /Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/gKtLQ4Ys-srj2T0OKKL-t/bin/python3\nlibpython:      /Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/python/cpython-3.11.12-macos-aarch64-none/lib/libpython3.11.dylib\npythonhome:     /Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/gKtLQ4Ys-srj2T0OKKL-t:/Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/gKtLQ4Ys-srj2T0OKKL-t\nvirtualenv:     /Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/gKtLQ4Ys-srj2T0OKKL-t/bin/activate_this.py\nversion:        3.11.12 (main, May 30 2025, 05:53:55) [Clang 20.1.4 ]\nnumpy:          /Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/gKtLQ4Ys-srj2T0OKKL-t/lib/python3.11/site-packages/numpy\nnumpy_version:  2.2.6\n\nNOTE: Python version was forced by py_require()\n\n\nYou can see that we have an ephemeral cached version of a python environment created with uv and reticulate.\n\n\nSet up the data\nA significant use case for me to use R in the workflow, especially for early stages of iterating on the model building process, would be to use the tidyverse for data wrangling. Right off the bat we can use dplyr::mutate() to scale the variables. You can imagine other types of transformations that are more complex that this would be beneficial to do in an interective setting.\nBecause we will model an interaction similar to mpg ~ cyl * hp, I added a variable that lm() or brms would typically do under the hood. (Car analogy only because we are using mtcars… otherwise I don’t know anything about cars)\n\nmtcars_scaled &lt;- mtcars %&gt;%\n  mutate(\n    hp_c = scale(hp)[, 1],\n    cyl_c = scale(cyl)[, 1],\n    hp_cyl = hp_c * cyl_c\n  )\npandas_mtcars &lt;- r_to_py(mtcars_scaled)\n\nprint(class(mtcars_scaled))\n\n[1] \"data.frame\"\n\nprint(class(pandas_mtcars))\n\n[1] \"pandas.core.frame.DataFrame\"        \"pandas.core.generic.NDFrame\"       \n[3] \"pandas.core.base.PandasObject\"      \"pandas.core.accessor.DirNamesMixin\"\n[5] \"pandas.core.indexing.IndexingMixin\" \"pandas.core.arraylike.OpsMixin\"    \n[7] \"python.builtin.object\"             \n\n\n\n\nPyMC Model\nThe python sytax would look very similar to this, only now instead of using objects with dot notation, we access methods and attributes with the $ character. If you are purely a python developer, this might look obscene. I choose to put up with this quirkyness because I find working with dataframes in R and plotting more than worth it.\n\npm &lt;- import('pymc', convert = FALSE)\n\nmod &lt;- pm$Model(\n  coords = list(\n    car = pandas_mtcars$index,\n    predictors = c('hp', 'cyl', 'hp_cyl')\n  )\n)\n\n\nwith(mod, {\n  X &lt;- pm$Data('X', pandas_mtcars$loc[, c('hp', 'cyl', 'hp_cyl')])\n\n  alpha = pm$StudentT(\"alpha\", nu = 3, mu = 19.2, sigma = 5.4)\n  sigma = pm$HalfStudentT(\"sigma\", nu = 3, sigma = 5.54)\n  beta = pm$Normal(\"b\", mu = 0, sigma = 1, dims = 'predictors')\n  mu = pm$Deterministic(\"mu\", alpha + pm$math$dot(X, beta), dims = 'car')\n\n  y = pm$Normal(\n    \"y\",\n    mu = mu,\n    sigma = sigma,\n    shape = X$shape[0], # python index\n    observed = pandas_mtcars$mpg,\n    dims = \"car\",\n  )\n  # using a single core and chain because of Quarto, normally this would be 4 chains\n  idata = pm$sample(random_seed = 527L, cores = 1L, chains=1L)\n})\n\n                                                                                \n                              Step      Grad      Sampli…                       \n  Progre…   Draws   Diverg…   size      evals     Speed     Elapsed   Remaini…  \n ────────────────────────────────────────────────────────────────────────────── \n  ━━━━━━━   2000    0         0.19      15        1392.39   0:00:01   0:00:00   \n                                                  draws/s                       \n                                                                                \n\nwith(mod, {\n  pp = pm$sample_posterior_predictive(idata, predictions = TRUE)\n})\n\nSampling ... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00 / 0:00:00\n\n\n\n\nPosterior\nNow that we have the model’s az.InferenceData object and posterior predictions, we can work on replicating some of the plots. Only this time we can use tidybayes directly!\n\ndf_posterior &lt;- idata$posterior$mu$to_dataframe()$reset_index() |&gt;\n  py_to_r() |&gt;\n  as_tibble() |&gt;\n  left_join(rownames_to_column(mtcars, 'car')) |&gt;\n  mutate(group = paste0(chain, draw, cyl))\n\nJoining with `by = join_by(car)`\n\ndf_predictions &lt;- pp$predictions$to_dataframe()$reset_index() |&gt;\n  py_to_r() |&gt;\n  as_tibble() |&gt;\n  left_join(rownames_to_column(mtcars, \"car\"))\n\nJoining with `by = join_by(car)`\n\n\n\n\nPlot\n\ndf_predictions |&gt;\n  ggplot(aes(hp, y, color = as.factor(cyl))) +\n  tidybayes::stat_lineribbon(.width = c(.99, .95, .8, .5), alpha = 0.25) +\n  geom_line(\n    aes(y = mu, group = group),\n    data = df_posterior |&gt; filter(draw %in% round(seq(5, 900, length.out = 5))),\n    alpha = 0.38\n  ) +\n  geom_point(aes(y = mpg), data = mtcars, shape = 21, size = 2, stroke = 1) +\n  #scale_fill_manual(values=c('#F8766D', '#00BA38', '#619CFF', 'white')) +\n  scale_fill_brewer(palette = \"Greys\") +\n  theme_light(base_size = 12) +\n  guides(fill = 'none') +\n  labs(\n    title = 'MPG predictions and values based on Horsepower and Cylinders',\n    x = 'hp',\n    y = 'mpg',\n    color = 'cyl'\n  )"
  },
  {
    "objectID": "posts/20250514-pymc-gog/index.html",
    "href": "posts/20250514-pymc-gog/index.html",
    "title": "TidyPyMC",
    "section": "",
    "text": "A few weeks ago, Benjamin Vincent posted this blog post on using Bayesian models in Python and leveraging the grammar of graphics for plotting. Please take the time to read that post as this is derived and inteded to augment the ideas shared there.\nAt the end of the post, Vincent asked “It would be interesting to see if this approach is appealing to people.” My answer to that is… YES‼️\nI like PyMC and ArviZ a lot, but it was a huge blow coming from R and libraries like tidybayes, bayesplots, and others that helped wrangle and visualize the posterior.\nI fully agree with the approach of ArviZ to work with high-dimensional data, but comming from a stats background it is more intuitive to work with things as dataframes rather than objects whenever it makes sense. This is especially true with visualizations and the grammar of graphics.\nI’m not sure how or what the best way to contribute to this, and it was mentioned on bluesky that GoG-like interface is being developped for ArviZ 1.0. The best thing I can do is create a post for me."
  },
  {
    "objectID": "posts/20250514-pymc-gog/index.html#tidybayes-in-python-would-be-cool",
    "href": "posts/20250514-pymc-gog/index.html#tidybayes-in-python-would-be-cool",
    "title": "TidyPyMC",
    "section": "",
    "text": "A few weeks ago, Benjamin Vincent posted this blog post on using Bayesian models in Python and leveraging the grammar of graphics for plotting. Please take the time to read that post as this is derived and inteded to augment the ideas shared there.\nAt the end of the post, Vincent asked “It would be interesting to see if this approach is appealing to people.” My answer to that is… YES‼️\nI like PyMC and ArviZ a lot, but it was a huge blow coming from R and libraries like tidybayes, bayesplots, and others that helped wrangle and visualize the posterior.\nI fully agree with the approach of ArviZ to work with high-dimensional data, but comming from a stats background it is more intuitive to work with things as dataframes rather than objects whenever it makes sense. This is especially true with visualizations and the grammar of graphics.\nI’m not sure how or what the best way to contribute to this, and it was mentioned on bluesky that GoG-like interface is being developped for ArviZ 1.0. The best thing I can do is create a post for me."
  },
  {
    "objectID": "posts/20250514-pymc-gog/index.html#tidypymc",
    "href": "posts/20250514-pymc-gog/index.html#tidypymc",
    "title": "TidyPyMC",
    "section": "TidyPyMC",
    "text": "TidyPyMC\nThis is definitely subjective, but I think the missing commponent right now is a consistent way to turn the arviz.InferenceData object into a dataframe. Both this and the code Vincent shared is highly custom to the model, and from my experience that is typically a common design choice between R and Python libraries. There is tradeoffs to both paradigms.\nThere are a couple of plots in tidybayes add_epred_draws() and add_predicted_draws() that show some of its capabilities. The goal of this is to replicate them.\nTo accomplish this, we will bring the observed data, linear predictions, and posterior predictions in the same dataframe.\n\nLibraries and data\nWe’ll use the mtcars dataset to replicate some of the tidybayes examples.\n\nimport pandas as pd\nimport pymc as pm\nimport arviz as az\nimport numpy as np\n\nfrom plotnine.data import mtcars\nfrom plotnine import * # elmo_fire.gif namespace, but they mostly start with geom_*\nmtcars.head()\n\n\n\n\n\n\n\n\nname\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\nMazda RX4\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\nDatsun 710\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\nHornet 4 Drive\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n4\nHornet Sportabout\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2"
  },
  {
    "objectID": "posts/20250514-pymc-gog/index.html#model",
    "href": "posts/20250514-pymc-gog/index.html#model",
    "title": "TidyPyMC",
    "section": "Model",
    "text": "Model\nThis is an attempt at replicating the model here.\nThe formula is following mpg ~ hp * cyl fit with brms.\n\n\n\n\n\n\nNote\n\n\n\nThe scope of this wasn’t necessarily to walk through creating a pymc model or walk through the workflow of prior predictive checks, diagnostics, etc. I took the priors from brms::stancode(brms::brm(mpg ~ hp * cyl, data = mtcars)) in R. I also used PyMC over Bambi, but both libraries work off the arviz.InferenceData object.\n\n\n\n# build model and sample posterior\nwith pm.Model(\n    coords={\"obs\": mtcars.index, \"predictors\": ['hp', 'cyl']}\n) as mod:\n\n    X = pm.Data(\"X\", mtcars[[\"hp\", \"cyl\"]], dims=(\"obs\", \"predictors\"))\n\n    alpha = pm.StudentT(\"alpha\", nu=3, mu=19.2, sigma=5.4)\n    sigma = pm.HalfStudentT(\"sigma\", nu=3, sigma=5.54)\n    beta = pm.Normal(\"b\", mu=0, sigma=1, dims='predictors')\n\n    mu = pm.Deterministic(\"mu\", alpha + pm.math.dot(X, beta), dims='obs')\n\n    y = pm.Normal(\n        \"y\",\n        mu=mu,\n        sigma=sigma,\n        shape=X.shape[0],\n        observed=mtcars[\"mpg\"],\n        dims=\"obs\",\n    )\n\n    idata = pm.sample(random_seed=527)\n\n# sample posterior predictive\nwith mod as model:\n    pp = pm.sample_posterior_predictive(idata, predictions=True)\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [alpha, sigma, b]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\nSampling: [y]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy up\nThe biggest takeaway I had from Vincent’s post was it wsa possible to get tidy data out of the arviz.InferenceData object, and this was by far the most difficult part to get my head around.\nFrom idata.posterior, we’ll take three things:\n\nglobal parameters: sigma, alpha\nparameters beta (2)\nlinear predictions mu\n\nThe key is to understand the dimensions of which attribute you want ot get and which ones are the same.\n\nprint(f'alpha: {idata.posterior[\"alpha\"].shape}')\nprint(f'sigma: {idata.posterior[\"sigma\"].shape}')\nprint(f'beta: {idata.posterior[\"b\"].shape}')\nprint(f'mu: {idata.posterior[\"mu\"].shape}')\n\nalpha: (4, 1000)\nsigma: (4, 1000)\nbeta: (4, 1000, 2)\nmu: (4, 1000, 32)\n\n\n\nBoth alpha and sigma are the same shape becuause they are global parameters.\nbeta has the same number of draws, each is represented as a row that will pivot\nmu has the same number of draws but for each observation\n\n\nparams = idata.posterior[[\"sigma\", \"alpha\"]].to_dataframe().reset_index()\nbetas = (\n    idata.posterior[\"b\"]\n    .to_dataframe()\n    .reset_index()\n    .pivot(index=[\"chain\", \"draw\"], columns=\"predictors\", values=\"b\")\n    .reset_index()\n)\n\ndf_posterior = params.merge(betas, on=[\"chain\", \"draw\"])\n\ndf_posterior = (\n    idata.posterior[\"mu\"]\n    .to_dataframe()\n    .reset_index()\n    .merge(mtcars[[\"cyl\", \"mpg\", \"hp\"]], left_on=\"obs\", right_on=mtcars.index)\n    .merge(params, on=[\"chain\", \"draw\"])\n    .merge(betas, on=[\"chain\", \"draw\"], suffixes=[\"\", \"_b\"])\n    .assign( # for plotting later\n        group=lambda x: x.cyl.astype(str)\n        + \"_\"\n        + x.draw.astype(str)\n        + \"_\"\n        + x.chain.astype(str)\n    )\n)\n\ndf_posterior\n\n\n\n\n\n\n\n\nchain\ndraw\nobs\nmu\ncyl\nmpg\nhp\nsigma\nalpha\ncyl_b\nhp_b\ngroup\n\n\n\n\n0\n0\n0\n0\n22.463651\n6\n21.0\n110\n4.074876\n33.091908\n-0.731355\n-0.056728\n6_0_0\n\n\n1\n0\n0\n1\n22.463651\n6\n21.0\n110\n4.074876\n33.091908\n-0.731355\n-0.056728\n6_0_0\n\n\n2\n0\n0\n2\n24.890745\n4\n22.8\n93\n4.074876\n33.091908\n-0.731355\n-0.056728\n4_0_0\n\n\n3\n0\n0\n3\n22.463651\n6\n21.4\n110\n4.074876\n33.091908\n-0.731355\n-0.056728\n6_0_0\n\n\n4\n0\n0\n4\n17.313594\n8\n18.7\n175\n4.074876\n33.091908\n-0.731355\n-0.056728\n8_0_0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n127995\n3\n999\n27\n24.983972\n4\n30.4\n113\n2.481254\n35.978550\n-2.033584\n-0.025312\n4_999_3\n\n\n127996\n3\n999\n28\n13.027543\n8\n15.8\n264\n2.481254\n35.978550\n-2.033584\n-0.025312\n8_999_3\n\n\n127997\n3\n999\n29\n19.347468\n6\n19.7\n175\n2.481254\n35.978550\n-2.033584\n-0.025312\n6_999_3\n\n\n127998\n3\n999\n30\n11.230399\n8\n15.0\n335\n2.481254\n35.978550\n-2.033584\n-0.025312\n8_999_3\n\n\n127999\n3\n999\n31\n25.085220\n4\n21.4\n109\n2.481254\n35.978550\n-2.033584\n-0.025312\n4_999_3\n\n\n\n\n128000 rows × 12 columns\n\n\n\nThe critical takeaway I had from implementing this was learn to leverage coords and dims in the model container. This makes it easier to work with the data later, especially as dimensions increase (i.e. groups in a multilevel model).\nThis sets the posterior to be represented in a tidy dataframe, exactly how Vincent did it. I’m really interested what ways we can cofigure the atributes with PyMC to make this generalize across different models and data.\n\n\n\n\n\n\nNote\n\n\n\nI’ve come back to this and realized adding in the parameters doesn’t make it “tidy”, and also doesn’t get used in the plots. It is still beneficial to include how to go about joining the parameters posterior draws to the data, and anyway… this post is for future me.\n\n\n\n\nPosterior predictive\nOne way would be to do this would be to use arviz.summary() on the sampled posterior predictions. This is a common workflow I would do with brms and tidybayes of parsing parameter outputs name to match the group, or join an id with the original dataset.\n\ndf_predictions = az.summary(pp)\n\n/Users/ryan/git-repos/plain-data/.venv/lib/python3.12/site-packages/arviz/stats/stats.py:1359: UserWarning: Selecting first found group: predictions\n\n\nNext steps to join it with the observed data.\n\ndf_predictions.index = df_predictions.index.str.extract(r\"y\\[(.*?)\\]\")[0]\n\ndf_predictions = df_predictions.merge(mtcars[[\"hp\", \"cyl\", \"mpg\"]], on=df_predictions.index)\ndf_predictions.head()\n\n\n\n\n\n\n\n\nkey_0\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\nhp\ncyl\nmpg\n\n\n\n\n0\n0\n21.567\n3.507\n14.961\n28.209\n0.056\n0.040\n3977.0\n4038.0\n1.0\n110\n6\n21.0\n\n\n1\n1\n21.535\n3.521\n15.004\n28.279\n0.057\n0.043\n3863.0\n3513.0\n1.0\n110\n6\n21.0\n\n\n2\n2\n25.074\n3.516\n18.532\n31.826\n0.061\n0.041\n3284.0\n3781.0\n1.0\n93\n4\n22.8\n\n\n3\n3\n21.548\n3.522\n14.778\n27.923\n0.056\n0.042\n4012.0\n3836.0\n1.0\n110\n6\n21.4\n\n\n4\n4\n16.346\n3.476\n9.992\n22.838\n0.058\n0.043\n3595.0\n3588.0\n1.0\n175\n8\n18.7\n\n\n\n\n\n\n\nThis worked well with the named index on mtcars. I’m not a fan of pandas, and I’ve long forgotten a lot of tips and tricks to work with the nuances of pandas after a couple of years of using polars. For future me, I’m going to include a standarad approach of working with the posterior.\n\ndf_posterior_predictive = (\n    pp.predictions[\"y\"]\n    .to_dataframe()\n    .reset_index()\n    .merge(mtcars[[\"cyl\", \"hp\"]], left_on=\"obs\", right_on=mtcars.index)\n)\n\ndf_posterior_predictive\n\n\n\n\n\n\n\n\nchain\ndraw\nobs\ny\ncyl\nhp\n\n\n\n\n0\n0\n0\n0\n19.189177\n6\n110\n\n\n1\n0\n0\n1\n24.255234\n6\n110\n\n\n2\n0\n0\n2\n21.979800\n4\n93\n\n\n3\n0\n0\n3\n24.701108\n6\n110\n\n\n4\n0\n0\n4\n17.615786\n8\n175\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n127995\n3\n999\n27\n20.405657\n4\n113\n\n\n127996\n3\n999\n28\n12.332614\n8\n264\n\n\n127997\n3\n999\n29\n17.688123\n6\n175\n\n\n127998\n3\n999\n30\n9.946518\n8\n335\n\n\n127999\n3\n999\n31\n25.946411\n4\n109\n\n\n\n\n128000 rows × 6 columns\n\n\n\nThe data is aggregated to match the az.summary() output since this particular geom_ribbon() visualization will only need the HDI values of the posterior predictive distribution.\n\ndf_predictions = (\n    df_posterior_predictive.groupby([\"obs\", \"cyl\", \"hp\"])\n    .agg(\n        pp_mean=(\"y\", \"mean\"),\n        pp_min=(\"y\", lambda x: x.quantile(0.03)),\n        pp_max=(\"y\", lambda x: x.quantile(0.97)),\n    )\n    .reset_index()\n)\ndf_predictions.head()\n\n\n\n\n\n\n\n\nobs\ncyl\nhp\npp_mean\npp_min\npp_max\n\n\n\n\n0\n0\n6\n110\n21.567234\n14.846709\n28.158264\n\n\n1\n1\n6\n110\n21.534675\n15.065439\n28.370696\n\n\n2\n2\n4\n93\n25.073569\n18.084567\n31.503317\n\n\n3\n3\n6\n110\n21.548498\n14.891987\n28.068302\n\n\n4\n4\n8\n175\n16.345941\n9.864550\n22.708540\n\n\n\n\n\n\n\n\n\nThe plot is coming together\nPlotnine! With the grammar of graphics, we’re able to:\n\nuse different datasets\nlayer aesthetics together\nthink about plots with data\n\n\n# sample draws for plotting purposes\nsamples = np.random.choice(\n    [x for x in range(999)], size=int(5), replace=False\n)\n\n(\n    ggplot(mtcars, aes(\"hp\", \"mpg\", color=\"factor(cyl)\", fill=\"factor(cyl)\"))\n    + geom_ribbon(\n        aes(y=\"pp_mean\", ymin=\"pp_min\", ymax=\"pp_max\"), data=df_predictions, alpha=0.2\n    )\n    + geom_line(\n        aes(y=\"mu\", group=\"group\"),\n        data=df_posterior[df_posterior.draw.isin(samples)],\n        alpha=0.6,\n    )\n    + geom_point()\n    + theme_minimal()\n    + labs(color='cyl', fill='cyl')\n)"
  },
  {
    "objectID": "posts/2025-05-31-reticulate-pymc/index.html",
    "href": "posts/2025-05-31-reticulate-pymc/index.html",
    "title": "Rython",
    "section": "",
    "text": "This is a nod to the title of Benjamin T. Vincent’s blog post, who inspired me to dive further into PyMC. The reason I started looking into using the grammar of graphics with PyMC, was to reduce a barrier and emulate tidyverse as much as possible. What if instead we just… use R and PyMC?\nIn my previous post, I used the Grammar of Graphics to vizualize the posterior distributions from a PyMC model. This required extracting the data from the az.InferenceData object, and converting to a dataframe to work with.\nI recently got a chance to use the new version of reticulate, which uses uv to manage the Python environment used in an R session, and fell in love. From the post:\n\nwith py_require(), Reticulate will automatically create and manage Python environments behind the scenes so you don’t have to.\n\nI’m a huge fan of uv, and how the developers of Reticulate integrated it really simplifies the process of bringing Python and R together.\n\nWhy Not Both Take Both GIFfrom Why Not Both GIFs"
  },
  {
    "objectID": "posts/2025-05-31-reticulate-pymc/index.html#bayesian-models-python-grammar-of-graphics-r",
    "href": "posts/2025-05-31-reticulate-pymc/index.html#bayesian-models-python-grammar-of-graphics-r",
    "title": "Rython",
    "section": "",
    "text": "This is a nod to the title of Benjamin T. Vincent’s blog post, who inspired me to dive further into PyMC. The reason I started looking into using the grammar of graphics with PyMC, was to reduce a barrier and emulate tidyverse as much as possible. What if instead we just… use R and PyMC?\nIn my previous post, I used the Grammar of Graphics to vizualize the posterior distributions from a PyMC model. This required extracting the data from the az.InferenceData object, and converting to a dataframe to work with.\nI recently got a chance to use the new version of reticulate, which uses uv to manage the Python environment used in an R session, and fell in love. From the post:\n\nwith py_require(), Reticulate will automatically create and manage Python environments behind the scenes so you don’t have to.\n\nI’m a huge fan of uv, and how the developers of Reticulate integrated it really simplifies the process of bringing Python and R together.\n\nWhy Not Both Take Both GIFfrom Why Not Both GIFs"
  },
  {
    "objectID": "posts/2025-05-31-reticulate-pymc/index.html#again-but-why",
    "href": "posts/2025-05-31-reticulate-pymc/index.html#again-but-why",
    "title": "Reticulate and PyMC",
    "section": "Again… but why?",
    "text": "Again… but why?"
  },
  {
    "objectID": "posts/2025-05-31-reticulate-pymc/index.html#workflow",
    "href": "posts/2025-05-31-reticulate-pymc/index.html#workflow",
    "title": "Rython",
    "section": "Workflow",
    "text": "Workflow\n\nSet up the environment\nLoad up reticulate and the tidyverse.\n\n\n\n\n\n\nNote\n\n\n\n\n\nI set the environment variable for RETICULATE_PYTHON to force Reticulate to use an ephemeral environment. I didn’t have to do this in an interactive session, but this blog already had a uv proejct setup - and I didn’t want it to be used. This could also be configured outside the script or workflow.\n\n\n\n\nSys.setenv(RETICULATE_PYTHON = \"managed\")\nlibrary(reticulate)\nlibrary(tidyverse)\nmtcars |&gt;\n  head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\nValiant\n18.1\n6\n225\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\n\n\n\n\nThe first difference in this workflow is that mtcars is a dataset available by default. It can be accessed directly within an R session.\n\n\nReticulate and uv environment\nI’ve listend to enough presentations and interviews from Charlie Marsh to know I can’t do it justice to explain how uv works. An oversimplification of uv is that it centralizes package downloads, and then distributes them when needed for new isolated enviornments, after resolving the dependencies. The key difference is that there isn’t reinstallation of packages needed across environements.\nUsing py_require() will specify which packages are needed and how to create a virtual environment.\n\npy_require('pymc')\npy_config()\n\npython:         /Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/gKtLQ4Ys-srj2T0OKKL-t/bin/python3\nlibpython:      /Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/python/cpython-3.11.12-macos-aarch64-none/lib/libpython3.11.dylib\npythonhome:     /Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/gKtLQ4Ys-srj2T0OKKL-t:/Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/gKtLQ4Ys-srj2T0OKKL-t\nvirtualenv:     /Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/gKtLQ4Ys-srj2T0OKKL-t/bin/activate_this.py\nversion:        3.11.12 (main, May 30 2025, 05:53:55) [Clang 20.1.4 ]\nnumpy:          /Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/gKtLQ4Ys-srj2T0OKKL-t/lib/python3.11/site-packages/numpy\nnumpy_version:  2.2.6\n\nNOTE: Python version was forced by py_require()\n\n\nYou can see that we have an ephemeral python environment created with uv and reticulate. This is really neat! Everytime I render this document, it will cache a new virtual environment for reticulate to use, but because of uv I only had to download pymc the first time.\nSpecifying the pymc dependency actually built a Python environment with 63 packages, all mapped and configured with uv. I’ve shown some of the most well-known dependencies of pymc.\n\n# &gt; py_list_packages() |&gt; dim()\n# [1] 63  3\n\npy_list_packages() |&gt; dplyr::filter(package %in% c('pandas', 'scipy', 'matplotlib', 'arviz'))\n\n\n\n\n\npackage\nversion\nrequirement\n\n\n\n\narviz\n0.21.0\narviz==0.21.0\n\n\nmatplotlib\n3.10.3\nmatplotlib==3.10.3\n\n\npandas\n2.2.3\npandas==2.2.3\n\n\nscipy\n1.15.3\nscipy==1.15.3\n\n\n\n\n\n\n\n\nSet up the data\nIn my last post, I incorrectly implemented the formula for the interaction model mpg ~ hp * cyl. I needed to explictly add in a variable so that we have mpg ~ hp + cyl + hp:cyl. To correct that, I’v added a variable that lm() or brms would typically do under the hood. Additionally added in centering the variables to help prevent divergece issues.\nA significant use case for me to use R in the workflow is to be able to do the data wrangling with the tidyverse to set up the data to input into the model. In this example, I used dplyr::mutate() to scale the variables. You can imagine other types of transformations, filters, and joins that could be dropped in here.\n\n\n\n\n\n\nNote\n\n\n\n\n\nStandardScaler does a fine job of this in the python ecosytem, so scaling isn’t necessarily the point.\n\n\n\n\nmtcars_scaled &lt;- mtcars %&gt;%\n  mutate(\n    hp_c = scale(hp)[, 1], # scale() keeps attributes that need to be removed\n    cyl_c = scale(cyl)[, 1],\n    hp_cyl = hp_c * cyl_c\n  )\n\n\n\nR to Python\n\npandas_mtcars &lt;- r_to_py(mtcars_scaled)\nprint(class(mtcars_scaled))\n\n[1] \"data.frame\"\n\nprint(class(pandas_mtcars))\n\n[1] \"pandas.core.frame.DataFrame\"        \"pandas.core.generic.NDFrame\"       \n[3] \"pandas.core.base.PandasObject\"      \"pandas.core.accessor.DirNamesMixin\"\n[5] \"pandas.core.indexing.IndexingMixin\" \"pandas.core.arraylike.OpsMixin\"    \n[7] \"python.builtin.object\"             \n\n\nThere are now two datasets:\n\nmtcars_scaled: an R data.frame() object\npandas_mtcars: a Python pandas.DataFrame() object\n\nThe pandas dataframe can be passed into PyMC and begin the Python portion of the workflow.\n\n\n\n\n\n\nWarning\n\n\n\nIntegrating Python and R has come a long way, and is incredibly accessible. There are some edge cases and things to be aware of when converting data and objects between the two. This post by Karin Hrovatin is one of the best consolidated sources of information to learn from.\n\n\n\n\nPyMC Model\nThe python sytax would look very similar to this, only now instead of using objects with dot notation, we access methods and attributes with the $ character. If you are purely a python developer, this might look obscene. I choose to put up with this quirkyness because I find working with dataframes and plotting in R worth the trade off.\n\nPython\nReviewing the Python model from before:\n\nwith pm.Model(\n    coords={\"obs\": mtcars.index, \"predictors\": ['hp', 'cyl']}\n) as mod:\n\n    X = pm.Data(\"X\", mtcars[[\"hp\", \"cyl\"]], dims=(\"obs\", \"predictors\"))\n\n    alpha = pm.StudentT(\"alpha\", nu=3, mu=19.2, sigma=5.4)\n    sigma = pm.HalfStudentT(\"sigma\", nu=3, sigma=5.54)\n    beta = pm.Normal(\"b\", mu=0, sigma=1, dims='predictors')\n\n    mu = pm.Deterministic(\"mu\", alpha + pm.math.dot(X, beta), dims='obs')\n\n    y = pm.Normal(\n        \"y\",\n        mu=mu,\n        sigma=sigma,\n        shape=X.shape[0],\n        observed=mtcars[\"mpg\"],\n        dims=\"obs\",\n    )\n\n    idata = pm.sample(random_seed=527)\n\n# sample posterior predictive\nwith mod as model:\n    pp = pm.sample_posterior_predictive(idata, predictions=True)\n\n\n\nR\nNow the same thing, but using reticulate to interface with pymc. I also add in the new column hp_cyl for the interaction term.\n\n# import pymc as pm\npm &lt;- import('pymc', convert = FALSE)\n\nmod &lt;- pm$Model(\n  coords = list(\n    car = pandas_mtcars$index,\n    predictors = c('hp', 'cyl', 'hp_cyl')\n  )\n)\n\n# with pm.Model() as model:\n# ...\nwith(mod, {\n  X &lt;- pm$Data('X', pandas_mtcars$loc[, c('hp', 'cyl', 'hp_cyl')])\n\n  alpha &lt;- pm$StudentT(\"alpha\", nu = 3, mu = 19.2, sigma = 5.4)\n  sigma &lt;- pm$HalfStudentT(\"sigma\", nu = 3, sigma = 5.54)\n  beta &lt;- pm$Normal(\"b\", mu = 0, sigma = 1, dims = 'predictors')\n  mu &lt;- pm$Deterministic(\"mu\", alpha + pm$math$dot(X, beta), dims = 'car')\n\n  y &lt;- pm$Normal(\n    \"y\",\n    mu = mu,\n    sigma = sigma,\n    shape = X$shape[0], # python index\n    observed = pandas_mtcars$mpg,\n    dims = \"car\",\n  )\n  # using a single core and chain because of Quarto page rendering,\n  # normally this would be 4 chains\n  idata = pm$sample(random_seed = 527L, cores = 1L, chains=1L)\n})\n\n                                                                                \n                              Step      Grad      Sampli…                       \n  Progre…   Draws   Diverg…   size      evals     Speed     Elapsed   Remaini…  \n ────────────────────────────────────────────────────────────────────────────── \n  ━━━━━━━   2000    0         0.19      15        1458.41   0:00:01   0:00:00   \n                                                  draws/s                       \n                                                                                \n\nwith(mod, {\n  pp = pm$sample_posterior_predictive(idata, predictions = TRUE)\n})\n\nSampling ... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00 / 0:00:00\n\n\nAs mentioned earlier, there are some caveats to understand when moving between the two languages. There were definitely a few gotcha’s when I was implementing this:\n\npandas_mtcars$loc[, c('hp', 'cyl', 'hp_cyl')]: this used the .loc pandas method but with the R data.frame() indexing syntax.\nX$shape[0]: Python is 0-based indexed while R is 1-based. Because this is a Python object, it can be indexed with 0. (I agree this is a bit weird to combine 😅)\nrandom_seed = 527L: Integer types need to be specified with the R syntax of adding L at the end of it.\n\n\n\n\nPosterior\nTranslating the PyMC model’s az.InferenceData object and posterior predictions to dataframes is still the same, with the addition of py_to_r() to convert a pandas dataframe to R.\n\n# posterior mu\ndf_posterior &lt;- idata$posterior$mu$to_dataframe()$reset_index() |&gt;\n  py_to_r() |&gt;\n  as_tibble() |&gt;\n  left_join(rownames_to_column(mtcars, 'car')) |&gt; # R mtcars has rownames for the car\n  mutate(group = paste0(chain, draw, cyl)) # for a particular plot later\n\nJoining with `by = join_by(car)`\n\n# posterior predictions of mpg\ndf_predictions &lt;- pp$predictions$to_dataframe()$reset_index() |&gt;\n  py_to_r() |&gt;\n  as_tibble() |&gt;\n  left_join(rownames_to_column(mtcars, \"car\"))\n\nJoining with `by = join_by(car)`\n\n\n\n\nPlot\nNow for my favorite part of this, plot with ggplot2 and use tidybayes directly!\n\ndf_predictions |&gt;\n  ggplot(aes(hp, y, color = as.factor(cyl))) +\n  tidybayes::stat_lineribbon(.width = c(.99, .95, .8, .5), alpha = 0.25) +\n  geom_line(\n    aes(y = mu, group = group),\n    data = df_posterior |&gt; filter(draw %in% round(seq(5, 900, length.out = 5))),\n    alpha = 0.38\n  ) +\n  geom_point(aes(y = mpg), data = mtcars, shape = 21, size = 2, stroke = 1) +\n  scale_fill_brewer(palette = \"Greys\") +\n  theme_light(base_size = 12) +\n  guides(fill = 'none') +\n  labs(\n    x = 'hp',\n    y = 'mpg',\n    color = 'cyl'\n  )"
  },
  {
    "objectID": "posts/2025-05-14-tidy-pymc/index.html",
    "href": "posts/2025-05-14-tidy-pymc/index.html",
    "title": "TidyPyMC",
    "section": "",
    "text": "A few weeks ago, Benjamin Vincent posted this blog post on using Bayesian models in Python and leveraging the grammar of graphics for plotting. Please take the time to read that post as this is derived and inteded to augment the ideas shared there.\nAt the end of the post, Vincent asked “It would be interesting to see if this approach is appealing to people.” My answer to that is… YES‼️\nI like PyMC and ArviZ a lot, but it was a huge blow coming from R and libraries like tidybayes, bayesplots, and others that helped wrangle and visualize the posterior.\nI fully agree with the approach of ArviZ to work with high-dimensional data, but comming from a stats background it is more intuitive to work with things as dataframes rather than objects whenever it makes sense. This is especially true with visualizations and the grammar of graphics.\nI’m not sure how or what the best way to contribute to this, and it was mentioned on bluesky that GoG-like interface is being developped for ArviZ 1.0. The best thing I can do is create a post for me."
  },
  {
    "objectID": "posts/2025-05-14-tidy-pymc/index.html#tidybayes-in-python-would-be-cool",
    "href": "posts/2025-05-14-tidy-pymc/index.html#tidybayes-in-python-would-be-cool",
    "title": "TidyPyMC",
    "section": "",
    "text": "A few weeks ago, Benjamin Vincent posted this blog post on using Bayesian models in Python and leveraging the grammar of graphics for plotting. Please take the time to read that post as this is derived and inteded to augment the ideas shared there.\nAt the end of the post, Vincent asked “It would be interesting to see if this approach is appealing to people.” My answer to that is… YES‼️\nI like PyMC and ArviZ a lot, but it was a huge blow coming from R and libraries like tidybayes, bayesplots, and others that helped wrangle and visualize the posterior.\nI fully agree with the approach of ArviZ to work with high-dimensional data, but comming from a stats background it is more intuitive to work with things as dataframes rather than objects whenever it makes sense. This is especially true with visualizations and the grammar of graphics.\nI’m not sure how or what the best way to contribute to this, and it was mentioned on bluesky that GoG-like interface is being developped for ArviZ 1.0. The best thing I can do is create a post for me."
  },
  {
    "objectID": "posts/2025-05-14-tidy-pymc/index.html#tidypymc",
    "href": "posts/2025-05-14-tidy-pymc/index.html#tidypymc",
    "title": "TidyPyMC",
    "section": "TidyPyMC",
    "text": "TidyPyMC\nThis is definitely subjective, but I think the missing commponent right now is a consistent way to turn the arviz.InferenceData object into a dataframe. Both this and the code Vincent shared is highly custom to the model, and from my experience that is typically a common design choice between R and Python libraries. There is tradeoffs to both paradigms.\nThere are a couple of plots in tidybayes add_epred_draws() and add_predicted_draws() that show some of its capabilities. The goal of this is to replicate them.\nTo accomplish this, we will bring the observed data, linear predictions, and posterior predictions in the same dataframe.\n\nLibraries and data\nWe’ll use the mtcars dataset to replicate some of the tidybayes examples.\n\nimport pandas as pd\nimport pymc as pm\nimport arviz as az\nimport numpy as np\n\nfrom plotnine.data import mtcars\nfrom plotnine import * # elmo_fire.gif namespace, but they mostly start with geom_*\nmtcars.head()\n\n\n\n\n\n\n\n\nname\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\nMazda RX4\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\nDatsun 710\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\nHornet 4 Drive\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n4\nHornet Sportabout\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2"
  },
  {
    "objectID": "posts/2025-05-14-tidy-pymc/index.html#model",
    "href": "posts/2025-05-14-tidy-pymc/index.html#model",
    "title": "TidyPyMC",
    "section": "Model",
    "text": "Model\nThis is an attempt at replicating the model here.\nThe formula is following mpg ~ hp * cyl fit with brms.\n\n\n\n\n\n\nNote\n\n\n\nThe scope of this wasn’t necessarily to walk through creating a pymc model or walk through the workflow of prior predictive checks, diagnostics, etc. I took the priors from brms::stancode(brms::brm(mpg ~ hp * cyl, data = mtcars)) in R. I also used PyMC over Bambi, but both libraries work off the arviz.InferenceData object.\n\n\n\n# build model and sample posterior\nwith pm.Model(\n    coords={\"obs\": mtcars.index, \"predictors\": ['hp', 'cyl']}\n) as mod:\n\n    X = pm.Data(\"X\", mtcars[[\"hp\", \"cyl\"]], dims=(\"obs\", \"predictors\"))\n\n    alpha = pm.StudentT(\"alpha\", nu=3, mu=19.2, sigma=5.4)\n    sigma = pm.HalfStudentT(\"sigma\", nu=3, sigma=5.54)\n    beta = pm.Normal(\"b\", mu=0, sigma=1, dims='predictors')\n\n    mu = pm.Deterministic(\"mu\", alpha + pm.math.dot(X, beta), dims='obs')\n\n    y = pm.Normal(\n        \"y\",\n        mu=mu,\n        sigma=sigma,\n        shape=X.shape[0],\n        observed=mtcars[\"mpg\"],\n        dims=\"obs\",\n    )\n\n    idata = pm.sample(random_seed=527)\n\n# sample posterior predictive\nwith mod as model:\n    pp = pm.sample_posterior_predictive(idata, predictions=True)\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [alpha, sigma, b]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 2 seconds.\nSampling: [y]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy up\nThe biggest takeaway I had from Vincent’s post was it wsa possible to get tidy data out of the arviz.InferenceData object, and this was by far the most difficult part to get my head around.\nFrom idata.posterior, we’ll take three things:\n\nglobal parameters: sigma, alpha\nparameters beta (2)\nlinear predictions mu\n\nThe key is to understand the dimensions of which attribute you want ot get and which ones are the same.\n\nprint(f'alpha: {idata.posterior[\"alpha\"].shape}')\nprint(f'sigma: {idata.posterior[\"sigma\"].shape}')\nprint(f'beta: {idata.posterior[\"b\"].shape}')\nprint(f'mu: {idata.posterior[\"mu\"].shape}')\n\nalpha: (4, 1000)\nsigma: (4, 1000)\nbeta: (4, 1000, 2)\nmu: (4, 1000, 32)\n\n\n\nBoth alpha and sigma are the same shape becuause they are global parameters.\nbeta has the same number of draws, each is represented as a row that will pivot\nmu has the same number of draws but for each observation\n\n\nparams = idata.posterior[[\"sigma\", \"alpha\"]].to_dataframe().reset_index()\nbetas = (\n    idata.posterior[\"b\"]\n    .to_dataframe()\n    .reset_index()\n    .pivot(index=[\"chain\", \"draw\"], columns=\"predictors\", values=\"b\")\n    .reset_index()\n)\n\ndf_posterior = params.merge(betas, on=[\"chain\", \"draw\"])\n\ndf_posterior = (\n    idata.posterior[\"mu\"]\n    .to_dataframe()\n    .reset_index()\n    .merge(mtcars[[\"cyl\", \"mpg\", \"hp\"]], left_on=\"obs\", right_on=mtcars.index)\n    .merge(params, on=[\"chain\", \"draw\"])\n    .merge(betas, on=[\"chain\", \"draw\"], suffixes=[\"\", \"_b\"])\n    .assign( # for plotting later\n        group=lambda x: x.cyl.astype(str)\n        + \"_\"\n        + x.draw.astype(str)\n        + \"_\"\n        + x.chain.astype(str)\n    )\n)\n\ndf_posterior\n\n\n\n\n\n\n\n\nchain\ndraw\nobs\nmu\ncyl\nmpg\nhp\nsigma\nalpha\ncyl_b\nhp_b\ngroup\n\n\n\n\n0\n0\n0\n0\n22.463651\n6\n21.0\n110\n4.074876\n33.091908\n-0.731355\n-0.056728\n6_0_0\n\n\n1\n0\n0\n1\n22.463651\n6\n21.0\n110\n4.074876\n33.091908\n-0.731355\n-0.056728\n6_0_0\n\n\n2\n0\n0\n2\n24.890745\n4\n22.8\n93\n4.074876\n33.091908\n-0.731355\n-0.056728\n4_0_0\n\n\n3\n0\n0\n3\n22.463651\n6\n21.4\n110\n4.074876\n33.091908\n-0.731355\n-0.056728\n6_0_0\n\n\n4\n0\n0\n4\n17.313594\n8\n18.7\n175\n4.074876\n33.091908\n-0.731355\n-0.056728\n8_0_0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n127995\n3\n999\n27\n24.983972\n4\n30.4\n113\n2.481254\n35.978550\n-2.033584\n-0.025312\n4_999_3\n\n\n127996\n3\n999\n28\n13.027543\n8\n15.8\n264\n2.481254\n35.978550\n-2.033584\n-0.025312\n8_999_3\n\n\n127997\n3\n999\n29\n19.347468\n6\n19.7\n175\n2.481254\n35.978550\n-2.033584\n-0.025312\n6_999_3\n\n\n127998\n3\n999\n30\n11.230399\n8\n15.0\n335\n2.481254\n35.978550\n-2.033584\n-0.025312\n8_999_3\n\n\n127999\n3\n999\n31\n25.085220\n4\n21.4\n109\n2.481254\n35.978550\n-2.033584\n-0.025312\n4_999_3\n\n\n\n\n128000 rows × 12 columns\n\n\n\nThe critical takeaway I had from implementing this was learn to leverage coords and dims in the model container. This makes it easier to work with the data later, especially as dimensions increase (i.e. groups in a multilevel model).\nThis sets the posterior to be represented in a tidy dataframe, exactly how Vincent did it. I’m really interested what ways we can cofigure the atributes with PyMC to make this generalize across different models and data.\n\n\n\n\n\n\nNote\n\n\n\nI’ve come back to this and realized adding in the parameters doesn’t make it “tidy”, and also doesn’t get used in the plots. It is still beneficial to include how to go about joining the parameters posterior draws to the data, and anyway… this post is for future me.\n\n\n\n\nPosterior predictive\nOne way would be to do this would be to use arviz.summary() on the sampled posterior predictions. This is a common workflow I would do with brms and tidybayes of parsing parameter outputs name to match the group, or join an id with the original dataset.\n\ndf_predictions = az.summary(pp)\n\n/Users/ryan/git-repos/plain-data/.venv/lib/python3.12/site-packages/arviz/stats/stats.py:1359: UserWarning: Selecting first found group: predictions\n\n\nNext steps to join it with the observed data.\n\ndf_predictions.index = df_predictions.index.str.extract(r\"y\\[(.*?)\\]\")[0]\n\ndf_predictions = df_predictions.merge(mtcars[[\"hp\", \"cyl\", \"mpg\"]], on=df_predictions.index)\ndf_predictions.head()\n\n\n\n\n\n\n\n\nkey_0\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\nhp\ncyl\nmpg\n\n\n\n\n0\n0\n21.554\n3.518\n14.560\n27.887\n0.061\n0.044\n3292.0\n3666.0\n1.0\n110\n6\n21.0\n\n\n1\n1\n21.470\n3.503\n14.949\n27.871\n0.056\n0.044\n3869.0\n3845.0\n1.0\n110\n6\n21.0\n\n\n2\n2\n25.176\n3.573\n17.968\n31.616\n0.059\n0.044\n3723.0\n3477.0\n1.0\n93\n4\n22.8\n\n\n3\n3\n21.540\n3.545\n14.597\n27.981\n0.056\n0.042\n3940.0\n3775.0\n1.0\n110\n6\n21.4\n\n\n4\n4\n16.393\n3.563\n10.187\n23.717\n0.057\n0.046\n3934.0\n3670.0\n1.0\n175\n8\n18.7\n\n\n\n\n\n\n\nThis worked well with the named index on mtcars. I’m not a fan of pandas, and I’ve long forgotten a lot of tips and tricks to work with the nuances of pandas after a couple of years of using polars. For future me, I’m going to include a standarad approach of working with the posterior.\n\ndf_posterior_predictive = (\n    pp.predictions[\"y\"]\n    .to_dataframe()\n    .reset_index()\n    .merge(mtcars[[\"cyl\", \"hp\"]], left_on=\"obs\", right_on=mtcars.index)\n)\n\ndf_posterior_predictive\n\n\n\n\n\n\n\n\nchain\ndraw\nobs\ny\ncyl\nhp\n\n\n\n\n0\n0\n0\n0\n19.104229\n6\n110\n\n\n1\n0\n0\n1\n25.039315\n6\n110\n\n\n2\n0\n0\n2\n24.256567\n4\n93\n\n\n3\n0\n0\n3\n19.098882\n6\n110\n\n\n4\n0\n0\n4\n15.877768\n8\n175\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n127995\n3\n999\n27\n26.447811\n4\n113\n\n\n127996\n3\n999\n28\n8.939379\n8\n264\n\n\n127997\n3\n999\n29\n19.937614\n6\n175\n\n\n127998\n3\n999\n30\n15.226820\n8\n335\n\n\n127999\n3\n999\n31\n29.357202\n4\n109\n\n\n\n\n128000 rows × 6 columns\n\n\n\nThe data is aggregated to match the az.summary() output since this particular geom_ribbon() visualization will only need the HDI values of the posterior predictive distribution.\n\ndf_predictions = (\n    df_posterior_predictive.groupby([\"obs\", \"cyl\", \"hp\"])\n    .agg(\n        pp_mean=(\"y\", \"mean\"),\n        pp_min=(\"y\", lambda x: x.quantile(0.03)),\n        pp_max=(\"y\", lambda x: x.quantile(0.97)),\n    )\n    .reset_index()\n)\ndf_predictions.head()\n\n\n\n\n\n\n\n\nobs\ncyl\nhp\npp_mean\npp_min\npp_max\n\n\n\n\n0\n0\n6\n110\n21.554171\n14.872339\n28.276621\n\n\n1\n1\n6\n110\n21.469598\n14.971328\n27.914067\n\n\n2\n2\n4\n93\n25.175523\n18.266339\n31.997126\n\n\n3\n3\n6\n110\n21.539521\n14.693196\n28.122148\n\n\n4\n4\n8\n175\n16.392580\n9.748206\n23.448341\n\n\n\n\n\n\n\n\n\nThe plot is coming together\nPlotnine! With the grammar of graphics, we’re able to:\n\nuse different datasets\nlayer aesthetics together\nthink about plots in terms of data\n\n\n# sample draws for plotting purposes\nsamples = np.random.choice(\n    [x for x in range(999)], size=int(5), replace=False\n)\n\n(\n    ggplot(mtcars, aes(\"hp\", \"mpg\", color=\"factor(cyl)\", fill=\"factor(cyl)\"))\n    + geom_ribbon(\n        aes(y=\"pp_mean\", ymin=\"pp_min\", ymax=\"pp_max\"), data=df_predictions, alpha=0.2\n    )\n    + geom_line(\n        aes(y=\"mu\", group=\"group\"),\n        data=df_posterior[df_posterior.draw.isin(samples)],\n        alpha=0.6,\n    )\n    + geom_point()\n    + theme_minimal()\n    + labs(color='cyl', fill='cyl')\n)"
  },
  {
    "objectID": "posts/2025-05-31-reticulate-pymc/index.html#summary",
    "href": "posts/2025-05-31-reticulate-pymc/index.html#summary",
    "title": "Reticulate and PyMC",
    "section": "Summary",
    "text": "Summary\nThis is an opinionated way of using PyMC and the grammar of graphics together to say the least. I really do like pymc, but I prefer to settle on the data I want to use and other parts of the model iteration with R. There is potential for reticulate::py_run_string() as well, if you wanted to be able to drop it directly back into a pure Python environment. Access to an LLM would also be able to easily reformat the R PyMC model to Python or at least get it most of the way there."
  },
  {
    "objectID": "posts/2025-05-31-reticulate-pymc/index.html#rython",
    "href": "posts/2025-05-31-reticulate-pymc/index.html#rython",
    "title": "Rython",
    "section": "Rython",
    "text": "Rython\nThis is an opinionated way of using PyMC and the grammar of graphics together to say the least. I really do like PyMC, but I prefer to settle on the data and other parts of the model iteration process with R if possible. There is potential for reticulate::py_run_string() as well, if you wanted to be able to drop it directly back into a pure Python environment. Access to an LLM would also be able to easily reformat the R-PyMC model to Python, or at least get it most of the way there.\nOverall really impressed with the state of combing R and Python together. When I started my career, there weren’t a ton of options outside of doing a ton of I/O with csv’s to bring them together. Now within the same process I can use a full-feature Python probablistic programming library, with non-standard evaluation for data transfromation and visualization in R.\nA typo I had in this at one point was Rython, and given my name.. and I quite like it."
  },
  {
    "objectID": "about.html#bio",
    "href": "about.html#bio",
    "title": "About",
    "section": "",
    "text": "I’m an all things data professional living in Colorado Springs, currently working in healthcare supporting their provider network infrastructure. Previously I worked in tech doing product analytics and data science. Prior to that I worked in aviation simulating passenger traffic through airport facilties.\nI’m passionate about making things more efficient, and a proponent of working with what tool and environment works best for you. Designing designing solutions that are inclusive for everyone is something I enjoy.\nIn my free time I love exploring the outdoors, climbing, yoga, snowboarding, and spending time with my wife, dogs, and cat (if she lets me)."
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nKaiser Permanente | Data Reporiting and Analytics Consultant\nClover | Sr. Data Analyst\nSouthwest Airlines | Data Analyst"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nGeorgia Institute of Technology | Atlanta, GA\nM.S. in Analytics\nThe University of North Texas | Denton, TX\nB.B.A. in Business Analytics"
  }
]