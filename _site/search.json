[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Just Plain Data",
    "section": "",
    "text": "Statistical Rethinking\n\n\n\n\n\n\n\nEssay\n\n\n\n\nTribute to Richard McElreath’s Statistical Rethinking\n\n\n\n\n\n\nJan 6, 2026\n\n\nRyan Plain\n\n\n\n\n\n\n  \n\n\n\n\nRython\n\n\n\n\n\n\n\nBayesian\n\n\nPyMC\n\n\nReticulate\n\n\n\n\nBayesian Modeling with PyMC in R with Reticulate.\n\n\n\n\n\n\nMay 31, 2025\n\n\nRyan Plain\n\n\n\n\n\n\n  \n\n\n\n\nTidyPyMC\n\n\n\n\n\n\n\nBayesian\n\n\nPyMC\n\n\nPlotnine\n\n\n\n\nGrammar of Graphics with PyMC\n\n\n\n\n\n\nMay 14, 2025\n\n\nRyan Plain\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "TidyPyMC",
    "section": "",
    "text": "A few weeks ago, Benjamin Vincent posted this blog post on using Bayesian models in Python and leveraging the grammar of graphics for plotting. Please take the time to read that post as this is derived and inteded to augment the ideas shared there.\nAt the end of the post, Vincent asked “It would be interesting to see if this approach is appealing to people.” My answer to that is… YES‼️\nI like PyMC and ArviZ a lot, but it was a huge blow coming from R and libraries like tidybayes, bayesplots, and others that helped wrangle and visualize the posterior.\nI fully agree with the approach of ArviZ to work with high-dimensional data, but comming from a stats background it is more intuitive to work with things as dataframes rather than objects whenever it makes sense. This is especially true with visualizations and the grammar of graphics.\nI’m not sure how or what the best way to contribute to this, and it was mentioned on bluesky that GoG-like interface is being developped for ArviZ 1.0. The best thing I can do is create a post for me."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m an all things data professional living in Colorado Springs, currently working in healthcare supporting their provider network infrastructure. Previously I worked in tech doing product analytics and data science. Prior to that I worked in aviation simulating passenger traffic through airport facilties.\nI’m passionate about making things more efficient, and a proponent of working with what tool and environment works best for you. Designing designing solutions that are inclusive for everyone is something I enjoy.\nIn my free time I love exploring the outdoors, climbing, yoga, snowboarding, and spending time with my wife, dogs, and cat (if she lets me)."
  },
  {
    "objectID": "posts/post-with-code/index.html#tidybayes-in-python-would-be-cool",
    "href": "posts/post-with-code/index.html#tidybayes-in-python-would-be-cool",
    "title": "TidyPyMC",
    "section": "",
    "text": "A few weeks ago, Benjamin Vincent posted this blog post on using Bayesian models in Python and leveraging the grammar of graphics for plotting. Please take the time to read that post as this is derived and inteded to augment the ideas shared there.\nAt the end of the post, Vincent asked “It would be interesting to see if this approach is appealing to people.” My answer to that is… YES‼️\nI like PyMC and ArviZ a lot, but it was a huge blow coming from R and libraries like tidybayes, bayesplots, and others that helped wrangle and visualize the posterior.\nI fully agree with the approach of ArviZ to work with high-dimensional data, but comming from a stats background it is more intuitive to work with things as dataframes rather than objects whenever it makes sense. This is especially true with visualizations and the grammar of graphics.\nI’m not sure how or what the best way to contribute to this, and it was mentioned on bluesky that GoG-like interface is being developped for ArviZ 1.0. The best thing I can do is create a post for me."
  },
  {
    "objectID": "posts/post-with-code/index.html#how-it-started",
    "href": "posts/post-with-code/index.html#how-it-started",
    "title": "TidyPyMC",
    "section": "How it started",
    "text": "How it started\nTL;DR here is the code\n\n\nCode\nimport polars as pl\nimport pymc as pm\nimport arviz as az\nimport numpy as np\n\nfrom plotnine import ggplot, aes, geom_point, geom_ribbon, theme_minimal, labs\n\nd = pl.read_csv(\n    \"https://raw.githubusercontent.com/dustinstansbury/statistical-rethinking-2023/refs/heads/main/data/Howell1.csv\",\n    separator=\";\",\n).filter(pl.col(\"age\") &gt;= 18)\n\nwith pm.Model() as model:\n    weight = pm.Data(\"weight\", d[\"weight\"], dims=[\"obs_id\"])\n\n    sigma = pm.Uniform(\"sigma\", lower=0, upper=50)\n    alpha = pm.Normal(\"alpha\", mu=178, sigma=20)\n\n    beta = pm.LogNormal(\"beta\", mu=0, sigma=1)\n\n    mu = alpha + beta * (weight - weight.mean())\n    y = pm.Normal(\n        \"y\",\n        mu=mu,\n        sigma=sigma,\n        observed=d[\"height\"],\n        shape=weight.shape,\n        dims=[\"obs_id\"],\n    )\n\n    idata = pm.sample()\n\n\nnew_weight = [46.95, 43.72, 64.78, 32.59, 54.63]\nwith model:\n    pm.set_data({\"weight\": new_weight})\n    pp = pm.sample_posterior_predictive(idata, predictions=True, var_names=[\"y\"])\n\nposterior = idata.posterior.to_dataframe().reset_index()\n\nsamples_beta = [\n    np.random.choice(posterior[\"beta\"], size=1000, replace=True).tolist()\n    for _ in new_weight\n]\nsamples_alpha = [\n    np.random.choice(posterior[\"alpha\"], size=1000, replace=True).tolist()\n    for _ in new_weight\n]\n\ndf = pl.DataFrame({\"weight\": new_weight, \"beta\": samples_beta, \"alpha\": samples_alpha})\n\n\ndf_post_epred = df.explode([\"beta\", \"alpha\"]).with_columns(\n    epred=pl.col(\"alpha\") + (pl.col(\"beta\") * (pl.col(\"weight\") - d[\"weight\"].mean()))\n)\ndf_base = df_post_epred.group_by(\"weight\").mean()\n\ndf_post_pred = pl.from_pandas(\n    pp.predictions.to_dataframe().reset_index()\n    # .assign(weight=lambda x: x.obs_id.map(dict(enumerate(new_weight))))\n).with_columns(\n    pl.col(\"obs_id\")\n    .cast(pl.Float64)\n    .replace(dict(enumerate(new_weight)))\n    .alias(\"weight\")\n)\n\n(\n    ggplot()\n    + geom_ribbon(\n        aes(x=\"weight\", ymin=\"y_min\", ymax=\"y_max\", fill=\"type\"),\n        data=df_post_pred.group_by(\"weight\")\n        .agg(y_min=pl.col(\"y\").min(), y_max=pl.col(\"y\").max())\n        .with_columns(type=pl.lit(\"prediction\")),\n        alpha=0.3,\n    )\n    + geom_ribbon(\n        aes(x=\"weight\", ymin=\"epred_min\", ymax=\"epred_max\", fill=\"type\"),\n        data=df_post_epred.group_by(\"weight\")\n        .agg(epred_min=pl.col(\"epred\").min(), epred_max=pl.col(\"epred\").max())\n        .with_columns(type=pl.lit(\"epred\")),\n        alpha=0.6,\n    )\n    + geom_point(aes(\"weight\", \"epred\"), data=df_base)\n    + theme_minimal()\n    + labs(title=\"expected predictions and posterior predictive distributions\")\n)"
  },
  {
    "objectID": "posts/post-with-code/index.html#why",
    "href": "posts/post-with-code/index.html#why",
    "title": "TidyPyMC",
    "section": "Why?",
    "text": "Why?\nI’m in the middle of a journey to learn Bayesian stats. Looking forward to getting to the top of the hill of overconfidence in my knowledge, but this has been hard and I am still at low competence low confidence.\nLearning Bayesian statistics has a steep learning curve on the theory and the tooling. Not to mention the tooling is dramatically different depending on the language or environment being used. When working with data, if are familiar with one language you can search for the most part how to use the API in another language. You learn the fundamentals of it, how data is structured, what works effectively, and it is largely transferrable. After that it is syntax and environment preference.\nNow the tools in probablelistic programmming languages (PPL)… it’s a lot. And it’s daunting!\nI can’t state enough what great work the community has done to make it open and accessible. I started out, like many others, with Richard McElreath’s Statistical Rethinking, and then following along with Solomon Kurz’s Statistical rethinking with brms, ggplot, and the tidyverse. Those are R environments but Allen Downey has great materieal and the work from PyMC community has a ton of efforts to lower the barrier to entry for new users.\nWorking with data in R allows access to some of the best tools to interactively work with data for wrangling and plotting.\nFor me, I love R - but wanted something different to learn in. brms abstracts so much of the model building away, it is fantastic to use but I found myself having trouble learning the\nTBC…"
  },
  {
    "objectID": "posts/post-with-code/index.html#tidypymc",
    "href": "posts/post-with-code/index.html#tidypymc",
    "title": "TidyPyMC",
    "section": "TidyPyMC",
    "text": "TidyPyMC\nThis is definitely subjective, but I think the missing commponent right now is a consistent way to turn the arviz.InferenceData object into a dataframe. Both this and the code Vincent shared is highly custom to the model, and from my experience that is typically a common design choice between R and Python libraries. There is tradeoffs to both paradigms.\nThere are a couple of plots in tidybayes add_epred_draws() and add_predicted_draws() that show some of its capabilities. The goal of this is to replicate them.\nTo accomplish this, we will bring the observed data, linear predictions, and posterior predictions in the same dataframe.\n\nLibraries and data\nWe’ll use the mtcars dataset to replicate some of the tidybayes examples.\n\nimport pandas as pd\nimport pymc as pm\nimport arviz as az\nimport numpy as np\n\nfrom plotnine.data import mtcars\nfrom plotnine import * # elmo_fire.gif namespace, but they mostly start with geom_*\nmtcars.head()\n\n\n\n\n\n\n\n\nname\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\nMazda RX4\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\nDatsun 710\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\nHornet 4 Drive\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n4\nHornet Sportabout\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2"
  },
  {
    "objectID": "posts/post-with-code/index.html#model",
    "href": "posts/post-with-code/index.html#model",
    "title": "TidyPyMC",
    "section": "Model",
    "text": "Model\nThis is an attempt at replicating the model here.\nThe formula is following mpg ~ hp * cyl fit with brms.\n\n\n\n\n\n\nNote\n\n\n\nThe scope of this wasn’t necessarily to walk through creating a pymc model or walk through the workflow of prior predictive checks, diagnostics, etc. I took the priors from brms::stancode(brms::brm(mpg ~ hp * cyl, data = mtcars)) in R. I also used PyMC over Bambi, but both libraries work off the arviz.InferenceData object.\n\n\n\n# build model and sample posterior\nwith pm.Model(\n    coords={\"obs\": mtcars.index, \"predictors\": ['hp', 'cyl']}\n) as mod:\n\n    X = pm.Data(\"X\", mtcars[[\"hp\", \"cyl\"]], dims=(\"obs\", \"predictors\"))\n\n    alpha = pm.StudentT(\"alpha\", nu=3, mu=19.2, sigma=5.4)\n    sigma = pm.HalfStudentT(\"sigma\", nu=3, sigma=5.54)\n    beta = pm.Normal(\"b\", mu=0, sigma=1, dims='predictors')\n\n    mu = pm.Deterministic(\"mu\", alpha + pm.math.dot(X, beta), dims='obs')\n\n    y = pm.Normal(\n        \"y\",\n        mu=mu,\n        sigma=sigma,\n        shape=X.shape[0],\n        observed=mtcars[\"mpg\"],\n        dims=\"obs\",\n    )\n\n    idata = pm.sample(random_seed=527)\n\n# sample posterior predictive\nwith mod as model:\n    pp = pm.sample_posterior_predictive(idata, predictions=True)\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [alpha, sigma, b]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 2 seconds.\nSampling: [y]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy up\nThe biggest takeaway I had from Vincent’s post was it wsa possible to get tidy data out of the arviz.InferenceData object, and this was by far the most difficult part to get my head around.\nFrom idata.posterior, we’ll take three things:\n\nglobal parameters: sigma, alpha\nparameters beta (2)\nlinear predictions mu\n\nThe key is to understand the dimensions of which attribute you want ot get and which ones are the same.\n\nprint(f'alpha: {idata.posterior[\"alpha\"].shape}')\nprint(f'sigma: {idata.posterior[\"sigma\"].shape}')\nprint(f'beta: {idata.posterior[\"b\"].shape}')\nprint(f'mu: {idata.posterior[\"mu\"].shape}')\n\nalpha: (4, 1000)\nsigma: (4, 1000)\nbeta: (4, 1000, 2)\nmu: (4, 1000, 32)\n\n\n\nBoth alpha and sigma are the same shape becuause they are global parameters.\nbeta has the same number of draws, each is represented as a row that will pivot\nmu has the same number of draws but for each observation\n\n\nparams = idata.posterior[[\"sigma\", \"alpha\"]].to_dataframe().reset_index()\nbetas = (\n    idata.posterior[\"b\"]\n    .to_dataframe()\n    .reset_index()\n    .pivot(index=[\"chain\", \"draw\"], columns=\"predictors\", values=\"b\")\n    .reset_index()\n)\n\ndf_posterior = params.merge(betas, on=[\"chain\", \"draw\"])\n\ndf_posterior = (\n    idata.posterior[\"mu\"]\n    .to_dataframe()\n    .reset_index()\n    .merge(mtcars[[\"cyl\", \"mpg\", \"hp\"]], left_on=\"obs\", right_on=mtcars.index)\n    .merge(params, on=[\"chain\", \"draw\"])\n    .merge(betas, on=[\"chain\", \"draw\"], suffixes=[\"\", \"_b\"])\n    .assign( # for plotting later\n        group=lambda x: x.cyl.astype(str)\n        + \"_\"\n        + x.draw.astype(str)\n        + \"_\"\n        + x.chain.astype(str)\n    )\n)\n\ndf_posterior\n\n\n\n\n\n\n\n\nchain\ndraw\nobs\nmu\ncyl\nmpg\nhp\nsigma\nalpha\ncyl_b\nhp_b\ngroup\n\n\n\n\n0\n0\n0\n0\n22.463651\n6\n21.0\n110\n4.074876\n33.091908\n-0.731355\n-0.056728\n6_0_0\n\n\n1\n0\n0\n1\n22.463651\n6\n21.0\n110\n4.074876\n33.091908\n-0.731355\n-0.056728\n6_0_0\n\n\n2\n0\n0\n2\n24.890745\n4\n22.8\n93\n4.074876\n33.091908\n-0.731355\n-0.056728\n4_0_0\n\n\n3\n0\n0\n3\n22.463651\n6\n21.4\n110\n4.074876\n33.091908\n-0.731355\n-0.056728\n6_0_0\n\n\n4\n0\n0\n4\n17.313594\n8\n18.7\n175\n4.074876\n33.091908\n-0.731355\n-0.056728\n8_0_0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n127995\n3\n999\n27\n24.983972\n4\n30.4\n113\n2.481254\n35.978550\n-2.033584\n-0.025312\n4_999_3\n\n\n127996\n3\n999\n28\n13.027543\n8\n15.8\n264\n2.481254\n35.978550\n-2.033584\n-0.025312\n8_999_3\n\n\n127997\n3\n999\n29\n19.347468\n6\n19.7\n175\n2.481254\n35.978550\n-2.033584\n-0.025312\n6_999_3\n\n\n127998\n3\n999\n30\n11.230399\n8\n15.0\n335\n2.481254\n35.978550\n-2.033584\n-0.025312\n8_999_3\n\n\n127999\n3\n999\n31\n25.085220\n4\n21.4\n109\n2.481254\n35.978550\n-2.033584\n-0.025312\n4_999_3\n\n\n\n\n128000 rows × 12 columns\n\n\n\nThe critical takeaway I had from implementing this was learn to leverage coords and dims in the model container. This makes it easier to work with the data later, especially as dimensions increase (i.e. groups in a multilevel model).\nThis sets the posterior to be represented in a tidy dataframe, exactly how Vincent did it. I’m really interested what ways we can cofigure the atributes with PyMC to make this generalize across different models and data.\n\n\n\n\n\n\nNote\n\n\n\nI’ve come back to this and realized adding in the parameters doesn’t make it “tidy”, and also doesn’t get used in the plots. It is still beneficial to include how to go about joining the parameters posterior draws to the data, and anyway… this post is for future me.\n\n\n\n\nPosterior predictive\nOne way would be to do this would be to use arviz.summary() on the sampled posterior predictions. This is a common workflow I would do with brms and tidybayes of parsing parameter outputs name to match the group, or join an id with the original dataset.\n\ndf_predictions = az.summary(pp)\n\n/Users/ryan/git-repos/plain-data/.venv/lib/python3.12/site-packages/arviz/stats/stats.py:1359: UserWarning: Selecting first found group: predictions\n\n\nNext steps to join it with the observed data.\n\ndf_predictions.index = df_predictions.index.str.extract(r\"y\\[(.*?)\\]\")[0]\n\ndf_predictions = df_predictions.merge(mtcars[[\"hp\", \"cyl\", \"mpg\"]], on=df_predictions.index)\ndf_predictions.head()\n\n\n\n\n\n\n\n\nkey_0\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\nhp\ncyl\nmpg\n\n\n\n\n0\n0\n21.561\n3.593\n14.962\n28.471\n0.059\n0.047\n3667.0\n3588.0\n1.0\n110\n6\n21.0\n\n\n1\n1\n21.557\n3.564\n15.082\n28.526\n0.055\n0.042\n4267.0\n3274.0\n1.0\n110\n6\n21.0\n\n\n2\n2\n25.175\n3.594\n18.473\n32.054\n0.062\n0.045\n3391.0\n3769.0\n1.0\n93\n4\n22.8\n\n\n3\n3\n21.533\n3.502\n15.182\n28.314\n0.057\n0.045\n3712.0\n3723.0\n1.0\n110\n6\n21.4\n\n\n4\n4\n16.380\n3.565\n9.941\n23.302\n0.059\n0.043\n3708.0\n4043.0\n1.0\n175\n8\n18.7\n\n\n\n\n\n\n\nThis worked well with the named index on mtcars. I’m not a fan of pandas, and I’ve long forgotten a lot of tips and tricks to work with the nuances of pandas after a couple of years of using polars. For future me, I’m going to include a standarad approach of working with the posterior.\n\ndf_posterior_predictive = (\n    pp.predictions[\"y\"]\n    .to_dataframe()\n    .reset_index()\n    .merge(mtcars[[\"cyl\", \"hp\"]], left_on=\"obs\", right_on=mtcars.index)\n)\n\ndf_posterior_predictive\n\n\n\n\n\n\n\n\nchain\ndraw\nobs\ny\ncyl\nhp\n\n\n\n\n0\n0\n0\n0\n33.841759\n6\n110\n\n\n1\n0\n0\n1\n15.208467\n6\n110\n\n\n2\n0\n0\n2\n26.193852\n4\n93\n\n\n3\n0\n0\n3\n20.961931\n6\n110\n\n\n4\n0\n0\n4\n19.215352\n8\n175\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n127995\n3\n999\n27\n23.539676\n4\n113\n\n\n127996\n3\n999\n28\n8.166691\n8\n264\n\n\n127997\n3\n999\n29\n17.398505\n6\n175\n\n\n127998\n3\n999\n30\n13.519691\n8\n335\n\n\n127999\n3\n999\n31\n28.362757\n4\n109\n\n\n\n\n128000 rows × 6 columns\n\n\n\nThe data is aggregated to match the az.summary() output since this particular geom_ribbon() visualization will only need the HDI values of the posterior predictive distribution.\n\ndf_predictions = (\n    df_posterior_predictive.groupby([\"obs\", \"cyl\", \"hp\"])\n    .agg(\n        pp_mean=(\"y\", \"mean\"),\n        pp_min=(\"y\", lambda x: x.quantile(0.03)),\n        pp_max=(\"y\", lambda x: x.quantile(0.97)),\n    )\n    .reset_index()\n)\ndf_predictions.head()\n\n\n\n\n\n\n\n\nobs\ncyl\nhp\npp_mean\npp_min\npp_max\n\n\n\n\n0\n0\n6\n110\n21.561444\n14.731715\n28.294984\n\n\n1\n1\n6\n110\n21.556898\n14.736975\n28.295248\n\n\n2\n2\n4\n93\n25.174758\n18.227956\n31.853902\n\n\n3\n3\n6\n110\n21.533499\n14.852328\n28.133632\n\n\n4\n4\n8\n175\n16.379716\n9.577502\n23.042973\n\n\n\n\n\n\n\n\n\nThe plot is coming together\nPlotnine! With the grammar of graphics, we’re able to:\n\nuse different datasets\nlayer aesthetics together\nthink about plots with data\n\n\n# sample draws for plotting purposes\nsamples = np.random.choice(\n    [x for x in range(999)], size=int(5), replace=False\n)\n\n(\n    ggplot(mtcars, aes(\"hp\", \"mpg\", color=\"factor(cyl)\", fill=\"factor(cyl)\"))\n    + geom_ribbon(\n        aes(y=\"pp_mean\", ymin=\"pp_min\", ymax=\"pp_max\"), data=df_predictions, alpha=0.2\n    )\n    + geom_line(\n        aes(y=\"mu\", group=\"group\"),\n        data=df_posterior[df_posterior.draw.isin(samples)],\n        alpha=0.6,\n    )\n    + geom_point()\n    + theme_minimal()\n    + labs(color='cyl', fill='cyl')\n)"
  },
  {
    "objectID": "posts/post-with-code/index.html#but-why",
    "href": "posts/post-with-code/index.html#but-why",
    "title": "TidyPyMC",
    "section": "But Why?",
    "text": "But Why?\nI’m in the middle of a journey to learn Bayesian statsistics.\nI can’t state enough what great work the community has done to make it open and accessible. I started out, like many others, with Richard McElreath’s Statistical Rethinking, and then following along with Solomon Kurz’s Statistical rethinking with brms, ggplot, and the tidyverse. Those are R environments but the PyMC community has gret material on getting started and using it for Bayesian Stats.\nThere is a steep learning curve on the theory, but also with the tooling. To do modern Bayesian modeling, you need to interact with a probablelistic programmming languages (PPL). The frameworks and libraries are dramatically different depending on the language or environment being used.\nIt’s daunting! Knowing the theory doesn’t necessarily make it easier to use the tool. Learning one PPL framework doesn’t mean it will be trivial to move to another. As a beginner, brms and Bambi are excellent libraries that lower the barrier of entry and make the model building process easier. The downside is with so much abstracted away, it can make it difficult to really learn what is going on.\nStepping out from brms to Stan was like falling off a cliff. I wanted something that worked better for me, and PyMC was a good blend of what I needed in my journey, and to my surprise have liked it a lot. I felt like I made a lot of progress solidifying concepts building models I had made in brms to PyMC, however I spent most of the time trying to learn how to use the az.InferenceData object.\nI see a lot of value getting a way to work with tidy datasets, both in learning and in application."
  },
  {
    "objectID": "posts/20250531-reticulate-pymc/index.html",
    "href": "posts/20250531-reticulate-pymc/index.html",
    "title": "Reticulate and PyMC",
    "section": "",
    "text": "In my previous post, I used the grammar of graphics to work with the posterior distributions on a PyMC model. Recently, I used the new version of reticulate that uses uv to manage the Python environment. From the post:\n|&gt; with py_require(), Reticulate will automatically create and manage Python environments behind the scenes so you don’t have to.\nI’m a huge fan of uv, and how the developers of Reticulate integrated this really enhances what you can do bringing Python and R together."
  },
  {
    "objectID": "posts/20250531-reticulate-pymc/index.html#bayesian-models-python-grammar-of-graphics-r",
    "href": "posts/20250531-reticulate-pymc/index.html#bayesian-models-python-grammar-of-graphics-r",
    "title": "Reticulate and PyMC",
    "section": "Bayesian models (Python) + grammar of graphics (R) = ❤️",
    "text": "Bayesian models (Python) + grammar of graphics (R) = ❤️\nThis is a nod to the title of Benjamin T. Vincent’s blog post, who inspired me to dive further into PyMC.\nThe reason I started looking into using the grammar of graphics with PyMC was to emulate an R enviornment and tidyverse as much as possible. What if instead we just use the R environement."
  },
  {
    "objectID": "posts/20250531-reticulate-pymc/index.html#again-but-why",
    "href": "posts/20250531-reticulate-pymc/index.html#again-but-why",
    "title": "Reticulate and PyMC",
    "section": "Again… but why?",
    "text": "Again… but why?"
  },
  {
    "objectID": "posts/20250531-reticulate-pymc/index.html#workflow",
    "href": "posts/20250531-reticulate-pymc/index.html#workflow",
    "title": "Reticulate and PyMC",
    "section": "Workflow",
    "text": "Workflow\n\nSet up the environment\nLoad up reticulate and the tidyverse. The first difference in this workflow is that mtcars is a dataset loaded into R by default.\n\nSys.setenv(RETICULATE_PYTHON = \"managed\")\n\n\nlibrary(reticulate)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nmtcars |&gt;\n    head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\nValiant\n18.1\n6\n225\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\n\n\n\n\nI’ve listend to enough presentations and interviews from Charlie Marsh to know I can’t do it justice to explain how uv works. An oversimplification of uv is that it centralizes package downloads, and then distributes them when needed for new isolated enviornments after resolving the dependencies. The main difference is that there isn’t reinstallation needed across environements.\nI’ve used PyMC with other uv environements, so running this was pretty straightforward.\n\npy_require('pymc', python_version=\"3.11\")\npy_config()\n\npython:         /Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/gKtLQ4Ys-srj2T0OKKL-t/bin/python3\nlibpython:      /Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/python/cpython-3.11.12-macos-aarch64-none/lib/libpython3.11.dylib\npythonhome:     /Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/gKtLQ4Ys-srj2T0OKKL-t:/Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/gKtLQ4Ys-srj2T0OKKL-t\nvirtualenv:     /Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/gKtLQ4Ys-srj2T0OKKL-t/bin/activate_this.py\nversion:        3.11.12 (main, May 30 2025, 05:53:55) [Clang 20.1.4 ]\nnumpy:          /Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/gKtLQ4Ys-srj2T0OKKL-t/lib/python3.11/site-packages/numpy\nnumpy_version:  2.2.6\n\nNOTE: Python version was forced by py_require()\n\n\nYou can see that we have an ephemeral cached version of a python environment created with uv and reticulate.\n\n\nSet up the data\nA significant use case for me to use R in the workflow, especially for early stages of iterating on the model building process, would be to use the tidyverse for data wrangling. Right off the bat we can use dplyr::mutate() to scale the variables. You can imagine other types of transformations that are more complex that this would be beneficial to do in an interective setting.\nBecause we will model an interaction similar to mpg ~ cyl * hp, I added a variable that lm() or brms would typically do under the hood. (Car analogy only because we are using mtcars… otherwise I don’t know anything about cars)\n\nmtcars_scaled &lt;- mtcars %&gt;%\n  mutate(\n    hp_c = scale(hp)[, 1],\n    cyl_c = scale(cyl)[, 1],\n    hp_cyl = hp_c * cyl_c\n  )\npandas_mtcars &lt;- r_to_py(mtcars_scaled)\n\nprint(class(mtcars_scaled))\n\n[1] \"data.frame\"\n\nprint(class(pandas_mtcars))\n\n[1] \"pandas.core.frame.DataFrame\"        \"pandas.core.generic.NDFrame\"       \n[3] \"pandas.core.base.PandasObject\"      \"pandas.core.accessor.DirNamesMixin\"\n[5] \"pandas.core.indexing.IndexingMixin\" \"pandas.core.arraylike.OpsMixin\"    \n[7] \"python.builtin.object\"             \n\n\n\n\nPyMC Model\nThe python sytax would look very similar to this, only now instead of using objects with dot notation, we access methods and attributes with the $ character. If you are purely a python developer, this might look obscene. I choose to put up with this quirkyness because I find working with dataframes in R and plotting more than worth it.\n\npm &lt;- import('pymc', convert = FALSE)\n\nmod &lt;- pm$Model(\n  coords = list(\n    car = pandas_mtcars$index,\n    predictors = c('hp', 'cyl', 'hp_cyl')\n  )\n)\n\n\nwith(mod, {\n  X &lt;- pm$Data('X', pandas_mtcars$loc[, c('hp', 'cyl', 'hp_cyl')])\n\n  alpha = pm$StudentT(\"alpha\", nu = 3, mu = 19.2, sigma = 5.4)\n  sigma = pm$HalfStudentT(\"sigma\", nu = 3, sigma = 5.54)\n  beta = pm$Normal(\"b\", mu = 0, sigma = 1, dims = 'predictors')\n  mu = pm$Deterministic(\"mu\", alpha + pm$math$dot(X, beta), dims = 'car')\n\n  y = pm$Normal(\n    \"y\",\n    mu = mu,\n    sigma = sigma,\n    shape = X$shape[0], # python index\n    observed = pandas_mtcars$mpg,\n    dims = \"car\",\n  )\n  # using a single core and chain because of Quarto, normally this would be 4 chains\n  idata = pm$sample(random_seed = 527L, cores = 1L, chains=1L)\n})\n\n                                                                                \n                              Step      Grad      Sampli…                       \n  Progre…   Draws   Diverg…   size      evals     Speed     Elapsed   Remaini…  \n ────────────────────────────────────────────────────────────────────────────── \n  ━━━━━━━   2000    0         0.19      15        1392.39   0:00:01   0:00:00   \n                                                  draws/s                       \n                                                                                \n\nwith(mod, {\n  pp = pm$sample_posterior_predictive(idata, predictions = TRUE)\n})\n\nSampling ... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00 / 0:00:00\n\n\n\n\nPosterior\nNow that we have the model’s az.InferenceData object and posterior predictions, we can work on replicating some of the plots. Only this time we can use tidybayes directly!\n\ndf_posterior &lt;- idata$posterior$mu$to_dataframe()$reset_index() |&gt;\n  py_to_r() |&gt;\n  as_tibble() |&gt;\n  left_join(rownames_to_column(mtcars, 'car')) |&gt;\n  mutate(group = paste0(chain, draw, cyl))\n\nJoining with `by = join_by(car)`\n\ndf_predictions &lt;- pp$predictions$to_dataframe()$reset_index() |&gt;\n  py_to_r() |&gt;\n  as_tibble() |&gt;\n  left_join(rownames_to_column(mtcars, \"car\"))\n\nJoining with `by = join_by(car)`\n\n\n\n\nPlot\n\ndf_predictions |&gt;\n  ggplot(aes(hp, y, color = as.factor(cyl))) +\n  tidybayes::stat_lineribbon(.width = c(.99, .95, .8, .5), alpha = 0.25) +\n  geom_line(\n    aes(y = mu, group = group),\n    data = df_posterior |&gt; filter(draw %in% round(seq(5, 900, length.out = 5))),\n    alpha = 0.38\n  ) +\n  geom_point(aes(y = mpg), data = mtcars, shape = 21, size = 2, stroke = 1) +\n  #scale_fill_manual(values=c('#F8766D', '#00BA38', '#619CFF', 'white')) +\n  scale_fill_brewer(palette = \"Greys\") +\n  theme_light(base_size = 12) +\n  guides(fill = 'none') +\n  labs(\n    title = 'MPG predictions and values based on Horsepower and Cylinders',\n    x = 'hp',\n    y = 'mpg',\n    color = 'cyl'\n  )"
  },
  {
    "objectID": "posts/20250514-pymc-gog/index.html",
    "href": "posts/20250514-pymc-gog/index.html",
    "title": "TidyPyMC",
    "section": "",
    "text": "A few weeks ago, Benjamin Vincent posted this blog post on using Bayesian models in Python and leveraging the grammar of graphics for plotting. Please take the time to read that post as this is derived and inteded to augment the ideas shared there.\nAt the end of the post, Vincent asked “It would be interesting to see if this approach is appealing to people.” My answer to that is… YES‼️\nI like PyMC and ArviZ a lot, but it was a huge blow coming from R and libraries like tidybayes, bayesplots, and others that helped wrangle and visualize the posterior.\nI fully agree with the approach of ArviZ to work with high-dimensional data, but comming from a stats background it is more intuitive to work with things as dataframes rather than objects whenever it makes sense. This is especially true with visualizations and the grammar of graphics.\nI’m not sure how or what the best way to contribute to this, and it was mentioned on bluesky that GoG-like interface is being developped for ArviZ 1.0. The best thing I can do is create a post for me."
  },
  {
    "objectID": "posts/20250514-pymc-gog/index.html#tidybayes-in-python-would-be-cool",
    "href": "posts/20250514-pymc-gog/index.html#tidybayes-in-python-would-be-cool",
    "title": "TidyPyMC",
    "section": "",
    "text": "A few weeks ago, Benjamin Vincent posted this blog post on using Bayesian models in Python and leveraging the grammar of graphics for plotting. Please take the time to read that post as this is derived and inteded to augment the ideas shared there.\nAt the end of the post, Vincent asked “It would be interesting to see if this approach is appealing to people.” My answer to that is… YES‼️\nI like PyMC and ArviZ a lot, but it was a huge blow coming from R and libraries like tidybayes, bayesplots, and others that helped wrangle and visualize the posterior.\nI fully agree with the approach of ArviZ to work with high-dimensional data, but comming from a stats background it is more intuitive to work with things as dataframes rather than objects whenever it makes sense. This is especially true with visualizations and the grammar of graphics.\nI’m not sure how or what the best way to contribute to this, and it was mentioned on bluesky that GoG-like interface is being developped for ArviZ 1.0. The best thing I can do is create a post for me."
  },
  {
    "objectID": "posts/20250514-pymc-gog/index.html#tidypymc",
    "href": "posts/20250514-pymc-gog/index.html#tidypymc",
    "title": "TidyPyMC",
    "section": "TidyPyMC",
    "text": "TidyPyMC\nThis is definitely subjective, but I think the missing commponent right now is a consistent way to turn the arviz.InferenceData object into a dataframe. Both this and the code Vincent shared is highly custom to the model, and from my experience that is typically a common design choice between R and Python libraries. There is tradeoffs to both paradigms.\nThere are a couple of plots in tidybayes add_epred_draws() and add_predicted_draws() that show some of its capabilities. The goal of this is to replicate them.\nTo accomplish this, we will bring the observed data, linear predictions, and posterior predictions in the same dataframe.\n\nLibraries and data\nWe’ll use the mtcars dataset to replicate some of the tidybayes examples.\n\nimport pandas as pd\nimport pymc as pm\nimport arviz as az\nimport numpy as np\n\nfrom plotnine.data import mtcars\nfrom plotnine import * # elmo_fire.gif namespace, but they mostly start with geom_*\nmtcars.head()\n\n\n\n\n\n\n\n\nname\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\nMazda RX4\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\nDatsun 710\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\nHornet 4 Drive\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n4\nHornet Sportabout\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2"
  },
  {
    "objectID": "posts/20250514-pymc-gog/index.html#model",
    "href": "posts/20250514-pymc-gog/index.html#model",
    "title": "TidyPyMC",
    "section": "Model",
    "text": "Model\nThis is an attempt at replicating the model here.\nThe formula is following mpg ~ hp * cyl fit with brms.\n\n\n\n\n\n\nNote\n\n\n\nThe scope of this wasn’t necessarily to walk through creating a pymc model or walk through the workflow of prior predictive checks, diagnostics, etc. I took the priors from brms::stancode(brms::brm(mpg ~ hp * cyl, data = mtcars)) in R. I also used PyMC over Bambi, but both libraries work off the arviz.InferenceData object.\n\n\n\n# build model and sample posterior\nwith pm.Model(\n    coords={\"obs\": mtcars.index, \"predictors\": ['hp', 'cyl']}\n) as mod:\n\n    X = pm.Data(\"X\", mtcars[[\"hp\", \"cyl\"]], dims=(\"obs\", \"predictors\"))\n\n    alpha = pm.StudentT(\"alpha\", nu=3, mu=19.2, sigma=5.4)\n    sigma = pm.HalfStudentT(\"sigma\", nu=3, sigma=5.54)\n    beta = pm.Normal(\"b\", mu=0, sigma=1, dims='predictors')\n\n    mu = pm.Deterministic(\"mu\", alpha + pm.math.dot(X, beta), dims='obs')\n\n    y = pm.Normal(\n        \"y\",\n        mu=mu,\n        sigma=sigma,\n        shape=X.shape[0],\n        observed=mtcars[\"mpg\"],\n        dims=\"obs\",\n    )\n\n    idata = pm.sample(random_seed=527)\n\n# sample posterior predictive\nwith mod as model:\n    pp = pm.sample_posterior_predictive(idata, predictions=True)\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [alpha, sigma, b]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\nSampling: [y]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy up\nThe biggest takeaway I had from Vincent’s post was it wsa possible to get tidy data out of the arviz.InferenceData object, and this was by far the most difficult part to get my head around.\nFrom idata.posterior, we’ll take three things:\n\nglobal parameters: sigma, alpha\nparameters beta (2)\nlinear predictions mu\n\nThe key is to understand the dimensions of which attribute you want ot get and which ones are the same.\n\nprint(f'alpha: {idata.posterior[\"alpha\"].shape}')\nprint(f'sigma: {idata.posterior[\"sigma\"].shape}')\nprint(f'beta: {idata.posterior[\"b\"].shape}')\nprint(f'mu: {idata.posterior[\"mu\"].shape}')\n\nalpha: (4, 1000)\nsigma: (4, 1000)\nbeta: (4, 1000, 2)\nmu: (4, 1000, 32)\n\n\n\nBoth alpha and sigma are the same shape becuause they are global parameters.\nbeta has the same number of draws, each is represented as a row that will pivot\nmu has the same number of draws but for each observation\n\n\nparams = idata.posterior[[\"sigma\", \"alpha\"]].to_dataframe().reset_index()\nbetas = (\n    idata.posterior[\"b\"]\n    .to_dataframe()\n    .reset_index()\n    .pivot(index=[\"chain\", \"draw\"], columns=\"predictors\", values=\"b\")\n    .reset_index()\n)\n\ndf_posterior = params.merge(betas, on=[\"chain\", \"draw\"])\n\ndf_posterior = (\n    idata.posterior[\"mu\"]\n    .to_dataframe()\n    .reset_index()\n    .merge(mtcars[[\"cyl\", \"mpg\", \"hp\"]], left_on=\"obs\", right_on=mtcars.index)\n    .merge(params, on=[\"chain\", \"draw\"])\n    .merge(betas, on=[\"chain\", \"draw\"], suffixes=[\"\", \"_b\"])\n    .assign( # for plotting later\n        group=lambda x: x.cyl.astype(str)\n        + \"_\"\n        + x.draw.astype(str)\n        + \"_\"\n        + x.chain.astype(str)\n    )\n)\n\ndf_posterior\n\n\n\n\n\n\n\n\nchain\ndraw\nobs\nmu\ncyl\nmpg\nhp\nsigma\nalpha\ncyl_b\nhp_b\ngroup\n\n\n\n\n0\n0\n0\n0\n22.463651\n6\n21.0\n110\n4.074876\n33.091908\n-0.731355\n-0.056728\n6_0_0\n\n\n1\n0\n0\n1\n22.463651\n6\n21.0\n110\n4.074876\n33.091908\n-0.731355\n-0.056728\n6_0_0\n\n\n2\n0\n0\n2\n24.890745\n4\n22.8\n93\n4.074876\n33.091908\n-0.731355\n-0.056728\n4_0_0\n\n\n3\n0\n0\n3\n22.463651\n6\n21.4\n110\n4.074876\n33.091908\n-0.731355\n-0.056728\n6_0_0\n\n\n4\n0\n0\n4\n17.313594\n8\n18.7\n175\n4.074876\n33.091908\n-0.731355\n-0.056728\n8_0_0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n127995\n3\n999\n27\n24.983972\n4\n30.4\n113\n2.481254\n35.978550\n-2.033584\n-0.025312\n4_999_3\n\n\n127996\n3\n999\n28\n13.027543\n8\n15.8\n264\n2.481254\n35.978550\n-2.033584\n-0.025312\n8_999_3\n\n\n127997\n3\n999\n29\n19.347468\n6\n19.7\n175\n2.481254\n35.978550\n-2.033584\n-0.025312\n6_999_3\n\n\n127998\n3\n999\n30\n11.230399\n8\n15.0\n335\n2.481254\n35.978550\n-2.033584\n-0.025312\n8_999_3\n\n\n127999\n3\n999\n31\n25.085220\n4\n21.4\n109\n2.481254\n35.978550\n-2.033584\n-0.025312\n4_999_3\n\n\n\n\n128000 rows × 12 columns\n\n\n\nThe critical takeaway I had from implementing this was learn to leverage coords and dims in the model container. This makes it easier to work with the data later, especially as dimensions increase (i.e. groups in a multilevel model).\nThis sets the posterior to be represented in a tidy dataframe, exactly how Vincent did it. I’m really interested what ways we can cofigure the atributes with PyMC to make this generalize across different models and data.\n\n\n\n\n\n\nNote\n\n\n\nI’ve come back to this and realized adding in the parameters doesn’t make it “tidy”, and also doesn’t get used in the plots. It is still beneficial to include how to go about joining the parameters posterior draws to the data, and anyway… this post is for future me.\n\n\n\n\nPosterior predictive\nOne way would be to do this would be to use arviz.summary() on the sampled posterior predictions. This is a common workflow I would do with brms and tidybayes of parsing parameter outputs name to match the group, or join an id with the original dataset.\n\ndf_predictions = az.summary(pp)\n\n/Users/ryan/git-repos/plain-data/.venv/lib/python3.12/site-packages/arviz/stats/stats.py:1359: UserWarning: Selecting first found group: predictions\n\n\nNext steps to join it with the observed data.\n\ndf_predictions.index = df_predictions.index.str.extract(r\"y\\[(.*?)\\]\")[0]\n\ndf_predictions = df_predictions.merge(mtcars[[\"hp\", \"cyl\", \"mpg\"]], on=df_predictions.index)\ndf_predictions.head()\n\n\n\n\n\n\n\n\nkey_0\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\nhp\ncyl\nmpg\n\n\n\n\n0\n0\n21.567\n3.507\n14.961\n28.209\n0.056\n0.040\n3977.0\n4038.0\n1.0\n110\n6\n21.0\n\n\n1\n1\n21.535\n3.521\n15.004\n28.279\n0.057\n0.043\n3863.0\n3513.0\n1.0\n110\n6\n21.0\n\n\n2\n2\n25.074\n3.516\n18.532\n31.826\n0.061\n0.041\n3284.0\n3781.0\n1.0\n93\n4\n22.8\n\n\n3\n3\n21.548\n3.522\n14.778\n27.923\n0.056\n0.042\n4012.0\n3836.0\n1.0\n110\n6\n21.4\n\n\n4\n4\n16.346\n3.476\n9.992\n22.838\n0.058\n0.043\n3595.0\n3588.0\n1.0\n175\n8\n18.7\n\n\n\n\n\n\n\nThis worked well with the named index on mtcars. I’m not a fan of pandas, and I’ve long forgotten a lot of tips and tricks to work with the nuances of pandas after a couple of years of using polars. For future me, I’m going to include a standarad approach of working with the posterior.\n\ndf_posterior_predictive = (\n    pp.predictions[\"y\"]\n    .to_dataframe()\n    .reset_index()\n    .merge(mtcars[[\"cyl\", \"hp\"]], left_on=\"obs\", right_on=mtcars.index)\n)\n\ndf_posterior_predictive\n\n\n\n\n\n\n\n\nchain\ndraw\nobs\ny\ncyl\nhp\n\n\n\n\n0\n0\n0\n0\n19.189177\n6\n110\n\n\n1\n0\n0\n1\n24.255234\n6\n110\n\n\n2\n0\n0\n2\n21.979800\n4\n93\n\n\n3\n0\n0\n3\n24.701108\n6\n110\n\n\n4\n0\n0\n4\n17.615786\n8\n175\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n127995\n3\n999\n27\n20.405657\n4\n113\n\n\n127996\n3\n999\n28\n12.332614\n8\n264\n\n\n127997\n3\n999\n29\n17.688123\n6\n175\n\n\n127998\n3\n999\n30\n9.946518\n8\n335\n\n\n127999\n3\n999\n31\n25.946411\n4\n109\n\n\n\n\n128000 rows × 6 columns\n\n\n\nThe data is aggregated to match the az.summary() output since this particular geom_ribbon() visualization will only need the HDI values of the posterior predictive distribution.\n\ndf_predictions = (\n    df_posterior_predictive.groupby([\"obs\", \"cyl\", \"hp\"])\n    .agg(\n        pp_mean=(\"y\", \"mean\"),\n        pp_min=(\"y\", lambda x: x.quantile(0.03)),\n        pp_max=(\"y\", lambda x: x.quantile(0.97)),\n    )\n    .reset_index()\n)\ndf_predictions.head()\n\n\n\n\n\n\n\n\nobs\ncyl\nhp\npp_mean\npp_min\npp_max\n\n\n\n\n0\n0\n6\n110\n21.567234\n14.846709\n28.158264\n\n\n1\n1\n6\n110\n21.534675\n15.065439\n28.370696\n\n\n2\n2\n4\n93\n25.073569\n18.084567\n31.503317\n\n\n3\n3\n6\n110\n21.548498\n14.891987\n28.068302\n\n\n4\n4\n8\n175\n16.345941\n9.864550\n22.708540\n\n\n\n\n\n\n\n\n\nThe plot is coming together\nPlotnine! With the grammar of graphics, we’re able to:\n\nuse different datasets\nlayer aesthetics together\nthink about plots with data\n\n\n# sample draws for plotting purposes\nsamples = np.random.choice(\n    [x for x in range(999)], size=int(5), replace=False\n)\n\n(\n    ggplot(mtcars, aes(\"hp\", \"mpg\", color=\"factor(cyl)\", fill=\"factor(cyl)\"))\n    + geom_ribbon(\n        aes(y=\"pp_mean\", ymin=\"pp_min\", ymax=\"pp_max\"), data=df_predictions, alpha=0.2\n    )\n    + geom_line(\n        aes(y=\"mu\", group=\"group\"),\n        data=df_posterior[df_posterior.draw.isin(samples)],\n        alpha=0.6,\n    )\n    + geom_point()\n    + theme_minimal()\n    + labs(color='cyl', fill='cyl')\n)"
  },
  {
    "objectID": "posts/2025-05-31-reticulate-pymc/index.html",
    "href": "posts/2025-05-31-reticulate-pymc/index.html",
    "title": "Rython",
    "section": "",
    "text": "The heading is a nod to the title of Benjamin T. Vincent’s blog post about using the grammar-of-graphics with PyMC, and what inspired me to look into different ways of working with PyMC.\nThis is also somewhat of a continuation of my previous post, where I used the grammar-of-graphics to vizualize the posterior distributions from a PyMC model. Accomplishing this built on Vincent’s work by extracting the data from the high dimensional arviz.InferenceData object, and organizing it into a tidy dataframe to work with.\nEverything in this post was all doable before with Reticulate, but I recently had an oppurtunity to use the new version of Reticulate which uses uv to manage the Python environment. I assumed you would need manage a seperate Python environment to some degree. However, from the post:\n\nwith py_require(), Reticulate will automatically create and manage Python environments behind the scenes so you don’t have to.\n\nI used it for a simple package integration, but came away floored with how performant and seemless it was to use – all while not having to manage anything in Python. Reticulate handles the installation of uv if not already available, and will take care of everything else for you."
  },
  {
    "objectID": "posts/2025-05-31-reticulate-pymc/index.html#bayesian-models-python-grammar-of-graphics-r",
    "href": "posts/2025-05-31-reticulate-pymc/index.html#bayesian-models-python-grammar-of-graphics-r",
    "title": "Rython",
    "section": "",
    "text": "The heading is a nod to the title of Benjamin T. Vincent’s blog post about using the grammar-of-graphics with PyMC, and what inspired me to look into different ways of working with PyMC.\nThis is also somewhat of a continuation of my previous post, where I used the grammar-of-graphics to vizualize the posterior distributions from a PyMC model. Accomplishing this built on Vincent’s work by extracting the data from the high dimensional arviz.InferenceData object, and organizing it into a tidy dataframe to work with.\nEverything in this post was all doable before with Reticulate, but I recently had an oppurtunity to use the new version of Reticulate which uses uv to manage the Python environment. I assumed you would need manage a seperate Python environment to some degree. However, from the post:\n\nwith py_require(), Reticulate will automatically create and manage Python environments behind the scenes so you don’t have to.\n\nI used it for a simple package integration, but came away floored with how performant and seemless it was to use – all while not having to manage anything in Python. Reticulate handles the installation of uv if not already available, and will take care of everything else for you."
  },
  {
    "objectID": "posts/2025-05-31-reticulate-pymc/index.html#again-but-why",
    "href": "posts/2025-05-31-reticulate-pymc/index.html#again-but-why",
    "title": "Reticulate and PyMC",
    "section": "Again… but why?",
    "text": "Again… but why?"
  },
  {
    "objectID": "posts/2025-05-31-reticulate-pymc/index.html#workflow",
    "href": "posts/2025-05-31-reticulate-pymc/index.html#workflow",
    "title": "Rython",
    "section": "Workflow",
    "text": "Workflow\n\nSet up with R\nLoad up reticulate and the tidyverse.\n\nSys.setenv(RETICULATE_PYTHON = \"managed\")\nlibrary(reticulate)\nlibrary(tidyverse)\nmtcars |&gt;\n  head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\nValiant\n18.1\n6\n225\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nI set the environment variable for RETICULATE_PYTHON to force Reticulate to use an ephemeral environment. I didn’t have to do this in an interactive session, but this blog already had a uv proejct setup - and I didn’t want it to be used. This could also be configured outside the script or workflow.\n\n\n\n\n\nReticulate and uv environment\nThe function reticulate::py_require() will specify which packages are needed for the project or workflow, and pass them to uv to resolve all the dependencies for the Python virtual environment. This is feasible to do ephemerally due to how performant uv is. You can compose py_require() to build out which packages, if there is a specific python version, and even a date to stop looking for new package updates.\nI’ve listend to enough talks from Charlie Marsh1 to know I can’t do it justice to explain how uv works. An oversimplification of the process is that uv centralizes package downloads, and then resolves dependencies at the environment level so that you do not have to reinstall packages across environments.2\nTo use it with Reticulate, simply run:\n\npy_require('pymc')\npy_config() # only if you want to look at the config\n\npython:         /Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/iNd5Zf03UOAIdigTZPziA/bin/python3\nlibpython:      /Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/python/cpython-3.11.12-macos-aarch64-none/lib/libpython3.11.dylib\npythonhome:     /Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/iNd5Zf03UOAIdigTZPziA:/Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/iNd5Zf03UOAIdigTZPziA\nvirtualenv:     /Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/iNd5Zf03UOAIdigTZPziA/bin/activate_this.py\nversion:        3.11.12 (main, May 30 2025, 05:53:55) [Clang 20.1.4 ]\nnumpy:          /Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/iNd5Zf03UOAIdigTZPziA/lib/python3.11/site-packages/numpy\nnumpy_version:  2.2.6\n\nNOTE: Python version was forced by py_require()\n\n\nWow! You can see that we have an ephemeral Python environment created with uv to be used with reticulate. Everytime this document is rendered, reticulate and uv will cache a new virtual environment to use. PyMC and all of its dependencies only had to be downloaded once. Not shown 456ms on this machine.\nSpecifying pymc with py_require() actually built a Python environment with a list of packages needed, all mapped and configured with uv. I’ve shown some of the most well-known dependencies included.\n\npy_list_packages() |&gt; dim()\n\n[1] 37  3\n\npy_list_packages() |&gt; dplyr::filter(package %in% c('pandas', 'scipy', 'matplotlib', 'arviz', 'pytensor'))\n\n\n\n\n\npackage\nversion\nrequirement\n\n\n\n\narviz\n0.21.0\narviz==0.21.0\n\n\nmatplotlib\n3.10.3\nmatplotlib==3.10.3\n\n\npandas\n2.2.3\npandas==2.2.3\n\n\npytensor\n2.31.3\npytensor==2.31.3\n\n\nscipy\n1.15.3\nscipy==1.15.3\n\n\n\n\n\n\n\n\nSet up the data\nIn the first post, I implemented the formula incorrectly for the interaction model: mpg ~ hp * cyl. The interaction term needed to be explictly added in to look like: mpg ~ hp + cyl + hp:cyl. To rectify that I’ve added the interaction variable to the data passed into the model, which functions like lm() or brms:brm() would implicitly handle. Additionally, I centered the variables to help prevent unnecessary divergece issues.\n\nmtcars_scaled &lt;- mtcars %&gt;%\n  mutate(\n    hp_c = scale(hp)[, 1], # scale() keeps attributes that need to be removed\n    cyl_c = scale(cyl)[, 1],\n    hp_cyl = hp_c * cyl_c\n  )\n\n\n\nR to Python\nPyMC doesn’t work with R, and we will need objects and data types that it knows how to use. reticulate::r_to_py() will handle that.\n\npandas_mtcars &lt;- r_to_py(mtcars_scaled)\nprint(class(mtcars_scaled))\n\n[1] \"data.frame\"\n\nprint(class(pandas_mtcars))\n\n[1] \"pandas.core.frame.DataFrame\"        \"pandas.core.generic.NDFrame\"       \n[3] \"pandas.core.base.PandasObject\"      \"pandas.core.accessor.DirNamesMixin\"\n[5] \"pandas.core.indexing.IndexingMixin\" \"pandas.core.arraylike.OpsMixin\"    \n[7] \"python.builtin.object\"             \n\n\nThere are now two datasets:\n\nmtcars_scaled is an R data.frame() object\npandas_mtcars is a Python pandas.DataFrame() object\n\nWe can now begin the Python portion of the workflow.\n\n\n\n\n\n\nWarning\n\n\n\nIntegrating Python and R has come a long way, and is incredibly accessible. There are some edge cases and things to be aware of when converting data and objects between the two. This post by Karin Hrovatin is one of the best consolidated sources of information to learn from.\n\n\n\n\nPyMC Model\nThe Python sytax would look very similar to this, with one of the main changes being instead of using dot notation, methods and attributes are accessed with the $ character.\nPython code from before.\n\n\nCode\n#|\nwith pm.Model(\n    coords={\"obs\": mtcars.index, \"predictors\": ['hp', 'cyl']}\n) as mod:\n\n    X = pm.Data(\"X\", mtcars[[\"hp\", \"cyl\"]], dims=(\"obs\", \"predictors\"))\n\n    alpha = pm.StudentT(\"alpha\", nu=3, mu=19.2, sigma=5.4)\n    sigma = pm.HalfStudentT(\"sigma\", nu=3, sigma=5.54)\n    beta = pm.Normal(\"b\", mu=0, sigma=1, dims='predictors')\n\n    mu = pm.Deterministic(\"mu\", alpha + pm.math.dot(X, beta), dims='obs')\n\n    y = pm.Normal(\n        \"y\",\n        mu=mu,\n        sigma=sigma,\n        shape=X.shape[0],\n        observed=mtcars[\"mpg\"],\n        dims=\"obs\",\n    )\n\n    idata = pm.sample(random_seed=527)\n\n# sample posterior predictive\nwith mod as model:\n    pp = pm.sample_posterior_predictive(idata, predictions=True)\n\n\nPyMC, but using Reticulate.\n\n# import pymc as pm\npm &lt;- import('pymc', convert = FALSE)\n\nmod &lt;- pm$Model(\n  coords = list(\n    car = pandas_mtcars$index,\n    predictors = c('hp', 'cyl', 'hp_cyl')\n  )\n)\n\n# with pm.Model() as model:\n# ...\nwith(mod, {\n  X &lt;- pm$Data('X', pandas_mtcars$loc[, c('hp', 'cyl', 'hp_cyl')])\n\n  alpha &lt;- pm$StudentT(\"alpha\", nu = 3, mu = 19.2, sigma = 5.4)\n  sigma &lt;- pm$HalfStudentT(\"sigma\", nu = 3, sigma = 5.54)\n  beta &lt;- pm$Normal(\"b\", mu = 0, sigma = 1, dims = 'predictors')\n  mu &lt;- pm$Deterministic(\"mu\", alpha + pm$math$dot(X, beta), dims = 'car')\n\n  y &lt;- pm$Normal(\n    \"y\",\n    mu = mu,\n    sigma = sigma,\n    shape = X$shape[0], # python index\n    observed = pandas_mtcars$mpg,\n    dims = \"car\",\n  )\n  # using a single core and chain because of Quarto page rendering,\n  idata = pm$sample(random_seed = 527L, cores = 1L, chains=4L)\n})\n\n                                                                                \n                              Step      Grad      Sampli…                       \n  Progre…   Draws   Diverg…   size      evals     Speed     Elapsed   Remaini…  \n ────────────────────────────────────────────────────────────────────────────── \n  ━━━━━━━   2000    0         0.19      15        1318.93   0:00:01   0:00:00   \n                                                  draws/s                       \n  ━━━━━━━   2000    0         0.14      15        652.90    0:00:03   0:00:00   \n                                                  draws/s                       \n  ━━━━━━━   2000    4         0.17      15        438.50    0:00:04   0:00:00   \n                                                  draws/s                       \n  ━━━━━━━   2000    1         0.14      31        334.03    0:00:05   0:00:00   \n                                                  draws/s                       \n                                                                                \n\nwith(mod, {\n  pp = pm$sample_posterior_predictive(idata, predictions = TRUE)\n})\n\nSampling ... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00 / 0:00:00\n\n\n\n\nGotchas\nSome quirks to be aware of:\n\npandas_mtcars$loc[, c('hp', 'cyl', 'hp_cyl')] Uses the .loc method from pandas, but with R-style dataframe indexing syntax.\nX$shape[0] Python is 0-based indexed while R is 1-based. Since X is a Python object, we use 0 for indexing.\nrandom_seed = 527L Integer literals in R require L to indicate an integer type, which Python expects here.\n\n\n\nDiagnostics\nAs you can see, any of the ArviZ plots can be used.\nMy heart sank a bit when I finally thought about including this in, as I had already written most of the post and was adding some finishing touches. If diagnostic plots weren’t available, this would all either be useless or require the very problem I sought out to destroy – duplicating efforts across environments. So this is great to see!\n\naz &lt;- import('arviz', convert = FALSE)\nplt &lt;- import(\"matplotlib.pyplot\", convert = FALSE)\naz$plot_trace(idata, var_names = c('alpha', 'b', 'sigma'))\nplt$show()\n\n\n\n\nPosterior\nTranslating the PyMC model’s arviz.InferenceData object and posterior predictions to dataframes is still the same, with the addition of reticulate::py_to_r() to convert a pandas dataframe to R.\n\n# posterior mu\n# reset index to include chain, draw, and index\ndf_posterior &lt;- idata[\"posterior\"]['mu']$to_dataframe()$reset_index() |&gt;\n  py_to_r() |&gt;\n  as_tibble() |&gt;\n  left_join(rownames_to_column(mtcars, 'car')) |&gt; # R mtcars has rownames for the car\n  mutate(group = paste0(chain, draw, cyl)) # for a particular plot later\n\nJoining with `by = join_by(car)`\n\n# posterior predictions of mpg\n# can also access az.InferenceData objects with `$`\ndf_predictions &lt;- pp$predictions$to_dataframe()$reset_index() |&gt;\n  py_to_r() |&gt;\n  as_tibble() |&gt;\n  left_join(rownames_to_column(mtcars, \"car\"))\n\nJoining with `by = join_by(car)`\n\n\nWe’re completely back in R, doing joins with the Tidyverse.\n\n\nPlot\nNow for my favorite part of this, plot with ggplot2 and use tidybayes directly!\n\ndf_predictions |&gt;\n  ggplot(aes(hp, y, color = as.factor(cyl))) +\n  tidybayes::stat_lineribbon(.width = c(.99, .95, .8, .5), alpha = 0.25) +\n  geom_line(\n    aes(y = mu, group = group),\n    data = df_posterior |&gt; filter(draw %in% round(seq(5, 900, length.out = 5))),\n    alpha = 0.38\n  ) +\n  geom_point(aes(y = mpg), data = mtcars, shape = 21, size = 2, stroke = 1) +\n  scale_fill_brewer(palette = \"Greys\") +\n  theme_light(base_size = 12) +\n  guides(fill = 'none') +\n  labs(\n    x = 'hp',\n    y = 'mpg',\n    color = 'cyl'\n  )\n\n\n\n\nIn one workflow we:\n\nused R for data ingestion (default dataset)\nused dplyr to do transformations\nfit a Bayesian model with PyMC in Python\nextracted pandas dataframes of the posterior into a tidy format\nUsed dplyr again to join it with the original dataset\nused ggplot2 and tidybayes to vizualize the samples\n\nAll without ever having to manage a seperate python environment."
  },
  {
    "objectID": "posts/2025-05-14-tidy-pymc/index.html",
    "href": "posts/2025-05-14-tidy-pymc/index.html",
    "title": "TidyPyMC",
    "section": "",
    "text": "A few weeks ago, Benjamin Vincent posted this blog post on using Bayesian models in Python and leveraging the grammar of graphics for plotting. Please take the time to read that post as this is derived and inteded to augment the ideas shared there.\nAt the end of the post, Vincent asked “It would be interesting to see if this approach is appealing to people.” My answer to that is… YES‼️\nI like PyMC and ArviZ a lot, but it was a huge blow coming from R and libraries like tidybayes, bayesplots, and others that helped wrangle and visualize the posterior.\nI fully agree with the approach of ArviZ to work with high-dimensional data, but comming from a stats background it is more intuitive to work with things as dataframes rather than objects whenever it makes sense. This is especially true with visualizations and the grammar of graphics.\nI’m not sure how or what the best way to contribute to this, and it was mentioned on bluesky that GoG-like interface is being developped for ArviZ 1.0. The best thing I can do is create a post for me."
  },
  {
    "objectID": "posts/2025-05-14-tidy-pymc/index.html#tidybayes-in-python-would-be-cool",
    "href": "posts/2025-05-14-tidy-pymc/index.html#tidybayes-in-python-would-be-cool",
    "title": "TidyPyMC",
    "section": "",
    "text": "A few weeks ago, Benjamin Vincent posted this blog post on using Bayesian models in Python and leveraging the grammar of graphics for plotting. Please take the time to read that post as this is derived and inteded to augment the ideas shared there.\nAt the end of the post, Vincent asked “It would be interesting to see if this approach is appealing to people.” My answer to that is… YES‼️\nI like PyMC and ArviZ a lot, but it was a huge blow coming from R and libraries like tidybayes, bayesplots, and others that helped wrangle and visualize the posterior.\nI fully agree with the approach of ArviZ to work with high-dimensional data, but comming from a stats background it is more intuitive to work with things as dataframes rather than objects whenever it makes sense. This is especially true with visualizations and the grammar of graphics.\nI’m not sure how or what the best way to contribute to this, and it was mentioned on bluesky that GoG-like interface is being developped for ArviZ 1.0. The best thing I can do is create a post for me."
  },
  {
    "objectID": "posts/2025-05-14-tidy-pymc/index.html#tidypymc",
    "href": "posts/2025-05-14-tidy-pymc/index.html#tidypymc",
    "title": "TidyPyMC",
    "section": "TidyPyMC",
    "text": "TidyPyMC\nThis is definitely subjective, but I think the missing commponent right now is a consistent way to turn the arviz.InferenceData object into a dataframe. Both this and the code Vincent shared is highly custom to the model, and from my experience that is typically a common design choice between R and Python libraries. There is tradeoffs to both paradigms.\nThere are a couple of plots in tidybayes add_epred_draws() and add_predicted_draws() that show some of its capabilities. The goal of this is to replicate them.\nTo accomplish this, we will bring the observed data, linear predictions, and posterior predictions in the same dataframe.\n\nLibraries and data\nWe’ll use the mtcars dataset to replicate some of the tidybayes examples.\n\nimport pandas as pd\nimport pymc as pm\nimport arviz as az\nimport numpy as np\n\nfrom plotnine.data import mtcars\nfrom plotnine import * # elmo_fire.gif namespace, but they mostly start with geom_*\nmtcars.head()\n\n\n\n\n\n\n\n\nname\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\nMazda RX4\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\nMazda RX4 Wag\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\nDatsun 710\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\nHornet 4 Drive\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n4\nHornet Sportabout\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2"
  },
  {
    "objectID": "posts/2025-05-14-tidy-pymc/index.html#model",
    "href": "posts/2025-05-14-tidy-pymc/index.html#model",
    "title": "TidyPyMC",
    "section": "Model",
    "text": "Model\nThis is an attempt at replicating the model here.\nThe formula is following mpg ~ hp * cyl fit with brms.\n\n\n\n\n\n\nNote\n\n\n\nThe scope of this wasn’t necessarily to walk through creating a pymc model or walk through the workflow of prior predictive checks, diagnostics, etc. I took the priors from brms::stancode(brms::brm(mpg ~ hp * cyl, data = mtcars)) in R. I also used PyMC over Bambi, but both libraries work off the arviz.InferenceData object.\n\n\n\n# build model and sample posterior\nwith pm.Model(\n    coords={\"obs\": mtcars.index, \"predictors\": ['hp', 'cyl']}\n) as mod:\n\n    X = pm.Data(\"X\", mtcars[[\"hp\", \"cyl\"]], dims=(\"obs\", \"predictors\"))\n\n    alpha = pm.StudentT(\"alpha\", nu=3, mu=19.2, sigma=5.4)\n    sigma = pm.HalfStudentT(\"sigma\", nu=3, sigma=5.54)\n    beta = pm.Normal(\"b\", mu=0, sigma=1, dims='predictors')\n\n    mu = pm.Deterministic(\"mu\", alpha + pm.math.dot(X, beta), dims='obs')\n\n    y = pm.Normal(\n        \"y\",\n        mu=mu,\n        sigma=sigma,\n        shape=X.shape[0],\n        observed=mtcars[\"mpg\"],\n        dims=\"obs\",\n    )\n\n    idata = pm.sample(random_seed=527)\n\n# sample posterior predictive\nwith mod as model:\n    pp = pm.sample_posterior_predictive(idata, predictions=True)\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [alpha, sigma, b]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 2 seconds.\nSampling: [y]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidy up\nThe biggest takeaway I had from Vincent’s post was it wsa possible to get tidy data out of the arviz.InferenceData object, and this was by far the most difficult part to get my head around.\nFrom idata.posterior, we’ll take three things:\n\nglobal parameters: sigma, alpha\nparameters beta (2)\nlinear predictions mu\n\nThe key is to understand the dimensions of which attribute you want ot get and which ones are the same.\n\nprint(f'alpha: {idata.posterior[\"alpha\"].shape}')\nprint(f'sigma: {idata.posterior[\"sigma\"].shape}')\nprint(f'beta: {idata.posterior[\"b\"].shape}')\nprint(f'mu: {idata.posterior[\"mu\"].shape}')\n\nalpha: (4, 1000)\nsigma: (4, 1000)\nbeta: (4, 1000, 2)\nmu: (4, 1000, 32)\n\n\n\nBoth alpha and sigma are the same shape becuause they are global parameters.\nbeta has the same number of draws, each is represented as a row that will pivot\nmu has the same number of draws but for each observation\n\n\nparams = idata.posterior[[\"sigma\", \"alpha\"]].to_dataframe().reset_index()\nbetas = (\n    idata.posterior[\"b\"]\n    .to_dataframe()\n    .reset_index()\n    .pivot(index=[\"chain\", \"draw\"], columns=\"predictors\", values=\"b\")\n    .reset_index()\n)\n\ndf_posterior = params.merge(betas, on=[\"chain\", \"draw\"])\n\ndf_posterior = (\n    idata.posterior[\"mu\"]\n    .to_dataframe()\n    .reset_index()\n    .merge(mtcars[[\"cyl\", \"mpg\", \"hp\"]], left_on=\"obs\", right_on=mtcars.index)\n    .merge(params, on=[\"chain\", \"draw\"])\n    .merge(betas, on=[\"chain\", \"draw\"], suffixes=[\"\", \"_b\"])\n    .assign( # for plotting later\n        group=lambda x: x.cyl.astype(str)\n        + \"_\"\n        + x.draw.astype(str)\n        + \"_\"\n        + x.chain.astype(str)\n    )\n)\n\ndf_posterior\n\n\n\n\n\n\n\n\nchain\ndraw\nobs\nmu\ncyl\nmpg\nhp\nsigma\nalpha\ncyl_b\nhp_b\ngroup\n\n\n\n\n0\n0\n0\n0\n22.463651\n6\n21.0\n110\n4.074876\n33.091908\n-0.731355\n-0.056728\n6_0_0\n\n\n1\n0\n0\n1\n22.463651\n6\n21.0\n110\n4.074876\n33.091908\n-0.731355\n-0.056728\n6_0_0\n\n\n2\n0\n0\n2\n24.890745\n4\n22.8\n93\n4.074876\n33.091908\n-0.731355\n-0.056728\n4_0_0\n\n\n3\n0\n0\n3\n22.463651\n6\n21.4\n110\n4.074876\n33.091908\n-0.731355\n-0.056728\n6_0_0\n\n\n4\n0\n0\n4\n17.313594\n8\n18.7\n175\n4.074876\n33.091908\n-0.731355\n-0.056728\n8_0_0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n127995\n3\n999\n27\n24.983972\n4\n30.4\n113\n2.481254\n35.978550\n-2.033584\n-0.025312\n4_999_3\n\n\n127996\n3\n999\n28\n13.027543\n8\n15.8\n264\n2.481254\n35.978550\n-2.033584\n-0.025312\n8_999_3\n\n\n127997\n3\n999\n29\n19.347468\n6\n19.7\n175\n2.481254\n35.978550\n-2.033584\n-0.025312\n6_999_3\n\n\n127998\n3\n999\n30\n11.230399\n8\n15.0\n335\n2.481254\n35.978550\n-2.033584\n-0.025312\n8_999_3\n\n\n127999\n3\n999\n31\n25.085220\n4\n21.4\n109\n2.481254\n35.978550\n-2.033584\n-0.025312\n4_999_3\n\n\n\n\n128000 rows × 12 columns\n\n\n\nThe critical takeaway I had from implementing this was learn to leverage coords and dims in the model container. This makes it easier to work with the data later, especially as dimensions increase (i.e. groups in a multilevel model).\nThis sets the posterior to be represented in a tidy dataframe, exactly how Vincent did it. I’m really interested what ways we can cofigure the atributes with PyMC to make this generalize across different models and data.\n\n\n\n\n\n\nNote\n\n\n\nI’ve come back to this and realized adding in the parameters doesn’t make it “tidy”, and also doesn’t get used in the plots. It is still beneficial to include how to go about joining the parameters posterior draws to the data, and anyway… this post is for future me.\n\n\n\n\nPosterior predictive\nOne way would be to do this would be to use arviz.summary() on the sampled posterior predictions. This is a common workflow I would do with brms and tidybayes of parsing parameter outputs name to match the group, or join an id with the original dataset.\n\ndf_predictions = az.summary(pp)\n\n/Users/ryan/git-repos/plain-data/.venv/lib/python3.12/site-packages/arviz/stats/stats.py:1359: UserWarning: Selecting first found group: predictions\n\n\nNext steps to join it with the observed data.\n\ndf_predictions.index = df_predictions.index.str.extract(r\"y\\[(.*?)\\]\")[0]\n\ndf_predictions = df_predictions.merge(mtcars[[\"hp\", \"cyl\", \"mpg\"]], on=df_predictions.index)\ndf_predictions.head()\n\n\n\n\n\n\n\n\nkey_0\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\nhp\ncyl\nmpg\n\n\n\n\n0\n0\n21.554\n3.518\n14.560\n27.887\n0.061\n0.044\n3292.0\n3666.0\n1.0\n110\n6\n21.0\n\n\n1\n1\n21.470\n3.503\n14.949\n27.871\n0.056\n0.044\n3869.0\n3845.0\n1.0\n110\n6\n21.0\n\n\n2\n2\n25.176\n3.573\n17.968\n31.616\n0.059\n0.044\n3723.0\n3477.0\n1.0\n93\n4\n22.8\n\n\n3\n3\n21.540\n3.545\n14.597\n27.981\n0.056\n0.042\n3940.0\n3775.0\n1.0\n110\n6\n21.4\n\n\n4\n4\n16.393\n3.563\n10.187\n23.717\n0.057\n0.046\n3934.0\n3670.0\n1.0\n175\n8\n18.7\n\n\n\n\n\n\n\nThis worked well with the named index on mtcars. I’m not a fan of pandas, and I’ve long forgotten a lot of tips and tricks to work with the nuances of pandas after a couple of years of using polars. For future me, I’m going to include a standarad approach of working with the posterior.\n\ndf_posterior_predictive = (\n    pp.predictions[\"y\"]\n    .to_dataframe()\n    .reset_index()\n    .merge(mtcars[[\"cyl\", \"hp\"]], left_on=\"obs\", right_on=mtcars.index)\n)\n\ndf_posterior_predictive\n\n\n\n\n\n\n\n\nchain\ndraw\nobs\ny\ncyl\nhp\n\n\n\n\n0\n0\n0\n0\n19.104229\n6\n110\n\n\n1\n0\n0\n1\n25.039315\n6\n110\n\n\n2\n0\n0\n2\n24.256567\n4\n93\n\n\n3\n0\n0\n3\n19.098882\n6\n110\n\n\n4\n0\n0\n4\n15.877768\n8\n175\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n127995\n3\n999\n27\n26.447811\n4\n113\n\n\n127996\n3\n999\n28\n8.939379\n8\n264\n\n\n127997\n3\n999\n29\n19.937614\n6\n175\n\n\n127998\n3\n999\n30\n15.226820\n8\n335\n\n\n127999\n3\n999\n31\n29.357202\n4\n109\n\n\n\n\n128000 rows × 6 columns\n\n\n\nThe data is aggregated to match the az.summary() output since this particular geom_ribbon() visualization will only need the HDI values of the posterior predictive distribution.\n\ndf_predictions = (\n    df_posterior_predictive.groupby([\"obs\", \"cyl\", \"hp\"])\n    .agg(\n        pp_mean=(\"y\", \"mean\"),\n        pp_min=(\"y\", lambda x: x.quantile(0.03)),\n        pp_max=(\"y\", lambda x: x.quantile(0.97)),\n    )\n    .reset_index()\n)\ndf_predictions.head()\n\n\n\n\n\n\n\n\nobs\ncyl\nhp\npp_mean\npp_min\npp_max\n\n\n\n\n0\n0\n6\n110\n21.554171\n14.872339\n28.276621\n\n\n1\n1\n6\n110\n21.469598\n14.971328\n27.914067\n\n\n2\n2\n4\n93\n25.175523\n18.266339\n31.997126\n\n\n3\n3\n6\n110\n21.539521\n14.693196\n28.122148\n\n\n4\n4\n8\n175\n16.392580\n9.748206\n23.448341\n\n\n\n\n\n\n\n\n\nThe plot is coming together\nPlotnine! With the grammar of graphics, we’re able to:\n\nuse different datasets\nlayer aesthetics together\nthink about plots in terms of data\n\n\n# sample draws for plotting purposes\nsamples = np.random.choice(\n    [x for x in range(999)], size=int(5), replace=False\n)\n\n(\n    ggplot(mtcars, aes(\"hp\", \"mpg\", color=\"factor(cyl)\", fill=\"factor(cyl)\"))\n    + geom_ribbon(\n        aes(y=\"pp_mean\", ymin=\"pp_min\", ymax=\"pp_max\"), data=df_predictions, alpha=0.2\n    )\n    + geom_line(\n        aes(y=\"mu\", group=\"group\"),\n        data=df_posterior[df_posterior.draw.isin(samples)],\n        alpha=0.6,\n    )\n    + geom_point()\n    + theme_minimal()\n    + labs(color='cyl', fill='cyl')\n)"
  },
  {
    "objectID": "posts/2025-05-31-reticulate-pymc/index.html#summary",
    "href": "posts/2025-05-31-reticulate-pymc/index.html#summary",
    "title": "Reticulate and PyMC",
    "section": "Summary",
    "text": "Summary\nThis is an opinionated way of using PyMC and the grammar of graphics together to say the least. I really do like pymc, but I prefer to settle on the data I want to use and other parts of the model iteration with R. There is potential for reticulate::py_run_string() as well, if you wanted to be able to drop it directly back into a pure Python environment. Access to an LLM would also be able to easily reformat the R PyMC model to Python or at least get it most of the way there."
  },
  {
    "objectID": "posts/2025-05-31-reticulate-pymc/index.html#rython",
    "href": "posts/2025-05-31-reticulate-pymc/index.html#rython",
    "title": "Rython",
    "section": "Rython",
    "text": "Rython\nThis is an opinionated way of using PyMC and the grammar-of-graphics together to say the least. I really do like PyMC, but I prefer to settle on the data and other parts of the model iteration process with R if possible. There is potential for reticulate::py_run_string() as well, if you wanted to be able to drop it directly back into a pure Python environment. Any AI model would also be able to easily reformat the R-PyMC model to Python, or at least get it most of the way there.\nI’m genuinely impressed by how far integrating R and Python has come. When I started my career, you had to do a bunch of clunky I/O to get features of both languages.\nA typo I had in drafting this at one point was Rython, and given my name… I quite like it."
  },
  {
    "objectID": "about.html#bio",
    "href": "about.html#bio",
    "title": "About",
    "section": "",
    "text": "I’m an all things data professional living in Colorado Springs, currently working in healthcare supporting their provider network infrastructure. Previously I worked in tech doing product analytics and data science. Prior to that I worked in aviation simulating passenger traffic through airport facilties.\nI’m passionate about making things more efficient, and a proponent of working with what tool and environment works best for you. Designing designing solutions that are inclusive for everyone is something I enjoy.\nIn my free time I love exploring the outdoors, climbing, yoga, snowboarding, and spending time with my wife, dogs, and cat (if she lets me)."
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nKaiser Permanente | Data Reporiting and Analytics Consultant\nClover | Sr. Data Analyst\nSouthwest Airlines | Data Analyst"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nGeorgia Institute of Technology | Atlanta, GA\nM.S. in Analytics\nThe University of North Texas | Denton, TX\nB.B.A. in Business Analytics"
  },
  {
    "objectID": "posts/2025-05-31-reticulate-pymc/index.html#footnotes",
    "href": "posts/2025-05-31-reticulate-pymc/index.html#footnotes",
    "title": "Rython",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCharlie Marsh is the lead developer of Ruff and discussed uv in multiple talks.↩︎\nuv is a Rust-based Python package manager that installs dependencies in a global cache and reuses them in isolated environments, improving reproducibility and speed. For more, see the uv docs.↩︎"
  },
  {
    "objectID": "posts/2026-01-06-statistical-rethinking/index.html",
    "href": "posts/2026-01-06-statistical-rethinking/index.html",
    "title": "Statistical Rethinking",
    "section": "",
    "text": "I’m looking forward to this course refresh of Statistical Rethinking. Richard McElreath has been one of the most uplifting and influential educators I’ve interacted with, and this course has had a significant impact on my life. \n\ny = mx + b\nI was someone that didn’t jive with the standard educational system. This is before educational YouTube videos were really a thing, and channel’s like 3Blue1Brown and other visualization first approaches weren’t commonly used.  \nI hit a wall hard with Algebra 2 in high school and got out of AP courses. I failed math for non-science majors, failed algebra twice, and only eventually passed with a “D”. It took me 5 years to complete community college, racking up two academic probations, an academic suspension, across three total community colleges.\nI enrolled in a university I could commute to, and while looking at degrees I needed to find a path forward with the least math possible. I felt broken from math. I was terrified of it, and saw myself with a limited and capped potential.\nFor better or worse, I ended up choosing a Business Economics major after over-indexing on an in-person aptitude test I was took. I read some information about Econ, took an intro course and decided it would be okay. Ultimately, I lucked out being naive how foundational math and statistics was to it.\nI looked over the course requirements and only a single Calculus course stood in my way. It was standing there like a behemoth-sized enemy in Elden Ring, but it was like that for most of, if not all, the majors in the business school.\nI took Calculus (for non-science majors again) at the local community college due to the credit transferring as pass/fail. To top it off, my (younger) sister, who was doing a minor in math, was in town for the summer. She was there to help tutor me and provide a safety net. All of this context is to highlight how much my confidence was shattered.\n\n\ndx/dy &gt; 0\nI passed. It wasn’t as bad as I thought, but we barely touched on the concept of an integral at the end of the semester.\nThen came statistics. Now, this was an introductory to statistics class in a business school that was… lets say not the most reputable for rigor in the university, but it was applied. It changed how I viewed math, or at least some parts of it. I could connect why we were learning this, how it would get used in real life, and the concept of distributions intuitively made sense to me. For once, everything was clicking and I was excelling in school. New territory for me. \nThere was this strange feeling of going down this path without a deeper calculus training and having experience in statistics. It was like driving a car without brakes. Actually, I don’t know a thing about cars - but the analogy would be if I could operate the car but not turn at speeds greater than 20 mph. Intro to Econometrics was by far my most difficult course. I remember looking around the room trying to see if anyone else was as lost as me when the instructor said “with respect to x”.\nThere was at least a reassuring moment for me in that class where a student who enjoyed studying calculus said that he hated statistics. It had me at least aligned with the notion to remove the sweeping generalization of “not being good with numbers”. I also was able to manage a “B”.\nFast forward, I graduated with a degree. Rushed through it and took 48 hours in a calendar year, and in hindsight learned very little. I did come out with a lot of confidence though, and it put a new perspective on life after ditching the fear of math. \nI Applied to Georgia Tech’s Online MS Analytics program (OMSA). It was relatively easy to get into, cost effective, online… and what I did not understand was how sink or swim it would be. There was a requirement for having 3 levels of calculus. My confidence (maybe arrogance) was at an all time high, and so with only a single semester of calculus for non-math majors - I asked for an admissions exception and agreed that I would meet the demands of the coursework. I’m thankful for that confidence because overall grad school was a great learning experience, but it was tough. Every class I signed up for was like walking into a minefield wondering if there were going to be integrals lurking somewhere. \nOne class in particular I was excited for was a course Bayesian statistics. One of the introductory classes in the OMSA curriculum touched a myriad of topics, and had a brief overview of Bayesian statistics and how it could be used to rank teams. The example took point differential of basketball games and reduced the noise to estimate the true point advantage of a team,. I explain it plainly like that because that really was the extend of my understanding at the time, which was hardly anything at all. I had always been interested in sports, found it fascinating and had a desire to learn more.\nI took the class and aced the first homework assignment, but it was hard work. The class was set up so that the first half was theory and the homework was typically analytical closed form solutions. The second half would use BUGS and be more applied. \n\n\ndx/dy &lt; 0\nI think… I didn’t make it past week 3. After tears, frustration, and agony trying to complete the second homework assignment, I dropped the class. I just didn’t have enough time to upskill what I needed for the class and was overwhelmed. \nDefeated. The feeling I thought I conquered was back. To think you have moved on or escaped from something, and then to fall back into it again, was devastating.\nI took different classes and ultimately finished my degree, but the confidence I had exiting undergrad vs grad school was a night and day difference - and this was more than just the Dunning-Kruger effect. I think there is some truth that it partially belonged to that, but there was this looming shadow of not knowing math. An area that I did not feel equipped to handle if I ever came across it again.  ### Statistical Rethinking\nI was aware of Statistical Rethinking from pre-pandemic days on twitter. When it was toxic in the ways most social media is, but also had a strong data science community. The second edition of the book was coming available and I thought it would be a good time to take a stab at it. I didn’t know much about it other than it would be accessible to all different levels of experience and highly recommended. Plus, I could focus solely on it without other classes.\nThe text itself is dense and covers a lot. It’s an actual zero to hero course that teaches not just how to implement the models, but also how to critically think through the problems. An area I did not have enough experience in. Throughout the book he clearly explains how large of an endeavor this is and breaks it up into manageable sections.\nFrom chapter 3:\n\nFirst, many scientists are uncomfortable with integral calculus, even though they have a strong and valid intuitions about how to summarize data.\n\nI remember reading this and feeling incredibly seen and validated. I felt confident in my ability to continue, and helped remove some of the imposter syndrome that I didn’t belong here. Even putting myself in with the term scientist.\nThe course is hard if you are just starting out. One of the most important lessons I learned through this book is that learning is nonlinear. I think I learned how to truly study, or at least be more effective with this. Prior to this, I always felt like I needed to learn everything as it came. I think that was one of my pitfalls with learning math, scribbling everything the instructor was throwing on the whiteboard, not asking questions, and not thinking critically about any of the presented information.\nMcElreath’s lecture videos pause at the midpoint. He reiterates that it is a lot of material, and suggests options based on where you are at with your learning. One of them was to go back to the start, and review the material.\nI’ve done that multiple times now. It took me several attempts to get past building the first linear model in chapter 4. I strongly sympathize with anyone who provided feedback that helped lead to the refreshed course to split into two parts.\nI’m excited to go through the second half of the course again, and if I feel I’m over my skis at any point - I’m confident to go back to earlier sections and refresh on the material.\n\n\nConfidence\nThis is N=1, but I have not had an educator so supportive and compassionate in the learning process. So many times in the text or videos he will say, “if you are confused, that’s okay”.\nI’ll never forget sitting on my patio when it clicked that the models are sampling parameters and not any outcome or target value. I have the satisfaction of being able to push through and learn something I thought was daunting and out of reach for me.\nI’m confident. I’m a scientist, an analytical thinker, experimenter, tinkerer, someone who wants to understand the world. I’m all of those things, and see all those things within myself, in large part thanks Richard McElreath and Statistical Rethinking."
  }
]