{
  "hash": "3d3a7d8921397f58b7f7e6494778138b",
  "result": {
    "markdown": "---\ntitle: \"Reticulate and PyMC\"\nauthor: \"Ryan Plain\"\ndate: \"2025-05-31\"\ncategories: [Bayesian, PyMC, Reticulate]\ndf-print: kable\n---\n\n\n## Bayesian models (Python) + grammar of graphics (R) = ❤️\n\nThis is a nod to the title of [Benjamin T. Vincent's blog post](https://drbenvincent.github.io/posts/mcmc_grammar_of_graphics.html), who inspired me to dive further into PyMC. The reason I started looking into using the grammar of graphics with PyMC, was to reduce a barrier and emulate **tidyverse** as much as possible. What if instead we just... use R and PyMC?\n\nIn my [previous post](../2025-05-14-tidy-pymc/), I used the Grammar of Graphics to vizualize the posterior distributions from a PyMC model. This required extracting the data from the `az.InferenceData` object, and converting to a dataframe to work with.\n\nI recently got a chance to use the [new version of reticulate](https://posit.co/blog/reticulate-1-41/), which uses `uv` to manage the Python environment used in an R session, and fell in love. From the post:\n\n> with `py_require()`, Reticulate will automatically create and manage Python environments behind the scenes so you don’t have to.\n\nI'm a huge fan of [uv](https://docs.astral.sh/uv/), and how the developers of Reticulate integrated it really simplifies the process of bringing Python and R together.\n\n<div class=\"tenor-gif-embed\" data-postid=\"11478682\" data-share-method=\"host\" data-aspect-ratio=\"1.625\" data-width=\"100%\"><a href=\"https://tenor.com/view/why-not-both-why-not-take-both-gif-11478682\">Why Not Both Take Both GIF</a>from <a href=\"https://tenor.com/search/why+not+both-gifs\">Why Not Both GIFs</a></div> <script type=\"text/javascript\" async src=\"https://tenor.com/embed.js\"></script>\n\n## Workflow\n\n### Set up the environment\n\nLoad up `reticulate` and the `tidyverse`.\n\n::: {.callout-note collapse=\"true\"}\nI set the environment variable for `RETICULATE_PYTHON` to force Reticulate to use an ephemeral environment. I didn't have to do this in an interactive session, but this blog already had a uv proejct setup - and I didn't want it to be used. This could also be configured outside the script or workflow.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSys.setenv(RETICULATE_PYTHON = \"managed\")\nlibrary(reticulate)\nlibrary(tidyverse)\nmtcars |>\n  head()\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|                  |  mpg| cyl| disp|  hp| drat|    wt|  qsec| vs| am| gear| carb|\n|:-----------------|----:|---:|----:|---:|----:|-----:|-----:|--:|--:|----:|----:|\n|Mazda RX4         | 21.0|   6|  160| 110| 3.90| 2.620| 16.46|  0|  1|    4|    4|\n|Mazda RX4 Wag     | 21.0|   6|  160| 110| 3.90| 2.875| 17.02|  0|  1|    4|    4|\n|Datsun 710        | 22.8|   4|  108|  93| 3.85| 2.320| 18.61|  1|  1|    4|    1|\n|Hornet 4 Drive    | 21.4|   6|  258| 110| 3.08| 3.215| 19.44|  1|  0|    3|    1|\n|Hornet Sportabout | 18.7|   8|  360| 175| 3.15| 3.440| 17.02|  0|  0|    3|    2|\n|Valiant           | 18.1|   6|  225| 105| 2.76| 3.460| 20.22|  1|  0|    3|    1|\n\n</div>\n:::\n:::\n\n\nThe first difference in this workflow is that `mtcars` is a dataset available by default. It can be accessed directly within an R session.\n\n### Reticulate and uv environment\n\nI've listend to enough presentations and interviews from [Charlie Marsh](https://github.com/charliermarsh) to know I can't do it justice to explain how `uv` works. An oversimplification of `uv` is that it centralizes package downloads, and then distributes them when needed for new isolated enviornments, after resolving the dependencies. The key difference is that there isn't reinstallation of packages needed across environements.\n\nUsing `py_require()` will specify which packages are needed and how to create a virtual environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npy_require('pymc')\npy_config()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\npython:         /Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/gKtLQ4Ys-srj2T0OKKL-t/bin/python3\nlibpython:      /Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/python/cpython-3.11.12-macos-aarch64-none/lib/libpython3.11.dylib\npythonhome:     /Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/gKtLQ4Ys-srj2T0OKKL-t:/Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/gKtLQ4Ys-srj2T0OKKL-t\nvirtualenv:     /Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/gKtLQ4Ys-srj2T0OKKL-t/bin/activate_this.py\nversion:        3.11.12 (main, May 30 2025, 05:53:55) [Clang 20.1.4 ]\nnumpy:          /Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/gKtLQ4Ys-srj2T0OKKL-t/lib/python3.11/site-packages/numpy\nnumpy_version:  2.2.6\n\nNOTE: Python version was forced by py_require()\n```\n:::\n:::\n\n\nYou can see that we have an ephemeral python environment created with `uv` and `reticulate`. This is really neat! Everytime I render this document, it will cache a new virtual environment for `reticulate` to use, but because of `uv` I only had to download `pymc` the first time.\n\nSpecifying the `pymc` dependency actually built a Python environment with 63 packages, all mapped and configured with `uv`. I've shown some of the most well-known dependencies of `pymc`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# > py_list_packages() |> dim()\n# [1] 63  3\n\npy_list_packages() |> dplyr::filter(package %in% c('pandas', 'scipy', 'matplotlib', 'arviz'))\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|package    |version |requirement        |\n|:----------|:-------|:------------------|\n|arviz      |0.21.0  |arviz==0.21.0      |\n|matplotlib |3.10.3  |matplotlib==3.10.3 |\n|pandas     |2.2.3   |pandas==2.2.3      |\n|scipy      |1.15.3  |scipy==1.15.3      |\n\n</div>\n:::\n:::\n\n\n\n\n### Set up the data\n\nIn my last post, I incorrectly implemented the formula for the interaction model `mpg ~ hp * cyl`. I needed to explictly add in a variable so that we have `mpg ~ hp + cyl + hp:cyl`. To correct that, I'v added a variable that `lm()` or `brms` would typically do under the hood. Additionally added in centering the variables to help prevent divergece issues.\n\nA significant use case for me to use R in the workflow is to be able to do the data wrangling with the `tidyverse` to set up the data to input into the model. In this example, I used `dplyr::mutate()` to scale the variables. You can imagine other types of transformations, filters, and joins that could be dropped in here.\n\n::: {.callout-note collapse=\"true\"}\n([StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) does a fine job of this in the python ecosytem, so scaling isn't necessarily the point)\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmtcars_scaled <- mtcars %>%\n  mutate(\n    hp_c = scale(hp)[, 1], # scale() keeps attributes that need to be removed\n    cyl_c = scale(cyl)[, 1],\n    hp_cyl = hp_c * cyl_c\n  )\n```\n:::\n\n\n### R to Python\n\n\n::: {.cell}\n\n```{.r .cell-code}\npandas_mtcars <- r_to_py(mtcars_scaled)\nprint(class(mtcars_scaled))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"data.frame\"\n```\n:::\n\n```{.r .cell-code}\nprint(class(pandas_mtcars))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"pandas.core.frame.DataFrame\"        \"pandas.core.generic.NDFrame\"       \n[3] \"pandas.core.base.PandasObject\"      \"pandas.core.accessor.DirNamesMixin\"\n[5] \"pandas.core.indexing.IndexingMixin\" \"pandas.core.arraylike.OpsMixin\"    \n[7] \"python.builtin.object\"             \n```\n:::\n:::\n\n\nThere are now two datasets:\n\n- **mtcars_scaled**: an R `data.frame()` object\n- **pandas_mtcars**: a Python `pandas.DataFrame()` object\n\nThe pandas dataframe can be passed into PyMC and begin the Python portion of the workflow.\n\n::: {.callout-warning}\nIntegrating Python and R has come a long way, and is incredibly accessible. There are some edge cases and things to be aware of when converting data and objects between the two. [This post by Karin Hrovatin](https://hrovatin.github.io/posts/r_python/) is one of the best consolidated sources of information to learn from.\n:::\n\n\n### PyMC Model\n\nThe python sytax would look very similar to this, only now instead of using objects with dot notation, we access methods and attributes with the `$` character. If you are purely a python developer, this might look obscene. I choose to put up with this quirkyness because I find working with dataframes and plotting in R worth the trade off.\n\n#### Python\n\nReviewing the Python model from before:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nwith pm.Model(\n    coords={\"obs\": mtcars.index, \"predictors\": ['hp', 'cyl']}\n) as mod:\n\n    X = pm.Data(\"X\", mtcars[[\"hp\", \"cyl\"]], dims=(\"obs\", \"predictors\"))\n\n    alpha = pm.StudentT(\"alpha\", nu=3, mu=19.2, sigma=5.4)\n    sigma = pm.HalfStudentT(\"sigma\", nu=3, sigma=5.54)\n    beta = pm.Normal(\"b\", mu=0, sigma=1, dims='predictors')\n\n    mu = pm.Deterministic(\"mu\", alpha + pm.math.dot(X, beta), dims='obs')\n\n    y = pm.Normal(\n        \"y\",\n        mu=mu,\n        sigma=sigma,\n        shape=X.shape[0],\n        observed=mtcars[\"mpg\"],\n        dims=\"obs\",\n    )\n\n    idata = pm.sample(random_seed=527)\n\n# sample posterior predictive\nwith mod as model:\n    pp = pm.sample_posterior_predictive(idata, predictions=True)\n```\n:::\n\n\n#### R\n\nNow the same thing, but using `reticulate` to interface with `pymc`. I also add in the new column `hp_cyl` for the interaction term.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# import pymc as pm\npm <- import('pymc', convert = FALSE)\n\nmod <- pm$Model(\n  coords = list(\n    car = pandas_mtcars$index,\n    predictors = c('hp', 'cyl', 'hp_cyl')\n  )\n)\n\n# with pm.Model() as model:\n# ...\nwith(mod, {\n  X <- pm$Data('X', pandas_mtcars$loc[, c('hp', 'cyl', 'hp_cyl')])\n\n  alpha <- pm$StudentT(\"alpha\", nu = 3, mu = 19.2, sigma = 5.4)\n  sigma <- pm$HalfStudentT(\"sigma\", nu = 3, sigma = 5.54)\n  beta <- pm$Normal(\"b\", mu = 0, sigma = 1, dims = 'predictors')\n  mu <- pm$Deterministic(\"mu\", alpha + pm$math$dot(X, beta), dims = 'car')\n\n  y <- pm$Normal(\n    \"y\",\n    mu = mu,\n    sigma = sigma,\n    shape = X$shape[0], # python index\n    observed = pandas_mtcars$mpg,\n    dims = \"car\",\n  )\n  # using a single core and chain because of Quarto page rendering,\n  # normally this would be 4 chains\n  idata = pm$sample(random_seed = 527L, cores = 1L, chains=1L)\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                                \n                              Step      Grad      Sampli…                       \n  Progre…   Draws   Diverg…   size      evals     Speed     Elapsed   Remaini…  \n ────────────────────────────────────────────────────────────────────────────── \n  ━━━━━━━   2000    0         0.19      15        1449.29   0:00:01   0:00:00   \n                                                  draws/s                       \n                                                                                \n```\n:::\n\n```{.r .cell-code}\nwith(mod, {\n  pp = pm$sample_posterior_predictive(idata, predictions = TRUE)\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSampling ... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00 / 0:00:00\n```\n:::\n:::\n\n\n\nAs mentioned earlier, there are some caveats to understand when moving between the two languages. There were definitely a few gotcha's when I was implementing this:\n\n- `pandas_mtcars$loc[, c('hp', 'cyl', 'hp_cyl')]`: this used the `.loc` pandas method but with the R `data.frame()` indexing syntax.\n- `X$shape[0]`: Python is 0-based indexed while R is 1-based. Because this is a Python object, it can be indexed with 0. (I agree this is a bit weird to combine 😅)\n- `random_seed = 527L`: Integer types need to be specified with the R syntax of adding `L` at the end of it.\n\n### Posterior\n\nTranslating the PyMC model's `az.InferenceData` object and posterior predictions to dataframes is still the same, with the addition of `py_to_r()` to convert a pandas dataframe to R.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# posterior mu\ndf_posterior <- idata$posterior$mu$to_dataframe()$reset_index() |>\n  py_to_r() |>\n  as_tibble() |>\n  left_join(rownames_to_column(mtcars, 'car')) |> # R mtcars has rownames for the car\n  mutate(group = paste0(chain, draw, cyl)) # for a particular plot later\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining with `by = join_by(car)`\n```\n:::\n\n```{.r .cell-code}\n# posterior predictions of mpg\ndf_predictions <- pp$predictions$to_dataframe()$reset_index() |>\n  py_to_r() |>\n  as_tibble() |>\n  left_join(rownames_to_column(mtcars, \"car\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining with `by = join_by(car)`\n```\n:::\n:::\n\n\n### Plot\n\nNow for my favorite part of this, plot with **ggplot2** and use [tidybayes](https://mjskay.github.io/tidybayes/reference/add_predicted_draws.html) directly!\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_predictions |>\n  ggplot(aes(hp, y, color = as.factor(cyl))) +\n  tidybayes::stat_lineribbon(.width = c(.99, .95, .8, .5), alpha = 0.25) +\n  geom_line(\n    aes(y = mu, group = group),\n    data = df_posterior |> filter(draw %in% round(seq(5, 900, length.out = 5))),\n    alpha = 0.38\n  ) +\n  geom_point(aes(y = mpg), data = mtcars, shape = 21, size = 2, stroke = 1) +\n  scale_fill_brewer(palette = \"Greys\") +\n  theme_light(base_size = 12) +\n  guides(fill = 'none') +\n  labs(\n    x = 'hp',\n    y = 'mpg',\n    color = 'cyl'\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n## Rython\n\nThis is an opinionated way of using PyMC and the grammar of graphics together to say the least. I really do like PyMC, but I prefer to settle on the data and other parts of the model iteration process with R if possible. There is potential for `reticulate::py_run_string()` as well, if you wanted to be able to drop it directly back into a pure Python environment. Access to an LLM would also be able to easily reformat the R-PyMC model to Python, or at least get it most of the way there.\n\nOverall really impressed with the state of combing R and Python together. When I started my career, there weren't a ton of options outside of doing a ton of I/O with csv's to bring them together. Now within the same process I can use a full-feature Python probablistic programming library, with non-standard evaluation for data transfromation and visualization in R.\n\nA typo I had in this at one point was Rython, and given my name.. and I quite like it.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}