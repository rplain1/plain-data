{
  "hash": "5ef08074df07489ec8e4c68b936ac9ea",
  "result": {
    "markdown": "---\ntitle: \"Rython\"\nauthor: \"Ryan Plain\"\ndate: \"2025-05-31\"\ncategories: [Bayesian, PyMC, Reticulate]\ndf-print: kable\ndescription: Bayesian Modeling with PyMC and Reticulate in R.\n---\n\n\n## Bayesian models (Python) + grammar of graphics (R) = ❤️\n\nThis is a nod to the title of [Benjamin T. Vincent's blog post](https://drbenvincent.github.io/posts/mcmc_grammar_of_graphics.html), who inspired me to dive further into PyMC. The reason I started looking into using the grammar of graphics with PyMC, was to reduce a barrier and emulate **tidyverse** as much as possible. What if I instead just... use R and PyMC?\n\nIn my [previous post](../2025-05-14-tidy-pymc/), I used the Grammar of Graphics to vizualize the posterior distributions from a PyMC model. This required extracting the data from the `az.InferenceData` object, and organizing into a tidy dataframe to work with.\n\nI recently got a chance to use the [new version of reticulate](https://posit.co/blog/reticulate-1-41/), which uses `uv` to manage the Python environment used in an R session, and fell in love. From the post:\n\n> with `py_require()`, Reticulate will automatically create and manage Python environments behind the scenes so you don’t have to.\n\nThis workflow was available prior to the new version of Reticulate, but it is now incredibly simplified with `uv`.\n\n## Workflow\n\n### Set up the environment\n\nLoad up `reticulate` and the `tidyverse`.\n\n::: {.callout-note collapse=\"true\"}\nI set the environment variable for `RETICULATE_PYTHON` to force Reticulate to use an ephemeral environment. I didn't have to do this in an interactive session, but this blog already had a uv proejct setup - and I didn't want it to be used. This could also be configured outside the script or workflow.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSys.setenv(RETICULATE_PYTHON = \"managed\")\nlibrary(reticulate)\nlibrary(tidyverse)\nmtcars |>\n  head()\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|                  |  mpg| cyl| disp|  hp| drat|    wt|  qsec| vs| am| gear| carb|\n|:-----------------|----:|---:|----:|---:|----:|-----:|-----:|--:|--:|----:|----:|\n|Mazda RX4         | 21.0|   6|  160| 110| 3.90| 2.620| 16.46|  0|  1|    4|    4|\n|Mazda RX4 Wag     | 21.0|   6|  160| 110| 3.90| 2.875| 17.02|  0|  1|    4|    4|\n|Datsun 710        | 22.8|   4|  108|  93| 3.85| 2.320| 18.61|  1|  1|    4|    1|\n|Hornet 4 Drive    | 21.4|   6|  258| 110| 3.08| 3.215| 19.44|  1|  0|    3|    1|\n|Hornet Sportabout | 18.7|   8|  360| 175| 3.15| 3.440| 17.02|  0|  0|    3|    2|\n|Valiant           | 18.1|   6|  225| 105| 2.76| 3.460| 20.22|  1|  0|    3|    1|\n\n</div>\n:::\n:::\n\n\nThe first difference in this workflow is that `mtcars` is a dataset available by default, and can be accessed directly.\n\n### Reticulate and uv environment\n\nI've listend to enough presentations and interviews from Charlie Marsh[^charlie] to know I can't fully explain what `uv` does. it centralizes downloads and resolves dependencies once, so you don't reinstall packages across environments.[^uv-overview]\n\n\nUsing `reticulate::py_require()` will specify which packages are needed and how to create a virtual environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npy_require('pymc')\npy_config()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\npython:         /Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/gKtLQ4Ys-srj2T0OKKL-t/bin/python3\nlibpython:      /Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/python/cpython-3.11.12-macos-aarch64-none/lib/libpython3.11.dylib\npythonhome:     /Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/gKtLQ4Ys-srj2T0OKKL-t:/Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/gKtLQ4Ys-srj2T0OKKL-t\nvirtualenv:     /Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/gKtLQ4Ys-srj2T0OKKL-t/bin/activate_this.py\nversion:        3.11.12 (main, May 30 2025, 05:53:55) [Clang 20.1.4 ]\nnumpy:          /Users/ryan/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/gKtLQ4Ys-srj2T0OKKL-t/lib/python3.11/site-packages/numpy\nnumpy_version:  2.2.6\n\nNOTE: Python version was forced by py_require()\n```\n:::\n:::\n\n\nYou can see that we have an ephemeral python environment created with `uv` to use with `reticulate`. This is really neat! Everytime I render this document, it will cache a new virtual environment for `reticulate` to use. Thanks to uv’s global package cache, I only had to download pymc once—even across multiple isolated environments.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# > py_list_packages() |> dim()\n# [1] 63  3\n\npy_list_packages() |> dplyr::filter(package %in% c('pandas', 'scipy', 'matplotlib', 'arviz'))\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|package    |version |requirement        |\n|:----------|:-------|:------------------|\n|arviz      |0.21.0  |arviz==0.21.0      |\n|matplotlib |3.10.3  |matplotlib==3.10.3 |\n|pandas     |2.2.3   |pandas==2.2.3      |\n|scipy      |1.15.3  |scipy==1.15.3      |\n\n</div>\n:::\n:::\n\n\nSpecifying the `pymc` dependency actually built a Python environment with 63 packages, all mapped and configured with `uv`. I've shown some of the most well-known dependencies of `pymc`.\n\n### Set up the data\n\nIn my last post, I incorrectly implemented the formula for the interaction model `mpg ~ hp * cyl`. I needed to explictly add in a variable so that we have `mpg ~ hp + cyl + hp:cyl`. To correct that, I've added a variable that `lm()` or `brms` would typically do under the hood. Additionally added in centering the variables to help prevent divergece issues.\n\nA use case for me to use R in the workflow is to be able to do the initial data wrangling with the `tidyverse`. In this example, I used `dplyr::mutate()` to scale the variables. You can imagine other types of transformations, filters, and joins that could be dropped in here.[^standard-scaler]\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmtcars_scaled <- mtcars %>%\n  mutate(\n    hp_c = scale(hp)[, 1], # scale() keeps attributes that need to be removed\n    cyl_c = scale(cyl)[, 1],\n    hp_cyl = hp_c * cyl_c\n  )\n```\n:::\n\n\n### R to Python\n\n\n::: {.cell}\n\n```{.r .cell-code}\npandas_mtcars <- r_to_py(mtcars_scaled)\nprint(class(mtcars_scaled))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"data.frame\"\n```\n:::\n\n```{.r .cell-code}\nprint(class(pandas_mtcars))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"pandas.core.frame.DataFrame\"        \"pandas.core.generic.NDFrame\"       \n[3] \"pandas.core.base.PandasObject\"      \"pandas.core.accessor.DirNamesMixin\"\n[5] \"pandas.core.indexing.IndexingMixin\" \"pandas.core.arraylike.OpsMixin\"    \n[7] \"python.builtin.object\"             \n```\n:::\n:::\n\n\nThere are now two datasets:\n\n- **`mtcars_scaled`** is an R `data.frame()` object\n- **`pandas_mtcars`** is a Python `pandas.DataFrame()` object\n\nThe pandas dataframe can be passed into PyMC and begin the Python portion of the workflow.\n\n::: {.callout-warning}\nIntegrating Python and R has come a long way, and is incredibly accessible. There are some edge cases and things to be aware of when converting data and objects between the two. [This post by Karin Hrovatin](https://hrovatin.github.io/posts/r_python/) is one of the best consolidated sources of information to learn from.\n:::\n\n\n### PyMC Model\n\nThe python sytax would look very similar to this, only now instead of using objects with dot notation, we access methods and attributes with the `$` character. If you are purely a python developer, this might look obscene. I choose to put up with this quirkyness because I find working with dataframes and plotting in R worth the trade off.\n\nPython code from before.\n\n::: {.cell}\n\n```{.python .cell-code  code-fold=\"true\"}\n#|\nwith pm.Model(\n    coords={\"obs\": mtcars.index, \"predictors\": ['hp', 'cyl']}\n) as mod:\n\n    X = pm.Data(\"X\", mtcars[[\"hp\", \"cyl\"]], dims=(\"obs\", \"predictors\"))\n\n    alpha = pm.StudentT(\"alpha\", nu=3, mu=19.2, sigma=5.4)\n    sigma = pm.HalfStudentT(\"sigma\", nu=3, sigma=5.54)\n    beta = pm.Normal(\"b\", mu=0, sigma=1, dims='predictors')\n\n    mu = pm.Deterministic(\"mu\", alpha + pm.math.dot(X, beta), dims='obs')\n\n    y = pm.Normal(\n        \"y\",\n        mu=mu,\n        sigma=sigma,\n        shape=X.shape[0],\n        observed=mtcars[\"mpg\"],\n        dims=\"obs\",\n    )\n\n    idata = pm.sample(random_seed=527)\n\n# sample posterior predictive\nwith mod as model:\n    pp = pm.sample_posterior_predictive(idata, predictions=True)\n```\n:::\n\n\n\nNow the same thing, but using `reticulate` to interface with `pymc`. I've also added in the new column `hp_cyl` for the interaction term.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# import pymc as pm\npm <- import('pymc', convert = FALSE)\n\nmod <- pm$Model(\n  coords = list(\n    car = pandas_mtcars$index,\n    predictors = c('hp', 'cyl', 'hp_cyl')\n  )\n)\n\n# with pm.Model() as model:\n# ...\nwith(mod, {\n  X <- pm$Data('X', pandas_mtcars$loc[, c('hp', 'cyl', 'hp_cyl')])\n\n  alpha <- pm$StudentT(\"alpha\", nu = 3, mu = 19.2, sigma = 5.4)\n  sigma <- pm$HalfStudentT(\"sigma\", nu = 3, sigma = 5.54)\n  beta <- pm$Normal(\"b\", mu = 0, sigma = 1, dims = 'predictors')\n  mu <- pm$Deterministic(\"mu\", alpha + pm$math$dot(X, beta), dims = 'car')\n\n  y <- pm$Normal(\n    \"y\",\n    mu = mu,\n    sigma = sigma,\n    shape = X$shape[0], # python index\n    observed = pandas_mtcars$mpg,\n    dims = \"car\",\n  )\n  # using a single core and chain because of Quarto page rendering,\n  # normally this would be 4 chains\n  idata = pm$sample(random_seed = 527L, cores = 1L, chains=1L)\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                                \n                              Step      Grad      Sampli…                       \n  Progre…   Draws   Diverg…   size      evals     Speed     Elapsed   Remaini…  \n ────────────────────────────────────────────────────────────────────────────── \n  ━━━━━━━   2000    0         0.19      15        1430.95   0:00:01   0:00:00   \n                                                  draws/s                       \n                                                                                \n```\n:::\n\n```{.r .cell-code}\nwith(mod, {\n  pp = pm$sample_posterior_predictive(idata, predictions = TRUE)\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSampling ... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00 / 0:00:00\n```\n:::\n:::\n\n\n### Gotchas\n\nSome quirks to be aware of:\n\n- **`pandas_mtcars$loc[, c('hp', 'cyl', 'hp_cyl')]`**\n  Uses the `.loc` method from pandas, but with R-style dataframe indexing syntax.\n\n- **`X$shape[0]`**\n  Python is 0-based indexed while R is 1-based. Since `X` is a Python object, we use `0` for indexing.\n\n- **`random_seed = 527L`**\n  Integer literals in R require `L` to indicate an integer type, which Python expects here.\n\n\n### Posterior\n\nTranslating the PyMC model's `az.InferenceData` object and posterior predictions to dataframes is still the same, with the addition of `py_to_r()` to convert a pandas dataframe to R.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# posterior mu\ndf_posterior <- idata$posterior$mu$to_dataframe()$reset_index() |>\n  py_to_r() |>\n  as_tibble() |>\n  left_join(rownames_to_column(mtcars, 'car')) |> # R mtcars has rownames for the car\n  mutate(group = paste0(chain, draw, cyl)) # for a particular plot later\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining with `by = join_by(car)`\n```\n:::\n\n```{.r .cell-code}\n# posterior predictions of mpg\ndf_predictions <- pp$predictions$to_dataframe()$reset_index() |>\n  py_to_r() |>\n  as_tibble() |>\n  left_join(rownames_to_column(mtcars, \"car\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining with `by = join_by(car)`\n```\n:::\n:::\n\n\n### Plot\n\nNow for my favorite part of this, plot with **ggplot2** and use [tidybayes](https://mjskay.github.io/tidybayes/reference/add_predicted_draws.html) directly!\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_predictions |>\n  ggplot(aes(hp, y, color = as.factor(cyl))) +\n  tidybayes::stat_lineribbon(.width = c(.99, .95, .8, .5), alpha = 0.25) +\n  geom_line(\n    aes(y = mu, group = group),\n    data = df_posterior |> filter(draw %in% round(seq(5, 900, length.out = 5))),\n    alpha = 0.38\n  ) +\n  geom_point(aes(y = mpg), data = mtcars, shape = 21, size = 2, stroke = 1) +\n  scale_fill_brewer(palette = \"Greys\") +\n  theme_light(base_size = 12) +\n  guides(fill = 'none') +\n  labs(\n    x = 'hp',\n    y = 'mpg',\n    color = 'cyl'\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n## Rython\n\nThis is an opinionated way of using PyMC and the grammar of graphics together to say the least. I really do like PyMC, but I prefer to settle on the data and other parts of the model iteration process with R if possible. There is potential for `reticulate::py_run_string()` as well, if you wanted to be able to drop it directly back into a pure Python environment. Access to an LLM would also be able to easily reformat the R-PyMC model to Python, or at least get it most of the way there.\n\nI'm genuinely impressed by how far integrating R and Python has come. When I started my career, you had to do a bunch of clunky I/O to get features of both languages. Now, in a single workflow, I can use a full-featured Python probabilistic programming library alongside R’s non-standard evaluation for data transformation and visualization.\n\nA typo I had in drafting this at one point was *Rython*, and given my name... and I quite like it.\n\n\n[^charlie]: Charlie Marsh is the lead developer of Ruff and discussed `uv` in multiple talks.\n\n[^uv-overview]: `uv` is a Rust-based Python package manager that installs dependencies in a global cache and reuses them in isolated environments, improving reproducibility and speed. For more, see the [uv docs](https://docs.astral.sh/uv/).\n\n[^standard-scaler]: StandardScaler from sci-kit learn does this in the python, so scaling isn't necessarily the point.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}