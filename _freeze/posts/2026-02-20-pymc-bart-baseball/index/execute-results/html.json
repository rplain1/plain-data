{
  "hash": "fef89093cafd88bdf266fb9f84b94ed1",
  "result": {
    "markdown": "---\ntitle: \"BART Baseball with PyMC\"\nauthor: \"Ryan Plain\"\ndate: \"2026-02-20\"\ncategories: [Bayesian, PyMC, BART]\ndescription: Replicating a PyMC Labs BART example for predicting pitcher whiff rates, with a look at hierarchical park effects and a Python tidybayes package.\nengine: jupyter\n---\n\nPyMC Labs put out a post recently called [Modeling Swinging Strikes with Bayesian Additive Regression Trees (BART)](https://www.pymc-labs.com/blog-posts/bayesian-additive-regression-tree-swinging-strikes). It goes over implementing BART models in PyMC and provides a nuanced use case.\n\nAt first I was largely dissapointed that only some of the code was included. While it would have been nice to have more of the sports logic to help replicate it, it did describe it through text. I have a better understanding now about the balance of detail in a post after working on this one. To include all the code would have required more time discussing the process, which takes away from the main premise of demonstrating the model.\n\nThe work following is my attempt at recreating the models, visualizations, and analysis.\n\nAdditionally, being an R enthusiast I built a [bayestidy](https://github.com/rplain1/bayestidy) package that replicates some of what [Tidybayes](https://mjskay.github.io/tidybayes/) does for the R ecosystem. This is an opinonated framework to working with the posterior distributions, built on previous work I've done.\n\n## Imports\n\nThe original post mentions that all of this can be run on a local laptop. It _can_ be done on a local laptop... technically. My machine is quite small relative to today's standards with 16 gb of memory, so that was a bit of a stretch.\n\nI have simple `RERUN` flag to compile the models or not.  To work more efficiently I saved the models to disk and subsequently loaded them back in with context managers to access what was needed. This way I didn't have to have all of it in memory at once, which was critical to render the document.\n\n::: {.callout-note collapse=\"true\"}\nI leveraged Claude to understand how to get PyMC to render in quarto and jupyter. The first set of imports handle that, and LLM commented as such.\n\nI never figured that out on my own, and ran the chains consecutively in previous posts...\n:::\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport sys\n\n# Python's multiprocessing spawn code assumes __main__.__spec__ exists,\nfrom mizani.scale import scale_continuous\n\n# but Jupyter/Quarto kernels don't set it. Fix before pymc_bart imports.\nif not hasattr(sys.modules.get(\"__main__\"), \"__spec__\"):\n    sys.modules[\"__main__\"].__spec__ = None\n\nfrom pybaseball import statcast, cache\nimport pymc as pm\nimport pymc_bart as pmb\nimport polars as pl\nimport pandas as pd\nimport numpy as np\nimport arviz as az\nfrom pathlib import Path\nfrom sklearn.preprocessing import StandardScaler\nfrom plotnine import *\nimport bayestidy as bt\nimport pickle\nimport xarray as xr\nfrom pymc_bart.utils import _sample_posterior  # needed for BART with new session\n\n# Set to True to re-fetch data from pybaseball and refit the model\nRERUN = False\n\nif RERUN or not Path(\"data/df.parquet\").exists():\n    cache.disable()\n    df = statcast(start_dt=\"2024-03-28\", end_dt=\"2024-09-29\")\n    pl.from_pandas(df).write_parquet(\"data/df.parquet\")\nelse:\n    df = pl.read_parquet(\"data/df.parquet\").to_pandas()\n```\n:::\n\n\nPlotting functions below.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\"}\ndef calibration_plot(p_hat, y, label=\"Train\"):\n    cal_df = pl.DataFrame({\"p_hat\": p_hat.astype(np.float64), \"y\": y})\n    breaks = np.linspace(cal_df[\"p_hat\"].min(), cal_df[\"p_hat\"].max(), 11)[\n        1:-1\n    ].tolist()\n    cal_summary = (\n        cal_df.with_columns(pl.col(\"p_hat\").cut(breaks).alias(\"bin\"))\n        .group_by(\"bin\")\n        .agg(\n            pred_mean=pl.col(\"p_hat\").mean(),\n            obs_mean=pl.col(\"y\").mean(),\n            n=pl.col(\"y\").count(),\n        )\n        .sort(\"pred_mean\")\n        .to_pandas()\n    )\n    return (\n        ggplot(cal_summary, aes(\"pred_mean\", \"obs_mean\", size=\"n\"))\n        + geom_abline(intercept=0, slope=1, linetype=\"dashed\", color=\"grey\")\n        + geom_point(color=\"#FF8C69\", alpha=0.85)\n        + scale_size_continuous(range=(2, 14), name=\"N Pitches\")\n        + labs(\n            title=f\"Calibration Plot ({label} Data)\",\n            x=\"Predicted Whiff Probability\",\n            y=\"Observed Whiff Rate\",\n        )\n        + theme_minimal()\n        + coord_cartesian(xlim=[0, 0.5], ylim=[0, 0.5])\n    )\n\n\ndef add_whiff_plus(df):\n    assert \"p_hat\" in df.columns, \"dataframe does not have `p_hat`\"\n    p_hat = pl.col(\"p_hat\")\n    return df.with_columns(whiff_plus=100 + 10 * (p_hat - p_hat.mean()) / p_hat.std())\n```\n:::\n\n\n## Model\n\nThe first model replicates PyMC Lab's 4-feature model. If my sports logic is somewhat correct for how they filtered the data, the rest was fairly straightforward.\n- filter for `4-Seam Fastball`\n- calculate whiffs as swings\n- I removed 200 records with null values\n- Split into train/test splits\n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\"}\ncols = [\n    \"release_speed\",\n    \"pfx_x\",\n    \"pfx_z\",\n    \"release_spin_rate\",\n    \"description\",\n    \"pitcher\",\n    \"player_name\",\n]\npitch_name = \"4-Seam Fastball\"\n\n# this came from assessing the following and removing bunts\n# df.assign(swing = lambda x: ~x['swing_length'].isna()).groupby(['swing', 'description']).count()\nswings = [\n    \"hit_into_play\",\n    \"swinging_strike\",\n    \"foul_tip\",\n    \"swinging_strike_blocked\",\n    \"foul\",\n]\n\nwhiffs = [\"swinging_strike\", \"swinging_strike_blocked\"]\n\ndf_model = df.copy().query(f'pitch_name == \"{pitch_name}\" and description in {swings}')[\n    cols\n]\n\ndf_model = df_model.dropna()\n\n\n\n# Hold out 20 random pitchers for test set\nrng = np.random.default_rng(42)\npitchers = df_model[\"pitcher\"].unique()\nholdout_pitchers = rng.choice(pitchers, size=20, replace=False)\n\ntrain_mask = ~df_model[\"pitcher\"].isin(holdout_pitchers)\nfeature_cols = [\"release_speed\", \"pfx_x\", \"pfx_z\", \"release_spin_rate\"]\n\ny = df_model[\"description\"].isin(whiffs).astype(int)\n\n# Standardize features to z-scores (fit on train only)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(df_model.loc[train_mask, feature_cols])\nX_test = scaler.transform(df_model.loc[~train_mask, feature_cols])\ny_train = y[train_mask].values\ny_test = y[~train_mask].values\n\nplayer_names_train = df_model.loc[train_mask, \"player_name\"].values\ncoords = {\"obs\": player_names_train}\n```\n:::\n\n\n{::: {.callout-caution}\n\nWorking with the posterior `az.InferenceData` object turned out to have different behavior than expected. One of the biggest gotchas was how BART models are saved to disk, and the tree information you lose when starting a new session. I've implemented it below, but it is covered in this [GitHub issue](https://github.com/pymc-devs/pymc-bart/issues/123#issuecomment-3105432020)\n\nEssentially I was able to still save the object and load it into new sessions to continue working with the posterior, however the tree information was not saved with the object (that is the extent of my understanding). Coming back to this in a new session, only the posterior distributions for the saved parameters and values were available. Predictions were all off.\n\nI ended up finding a solution documented in this [GitHub issue](https://github.com/pymc-devs/pymc-bart/issues/123#issuecomment-3105432020).\n\nSo here, I'm saving the `az.InferenceData` object and additionally a pickle file of the trees. Later on I used the functions in that github issue to do predictions.\n:::}\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nif (\n    RERUN\n    or not Path(\"models/trace.nc\").exists()\n    or not Path(\"models/bart_trees.pkl\").exists()\n):\n    with pm.Model(coords=coords) as bart_model:\n\n        X_data = pm.Data(\"X_data\", X_train, dims=(\"obs\", \"feature\"))\n        y_data = pm.Data(\"y_data\", y_train, dims=\"obs\")\n\n        mu_bart = pmb.BART(\"mu_bart\", X=X_data, Y=y_data, m=100)\n\n        mu = pm.Deterministic(\"mu\", mu_bart, dims=\"obs\")\n        p = pm.Deterministic(\"p\", pm.math.sigmoid(mu), dims=\"obs\")\n\n        pm.Bernoulli(\"y_obs\", p=p, observed=y_data, dims=\"obs\")\n        idata = pm.sample(draws=2000, chains=2, random_seed=42)\n\n    idata.to_netcdf(\"models/trace.nc\")\n\n    del idata\n\n    all_trees = list(mu_bart.owner.op.all_trees)\n    with open(\"models/bart_trees.pkl\", \"wb\") as f:\n        pickle.dump(all_trees, f)\n\nelse:\n\n    with open(\"models/bart_trees.pkl\", \"rb\") as f:\n        all_trees = pickle.load(f)\n```\n:::\n\n\n### Working with constraints\n\nThe `p_train` aggregation is used for the calibration plots.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nwith xr.open_dataset(\"models/trace.nc\", group=\"posterior\") as post:\n    p_train = post[\"p\"].mean(dim=(\"chain\", \"draw\")).values\n    obs_names_train = post.coords[\"obs\"].values\n\nwith xr.open_dataset(\"models/trace.nc\", group=\"observed_data\") as obs_data:\n    y_train = obs_data[\"y_obs\"].values\n\n# Update coords to match the saved model's obs ordering\ncoords = {\"obs\": obs_names_train}\n```\n:::\n\n\n### Test Set\n\nThe workaround for BART in new sessions is to use `pymc_bart.utils._sample_posterior()`. Unlike most Bayesian models I've worked with, the predictions are from the trees and not a draw from a combination of parameters. I beleive if I wanted to sample the posterior predictive values for the test set, I could have done something like this:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nwith bart_model:\n    pm.set_data({\"X_data\": X_test})\n    ppc = pm.sample_posterior_predictive(idata, var_names=[\"mu\", \"p\"])\n```\n:::\n\n\nThen saved that with the `az.Inferencedata` object. For the BART model, the calibrations on the test set give insight to the validity of the model's ability to predict, so I didn't go through that process here.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nrng = np.random.default_rng(42)\nmu_test_samples = _sample_posterior(all_trees, X_test, rng=rng, size=500, shape=1)\n# shape: (500, n_test, 1) → squeeze last dim → (500, n_test)\nmu_test_samples = mu_test_samples.squeeze(-1)\np_test = (1 / (1 + np.exp(-mu_test_samples))).mean(axis=0)\n```\n:::\n\n\n### Calibration Plots\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ncalibration_plot(p_train, y_train, label=\"Train\")\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n![](index_files/figure-html/cell-9-output-1.png){width=672 height=480}\n:::\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ncalibration_plot(p_test, y_test, label=\"Test\")\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n![](index_files/figure-html/cell-10-output-1.png){width=672 height=480}\n:::\n:::\n\n\nLooking good!\n\n## Whiff+\n\nI appreciated this metric as I have little to no domain knowledge in baseball. The pitcher names could be replaced with human-readable uuids and it would not effect on my ability to work with the data.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code code-fold=\"true\"}\nif not isinstance(df, pl.DataFrame):\n    df = pl.from_pandas(df)\n\ndf_train = pl.DataFrame({\n    \"player_name\": obs_names_train,\n    \"p_hat\": p_train.astype(np.float64),\n}).pipe(add_whiff_plus)\n\ndf_test = pl.from_pandas(df_model.loc[~train_mask].assign(p_hat=p_test)).pipe(add_whiff_plus)\n```\n:::\n\n\n### Median whiff plus leaderboard - training set\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nleaderboard = df_train.group_by(\"player_name\").agg(\n    pl.col(\"whiff_plus\").median()\n).sort(\"whiff_plus\", descending=True)\nleaderboard\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (730, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>player_name</th><th>whiff_plus</th></tr><tr><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;Kopech, Michael&quot;</td><td>125.462949</td></tr><tr><td>&quot;Montgomery, Mason&quot;</td><td>124.361451</td></tr><tr><td>&quot;Helsley, Ryan&quot;</td><td>120.247161</td></tr><tr><td>&quot;Ort, Kaleb&quot;</td><td>119.918061</td></tr><tr><td>&quot;Estrada, Jeremiah&quot;</td><td>119.773609</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;Lyles, Jordan&quot;</td><td>83.248874</td></tr><tr><td>&quot;Dunning, Dane&quot;</td><td>83.025733</td></tr><tr><td>&quot;Lively, Ben&quot;</td><td>82.574925</td></tr><tr><td>&quot;Rogers, Tyler&quot;</td><td>82.3153</td></tr><tr><td>&quot;Crawford, Brandon&quot;</td><td>82.221916</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n### Visualizing the leaderboard\n\nThe size of the dataset did not lend well to work with the new package. In its current state, `bayestidy` will convert everything to pandas in underlying `plotnine` functions, as it is yet to have first class `polars` support.\n\n::: {.callout-note collapse=\"true\"}\nThis is a long-standing [issue](https://github.com/has2k1/plotnine/issues/602) that needs a solution, outside of polars from what I can tell.\n:::\n\nThe tidybayes approach would have been something like the following:\n\n```r\nfit  |>\n    as_draws_df() |>\n    pivot_longer(everything()) |>\n    filter(player %in% player_names) |>\n    ggplot(aes(value, player_name)) |>\n    stat_halfeye()\n```\n\nWith this size of dataset, `bt.spread_draws()` also really struggled here. I'm still learning how I can make this better, but for now I compute whiff+ in xarray space across the full pitcher population before subsetting.\n\nAt least I can still use the tidy framework for plotting!\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ndef whiff_plus_plot(leaderboard, post, coords, n=15, top=True, var=\"p\"):\n    players = (leaderboard.head(n) if top else leaderboard.tail(n))[\n        \"player_name\"\n    ].to_list()\n\n    indices = np.where(np.isin(coords[\"obs\"], players))[0]\n\n    p_da = post[var]\n    p_mean = p_da.mean(dim=\"obs\")\n    p_std = p_da.std(dim=\"obs\")\n    whiff_plus_da = (100 + 10 * (p_da - p_mean) / p_std).isel(obs=indices)\n\n    df_draws = bt.spread_draws(\n        whiff_plus_da.to_dataset(name=\"whiff_plus\"), \"whiff_plus\", group=None\n    )\n\n    order = (\n        df_draws.group_by(\"obs\")\n        .agg(pl.col(\"whiff_plus\").mean())\n        .sort(\"whiff_plus\")[\"obs\"]\n        .to_list()\n    )\n\n    plot_data = df_draws.group_by(\"obs\").map_groups(\n        lambda df: df.sample(n=min(100_000, len(df)), with_replacement=False)\n    )\n\n    title = f\"Top {n} Whiff+\" if top else f\"Bottom {n} Whiff+\"\n    fill = \"lightblue\" if top else \"lightsalmon\"\n    return (\n        ggplot(plot_data, aes(\"whiff_plus\", \"obs\"))\n        + bt.stat_halfeye(fill=fill, alpha=0.5)\n        + scale_y_discrete(limits=order)\n        + labs(title=title, x=\"Whiff+\", y=\"Pitcher\")\n        + theme_minimal()\n    )\n```\n:::\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code code-fold=\"true\"}\nwith xr.open_dataset(\"models/trace.nc\", group=\"posterior\") as post:\n    plot_top = whiff_plus_plot(leaderboard, post, coords, n=10, top=True)\n    plot_bottom = whiff_plus_plot(leaderboard, post, coords, n=10, top=False)\n\nplot_top\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n![](index_files/figure-html/cell-14-output-1.png){width=672 height=480}\n:::\n:::\n\n\n::: {.cell execution_count=14}\n``` {.python .cell-code code-fold=\"true\"}\nplot_bottom\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n![](index_files/figure-html/cell-15-output-1.png){width=672 height=480}\n:::\n:::\n\n\n## Model (Improved)\n\nNext to implement their improved model, which included a lot of baseball physics that I barely grasp. I'm just trying to learn BART to be able to use it in a domain I'm familiar with, so this logic was mostly LLM generated.\n\nWhat I did find incredibly useful was it's combination of a hierarchical component for park, in conjunction with a tree based model. PyMC made this really slick to implement.\n\nLogic to setup the training and test sets below. They both have similar training splits, but I have inconsistent coordinate mappings and it is not worth the couple of hours it takes to run on my machine to work through a clean refactor.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code code-fold=\"true\"}\nif isinstance(df, pl.DataFrame):\n    df = df.to_pandas()\n\ncols = [\n    \"release_speed\",\n    \"pfx_x\",\n    \"pfx_z\",\n    \"release_spin_rate\",\n    \"description\",\n    \"pitcher\",\n    \"player_name\",\n    \"home_team\",\n]\ndf[\"platoon_indicator\"] = (df[\"stand\"] == df[\"p_throws\"]).astype(int)\n\n# ------------- Disclaimer ------------------------------\n# This is outside the scope of what I wanted to learn here. All of the physics\n# features are generated from claude code.\n\n# Spin axis is a clock-face angle in degrees\n# Convert to a predicted movement direction unit vector\nspin_rad = np.radians(df[\"spin_axis\"])\npred_x = np.sin(spin_rad)  # horizontal component of predicted movement\npred_z = np.cos(spin_rad)  # vertical component of predicted movement\n\n# Actual movement vector\nact_x = df[\"pfx_x\"]\nact_z = df[\"pfx_z\"]\n\n# Angle between predicted and actual movement vectors\ndot = pred_x * act_x + pred_z * act_z\npred_mag = np.sqrt(pred_x**2 + pred_z**2)  # always 1 since it's a unit vector\nact_mag = np.sqrt(act_x**2 + act_z**2)\n\ndf[\"axis_differential\"] = np.degrees(\n    np.arccos(np.clip(dot / (pred_mag * act_mag), -1, 1))\n)\n\n# ----------------------------------------------------------\n\nnew_cols = [\n    \"release_pos_x\",\n    \"release_pos_z\",\n    \"release_extension\",\n    \"platoon_indicator\",\n    \"axis_differential\",\n]\n\npitch_name = \"4-Seam Fastball\"\n\n# this came from assessing the following and removing bunts\n# df.assign(swing = lambda x: ~x['swing_length'].isna()).groupby(['swing', 'description']).count()\nswings = [\n    \"hit_into_play\",\n    \"swinging_strike\",\n    \"foul_tip\",\n    \"swinging_strike_blocked\",\n    \"foul\",\n]\n\nwhiffs = [\"swinging_strike\", \"swinging_strike_blocked\"]\n\ndf_model_imp = df.copy().query(\n    f'pitch_name == \"{pitch_name}\" and description in {swings}'\n)[cols + new_cols]\n\ndf_model_imp = df_model_imp.dropna()\n\n# Hold out 20 random pitchers for test set\nrng = np.random.default_rng(42)\npitchers_imp = df_model_imp[\"pitcher\"].unique()\nholdout_pitchers_imp = rng.choice(pitchers_imp, size=20, replace=False)\n\ntrain_mask_imp = ~df_model_imp[\"pitcher\"].isin(holdout_pitchers_imp)\nfeature_cols_imp = [\"release_speed\", \"pfx_x\", \"pfx_z\", \"release_spin_rate\"] + new_cols\n\ny_imp = df_model_imp[\"description\"].isin(whiffs).astype(int)\n\n# Standardize features to z-scores (fit on train only)\nscaler_imp = StandardScaler()\nX_imp_train = scaler_imp.fit_transform(\n    df_model_imp.loc[train_mask_imp, feature_cols_imp]\n)\nX_imp_test = scaler_imp.transform(df_model_imp.loc[~train_mask_imp, feature_cols_imp])\ny_imp_train = y_imp[train_mask_imp].values\ny_imp_test = y_imp[~train_mask_imp].values\n\n# Encode home_team as integer park indices (fit on train only)\npark_categories = pd.Categorical(\n    df_model_imp.loc[train_mask_imp, \"home_team\"]\n).categories\nn_parks_improved = len(park_categories)\npark_idx_imp_train = pd.Categorical(\n    df_model_imp.loc[train_mask_imp, \"home_team\"], categories=park_categories\n).codes\npark_idx_imp_test = pd.Categorical(\n    df_model_imp.loc[~train_mask_imp, \"home_team\"], categories=park_categories\n).codes\n\nplayer_names_imp_train = df_model_imp.loc[train_mask_imp, \"player_name\"].values\ncoords_imp = {\"obs\": player_names_imp_train}\n```\n:::\n\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nif (\n    RERUN\n    or not Path(\"models/trace_improved.nc\").exists()\n    or not Path(\"models/bart_trees_imp.pkl\").exists()\n):\n    with pm.Model(coords=coords_imp) as improved_bart_model:\n\n        # ------------- DATA -------------------------\n        X_imp_data = pm.Data(\"X_imp_data\", X_imp_train)\n        y_imp_data = pm.Data(\"y_imp_data\", y_imp_train)\n        park_idx_imp_data = pm.Data(\"park_idx_imp_data\", park_idx_imp_train)\n\n        # ------------- PARAMS -------------------------\n        mu_bart_imp = pmb.BART(\"mu_bart_imp\", X=X_imp_data, Y=y_imp_data, m=150)\n        park_sigma_imp = pm.HalfNormal(\"park_sigma_imp\", sigma=0.5)\n        park_effect_imp = pm.Normal(\n            \"park_effect_imp\", mu=0, sigma=park_sigma_imp, shape=n_parks_improved\n        )\n\n        mu_imp = pm.Deterministic(\n            \"mu_imp\",\n            mu_bart_imp + park_effect_imp[park_idx_imp_data],\n        )\n        p_imp = pm.Deterministic(\"p_imp\", pm.math.sigmoid(mu_imp))\n\n        # ------------- Likelihood -------------------------\n        pm.Bernoulli(\"y_obs_imp\", p=p_imp, observed=y_imp_data)\n        idata_imp = pm.sample(draws=2000, chains=2, random_seed=42)\n\n\n    idata_imp.to_netcdf(\"models/trace_improved.nc\")\n    del idata_imp\n    all_trees_imp = list(mu_bart_imp.owner.op.all_trees)\n\n    with open(\"models/bart_trees_imp.pkl\", \"wb\") as f:\n        pickle.dump(all_trees_imp, f)\nelse:\n    with open(\"models/bart_trees_imp.pkl\", \"rb\") as f:\n        all_trees_imp = pickle.load(f)\n```\n:::\n\n\n### Predictions\n\nI had to use the same `_sample_posterior()` function here to work with the object in new sessions. In addition to `mu`, there is also the `park_effect` parameter that needs to be incorporated.\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nwith xr.open_dataset(\"models/trace_improved.nc\", group=\"posterior\") as post:\n    p_imp_train = post[\"p_imp\"].mean(dim=(\"chain\", \"draw\")).values\n    park_effects_all = post[\"park_effect_imp\"].values.reshape(-1, n_parks_improved)\n\n# Predict on test set using _sample_posterior + park effects\nrng_imp = np.random.default_rng(42)\nmu_bart_test_samples = _sample_posterior(all_trees_imp, X_imp_test, rng=rng_imp, size=500, shape=1)\n# shape: (500, n_test, 1) → (500, n_test)\nmu_bart_test_samples = mu_bart_test_samples.squeeze(-1)\n\n# Sample matching park effects from posterior\nn_samples = mu_bart_test_samples.shape[0]\nsample_idx = np.random.default_rng(42).choice(len(park_effects_all), size=n_samples, replace=False)\npark_effects_test = park_effects_all[sample_idx][:, park_idx_imp_test]  # (500, n_test)\n\nmu_imp_test = mu_bart_test_samples + park_effects_test\np_imp_test = (1 / (1 + np.exp(-mu_imp_test))).mean(axis=0)\n```\n:::\n\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\ncalibration_plot(p_imp_train, y_imp_train, label=\"Train (Improved)\")\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n![](index_files/figure-html/cell-19-output-1.png){width=672 height=480}\n:::\n:::\n\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\ncalibration_plot(p_imp_test, y_imp_test, label=\"Test (Improved)\")\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n![](index_files/figure-html/cell-20-output-1.png){width=672 height=480}\n:::\n:::\n\n\nCalibration looking even better on the test set!\n\n### Median whiff plus leaderboard (Improved)\n\n::: {.cell execution_count=20}\n``` {.python .cell-code code-fold=\"true\"}\ndf_train_imp = pl.from_pandas(\n    df_model_imp.loc[train_mask_imp].assign(p_hat=p_imp_train)\n).pipe(add_whiff_plus)\n\ndf_test_imp = pl.from_pandas(\n    df_model_imp.loc[~train_mask_imp].assign(p_hat=p_imp_test)\n).pipe(add_whiff_plus)\n\nleaderboard_imp = (\n    df_train_imp.group_by([\"player_name\", \"pitcher\"])\n    .agg(pl.col(\"whiff_plus\").median())\n    .sort(\"whiff_plus\", descending=True)\n)\nleaderboard_imp\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (731, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>player_name</th><th>pitcher</th><th>whiff_plus</th></tr><tr><td>str</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;Kopech, Michael&quot;</td><td>656629</td><td>125.205007</td></tr><tr><td>&quot;Ort, Kaleb&quot;</td><td>672391</td><td>120.787765</td></tr><tr><td>&quot;Otañez, Michel&quot;</td><td>671305</td><td>119.323257</td></tr><tr><td>&quot;Núñez, Dedniel&quot;</td><td>673380</td><td>118.740277</td></tr><tr><td>&quot;García, Yimi&quot;</td><td>554340</td><td>118.655864</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;Barnes, Matt&quot;</td><td>598264</td><td>82.841306</td></tr><tr><td>&quot;Smyly, Drew&quot;</td><td>592767</td><td>82.437835</td></tr><tr><td>&quot;Loutos, Ryan&quot;</td><td>702795</td><td>82.084812</td></tr><tr><td>&quot;Buchanan, David&quot;</td><td>571527</td><td>81.49745</td></tr><tr><td>&quot;Bauers, Jake&quot;</td><td>641343</td><td>79.865233</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=21}\n``` {.python .cell-code code-fold=\"true\"}\nwith xr.open_dataset(\"models/trace_improved.nc\", group=\"posterior\") as post:\n    post_renamed = (\n        post\n        .assign_coords(p_imp_dim_0=(\"p_imp_dim_0\", player_names_imp_train))\n        .rename({\"p_imp_dim_0\": \"obs\"})\n    )\n    plot_top_imp = whiff_plus_plot(leaderboard_imp, post_renamed, coords_imp, n=10, top=True, var=\"p_imp\")\n    plot_bottom_imp = whiff_plus_plot(leaderboard_imp, post_renamed, coords_imp, n=10, top=False, var=\"p_imp\")\n    park_effects = bt.spread_draws(post, \"park_effect_imp\", group=None).join(\n        pl.DataFrame({\"park\": pl.Series(park_categories)}).with_row_index(),\n        left_on=\"park_effect_imp_dim_0\",\n        right_on=\"index\",\n    )\n\nplot_top_imp\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n![](index_files/figure-html/cell-22-output-1.png){width=672 height=480}\n:::\n:::\n\n\n::: {.cell execution_count=22}\n``` {.python .cell-code code-fold=\"true\"}\nplot_bottom_imp\n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n![](index_files/figure-html/cell-23-output-1.png){width=672 height=480}\n:::\n:::\n\n\nAgain, I don't feel like refactoring too much -- but a more effective plot would have been to combine the top and bottom players together and colored them appropriately. This would show the discrepancy.\n\n### Park Effects\n\nThis is what fascinated me and exactly why I was driven to learn some of this. I'm thankful that they made the blog post to show it in action.\n\nUnlike the previous posterior distributions, `bayestidy` is well suited here as there are only 30 parks, so the dataset is only `30 x 2 x 2000`. This is where the package and tidy framework can really shine. (on my small machine)\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\npark_means = park_effects.group_by(\"park\").agg(\n    pl.col(\"park_effect_imp\").mean().alias(\"park_mean\")\n)\n\norder = park_means.sort(\"park_mean\")[\"park\"].to_list()\n\npark_effects_plot = park_effects.join(park_means, on=\"park\").with_columns(\n    sign=pl.when(pl.col(\"park_mean\") < 0)\n    .then(pl.lit(\"negative\"))\n    .otherwise(pl.lit(\"positive\"))\n)\n\n(\n    ggplot(park_effects_plot, aes(\"park_effect_imp\", \"park\", color=\"sign\"))\n    + bt.stat_pointinterval()\n    + scale_y_discrete(limits=order)\n    + geom_vline(xintercept=0, linetype=\"dashed\")\n    + scale_color_manual(\n        values={\"negative\": \"lightsalmon\", \"positive\": \"lightblue\"}, guide=None\n    )\n    + theme_minimal()\n    + theme(panel_grid_minor=element_blank())\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n![](index_files/figure-html/cell-24-output-1.png){width=672 height=480}\n:::\n:::\n\n\n## Comparing both models\n\nPyMC Labs did a rigorous model comparison with WAIC and log-likelihood. I'm not going to re-implement that here, as this post is mainly for me to reference. What I do find interesting is conveying results visually.\n\nI often have use cases where I want custom plots comparing distributions between multiple models, and this is the beginning framework.\n\nThe code below just efficiently extracts what I need out of the two models.\n\n::: {.cell execution_count=24}\n``` {.python .cell-code code-fold=\"true\"}\nwith xr.open_dataset(\"models/trace.nc\", group=\"posterior\") as post:\n    p = post[\"p\"]  # lazy, shape: (chain, draw, obs)\n    p_mean = float(p.mean())  # triggers a single pass, returns scalar\n    p_std = float(p.std())\n\n    # now only materialize what you need\n    player_means = p.mean(dim=(\"chain\", \"draw\"))  # shape: (obs,)\n    players = [\"Kopech, Michael\"]\n\n    indices = np.where(np.isin(coords[\"obs\"], players))[0]\n\n    df_basic = bt.spread_draws(post[[\"p\"]].isel(obs=indices), \"p\", group=None)\n\nwith xr.open_dataset(\"models/trace_improved.nc\", group=\"posterior\") as post:\n\n    players = [\"Kopech, Michael\"]\n\n    indices = np.where(np.isin(coords_imp[\"obs\"], players))[0]\n\n    df_improved = bt.spread_draws(\n        post[[\"p_imp\"]].isel(p_imp_dim_0=indices), \"p_imp\", group=None\n    )\n```\n:::\n\n\nAgain, nothing novel in Python. This can all be done with Arviz--it's more about the syntax and the approach to building the dataset and plot. With this framework, it is already in the dataframe format and easy to join, filter, and derive new metrics from it.\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\n(\n    ggplot(aes(fill=\"model\"))\n    + bt.stat_halfeye(\n        aes(\"p\", \"obs\"),\n        data=df_basic.with_columns(model=pl.lit(\"basic\")),\n        alpha=0.4,\n    )\n    + bt.stat_halfeye(\n        aes(\"p_imp\", \"obs\"),\n        data=df_improved.with_columns(\n            obs=pl.lit(\"Kopech, Michael\"), model=pl.lit(\"improved\")\n        ),\n        alpha=0.4,\n    )\n    + theme_minimal()\n    + theme(panel_grid_minor= element_blank(), panel_grid_major_y=element_blank(), legend_position='bottom')\n    + labs(y = '', title=\"Kopech estimated parameter comparison\")\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n![](index_files/figure-html/cell-26-output-1.png){width=672 height=480}\n:::\n:::\n\n\nBeautiful!\n\n## Conclusion\n\nThis was a multi-faceted learning project for me. I wanted to learn more about BART, and it also lead me to get familiar with using integrated AI tools. As I started recreating this blog post, I found myself referencing other work I had done with this tidy pymc implementation.\n\nI realized it would be a lot more effective to have a package than copying over several functions, and I started to build that with **a ton of help from Claude Code**.\n\nHaving this project to go alongside it was great, because I got real immediate feedback on the `bayestidy` package as I tried to use it. First, Claude provided a lot of the scaffolding from the context of other work I provided. It built out the initial foundation of the package, and of course it built tests that passed.\n\nIt's really hard to go through everything generated and make sure it is covering exactly what you need. I believe even more so for data science and plotting packages.\n\nAs I started working with it, I would notice little things off. The visualizations were layered wrong, needed to custom flip what was filled or colored, random things I picked out from experience using tidybayes and replicating it before.\n\nThe development process was a lot more of dialogue to a chat promt from me, but I felt like it got well refined during this process of actively trying to use it.\n\nI'm glad I integrated Claude because it gave me a new respect for what that development workflow is and ways I can work better with it. I'm not going to manifest a package with words out of thin air, but if I have a problem to solve and some experience with what I want to do -- it can go a long way. In addition to replicating this blog post, I was also able to build a package that I really am interested in along with it. In my spare time less than a week from reading it.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}